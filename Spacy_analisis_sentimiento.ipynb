{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de spacy para hacer el análisis de sentimiento de los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports y configuraciones necesarias\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos los csv generados con datos en inglés y en español para probar con spacy.\n",
    "\n",
    "Estos datos ya con sentimiento calculado para cada texto, podremos usarlos para entrenar y testear los modelos que generemos para clasificar los tweets por sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 1759315\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "# Los datos de sentimiento vienen con los valores: positivo=2, neutral=1, negativo=0\n",
    "# cargamos datos en inglés\n",
    "df_anotados_english = pd.read_csv('./data/df_result_english.csv', sep=',')\n",
    "\n",
    "print(\"num_rows: %d\\tColumnas: %d\\n\" % (df_anotados_english.shape[0], df_anotados_english.shape[1]) )\n",
    "print(\"Columnas:\\n\", list(df_anotados_english.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0                @VirginAmerica What @dhepburn said.        1.0\n",
       "1  @VirginAmerica plus you've added commercials t...        2.0\n",
       "2  @VirginAmerica I didn't today... Must mean I n...        1.0\n",
       "3  @VirginAmerica it's really aggressive to blast...        0.0\n",
       "4  @VirginAmerica and it's a really big bad thing...        0.0\n",
       "5  @VirginAmerica seriously would pay $30 a fligh...        0.0\n",
       "6  @VirginAmerica yes, nearly every time I fly VX...        2.0\n",
       "7  @VirginAmerica Really missed a prime opportuni...        1.0\n",
       "8    @virginamerica Well, I didn't…but NOW I DO! :-D        2.0\n",
       "9  @VirginAmerica it was amazing, and arrived an ...        2.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anotados_english.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha cargado los datos de sentimiento como float, posiblemente porque algún valor viene como NaN o similar, sustituimos esos valores por -1 y luego eliminamos esos registros antes de convertir la columna en int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anotados_english = df_anotados_english.fillna(-1)\n",
    "df_anotados_english = df_anotados_english[df_anotados_english['sentiment']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_float = df_anotados_english.columns[df_anotados_english.dtypes == float]\n",
    "df_anotados_english[c_float] = df_anotados_english[c_float].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0                @VirginAmerica What @dhepburn said.          1\n",
       "1  @VirginAmerica plus you've added commercials t...          2\n",
       "2  @VirginAmerica I didn't today... Must mean I n...          1\n",
       "3  @VirginAmerica it's really aggressive to blast...          0\n",
       "4  @VirginAmerica and it's a really big bad thing...          0\n",
       "5  @VirginAmerica seriously would pay $30 a fligh...          0\n",
       "6  @VirginAmerica yes, nearly every time I fly VX...          2\n",
       "7  @VirginAmerica Really missed a prime opportuni...          1\n",
       "8    @virginamerica Well, I didn't…but NOW I DO! :-D          2\n",
       "9  @VirginAmerica it was amazing, and arrived an ...          2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anotados_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 48658\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "# Los datos de sentimiento vienen con los valores: positivo=2, neutral=1, negativo=0\n",
    "# cargamos datos en español\n",
    "df_anotados_spanish = pd.read_csv('./data/df_result_spanish.csv', sep=',')\n",
    "\n",
    "print(\"num_rows: %d\\tColumnas: %d\\n\" % (df_anotados_spanish.shape[0], df_anotados_spanish.shape[1]) )\n",
    "print(\"Columnas:\\n\", list(df_anotados_spanish.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Desde el escaño. Todo listo para empezar #endi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bdías. EM no se ira de puente. Si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Un sistema económico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  @PauladeLasHeras No te libraras de ayudar me/n...          1\n",
       "1                          @marodriguezb Gracias MAR          2\n",
       "2  Off pensando en el regalito Sinde, la que se v...          0\n",
       "3  Conozco a alguien q es adicto al drama! Ja ja ...          2\n",
       "4  Toca @crackoviadeTV3 . Grabación dl especial N...          2\n",
       "5  Buen día todos! Lo primero mandar un abrazo gr...          2\n",
       "6  Desde el escaño. Todo listo para empezar #endi...          2\n",
       "7  Bdías. EM no se ira de puente. Si vosotros os ...          2\n",
       "8  Un sistema económico q recorta dinero para pre...          2\n",
       "9                  #programascambiados caca d ajuste          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anotados_spanish.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuento valores datos en inglés: 2    872864\n",
      "0    871980\n",
      "1     14470\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "Recuento valores datos en español: 2    26204\n",
      "0    19344\n",
      "1     3110\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# vemos el recuento de los distintos valores que tiene la columna del sentimiento en el dataset\n",
    "print(\"Recuento valores datos en inglés:\",pd.value_counts(df_anotados_english['sentiment']))\n",
    "print(\"\\n\")\n",
    "print(\"Recuento valores datos en español:\",pd.value_counts(df_anotados_spanish['sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de los textos.\n",
    "\n",
    "Vamos a aplicar a todos los textos de los dataframes, técnicas de preprocesado y limpieza de textos para hacer más efectivos y sencillos los análisis de los textos y la clasificación del sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones de preprocesado y limpieza de los textos\n",
    "def limpiar_tweet(tweet):\n",
    "    # quitamos RT, @nombre_usuario, links y urls, hashtags, menciones, caracteres extraños o emoticonos\n",
    "    tweet = re.sub('  +', ' ', tweet)\n",
    "    # eliminar acentos\n",
    "    tweet = ''.join((c for c in unicodedata.normalize('NFD', tweet) if unicodedata.category(c) != 'Mn'))  \n",
    "    # convertir la repetición de una letra más de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "    # eliminar \"RT\", \"@usuario\", o los enlaces que es información que no sería útil analizar\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub('','',tweet).lower() \n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet)\n",
    "    tweet = re.sub(r'[0-9]', '', tweet) \n",
    "\n",
    "    return tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicamos las funciones a los textos en inglés\n",
    "df_clean_english = df_anotados_english.copy()\n",
    "df_clean_english['text'] = df_clean_english['text'].apply(limpiar_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what   said</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus you ve added commercials to the experie...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i didn t today   must mean i need to take an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it s really aggressive to blast obnoxious  e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and it s a really big bad thing about it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>seriously would pay   a flight for seats tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes  nearly every time i fly vx this  ear wo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>really missed a prime opportunity for men wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>well  i didn t but now i do    d</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>it was amazing  and arrived an hour early  y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>did you know that suicide is the second lead...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i  lt  pretty graphics  so much better than ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>this is such a great deal  already thinking ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i m flying your fabulous seductive skies a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>thanks</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sfo pdx schedule is still mia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>so excited for my first cross country flight...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i flew from nyc to sfo last week and couldn ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>i   flying</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>you know what would be amazingly awesome  bo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "0                                        what   said           1\n",
       "1     plus you ve added commercials to the experie...          2\n",
       "2     i didn t today   must mean i need to take an...          1\n",
       "3     it s really aggressive to blast obnoxious  e...          0\n",
       "4            and it s a really big bad thing about it          0\n",
       "5     seriously would pay   a flight for seats tha...          0\n",
       "6     yes  nearly every time i fly vx this  ear wo...          2\n",
       "7     really missed a prime opportunity for men wi...          1\n",
       "8                    well  i didn t but now i do    d          2\n",
       "9     it was amazing  and arrived an hour early  y...          2\n",
       "10    did you know that suicide is the second lead...          1\n",
       "11    i  lt  pretty graphics  so much better than ...          2\n",
       "12    this is such a great deal  already thinking ...          2\n",
       "13      i m flying your fabulous seductive skies a...          2\n",
       "14                                            thanks           2\n",
       "15                     sfo pdx schedule is still mia           0\n",
       "16    so excited for my first cross country flight...          2\n",
       "17    i flew from nyc to sfo last week and couldn ...          0\n",
       "18                                   i   flying                2\n",
       "19    you know what would be amazingly awesome  bo...          2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# damos un vistazo a los textos resultantes\n",
    "df_clean_english.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicamos las funciones a los textos en español\n",
    "df_clean_spanish = df_anotados_spanish.copy()\n",
    "df_clean_spanish['text'] = df_clean_spanish['text'].apply(limpiar_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no te libraras de ayudar me nos  besos y gra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gracias mar</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>off pensando en el regalito sinde  la que se v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conozco a alguien q es adicto al drama  ja ja ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toca     grabacion dl especial navideno  mari ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>buen dia todos  lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>desde el escano  todo listo para empezar endia...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bdias  em no se ira de puente  si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>un sistema economico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>buen viernes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>programascambiados es tt gracias a   lat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>la universidad confia en de la calle para enca...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>me ayudais a que indultoneiro sea tt  por si ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>abcdesevilla es  recio no tiene  indicios pote...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abcdesevilla es  cuatro altos cargos de empleo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>la marcha atras del pp en posponer devolucion ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>viernes negro  aumenta el paro en noviembre y ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>accidente en bus vao a  km    motorista de  an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>la verdadera seriedad es comica   niall binns...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "0     no te libraras de ayudar me nos  besos y gra...          1\n",
       "1                                         gracias mar          2\n",
       "2   off pensando en el regalito sinde  la que se v...          0\n",
       "3   conozco a alguien q es adicto al drama  ja ja ...          2\n",
       "4   toca     grabacion dl especial navideno  mari ...          2\n",
       "5   buen dia todos  lo primero mandar un abrazo gr...          2\n",
       "6   desde el escano  todo listo para empezar endia...          2\n",
       "7   bdias  em no se ira de puente  si vosotros os ...          2\n",
       "8   un sistema economico q recorta dinero para pre...          2\n",
       "9                    programascambiados caca d ajuste          0\n",
       "10                                       buen viernes          2\n",
       "11        programascambiados es tt gracias a   lat...          2\n",
       "12  la universidad confia en de la calle para enca...          2\n",
       "13   me ayudais a que indultoneiro sea tt  por si ...          2\n",
       "14  abcdesevilla es  recio no tiene  indicios pote...          0\n",
       "15  abcdesevilla es  cuatro altos cargos de empleo...          0\n",
       "16  la marcha atras del pp en posponer devolucion ...          0\n",
       "17  viernes negro  aumenta el paro en noviembre y ...          0\n",
       "18  accidente en bus vao a  km    motorista de  an...          0\n",
       "19   la verdadera seriedad es comica   niall binns...          2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# damos un vistazo a los textos resultantes\n",
    "df_clean_spanish.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis morfosintáctico con Spacy\n",
    "\n",
    "Cuando el texto está expresado de forma natural es normal que una palabra aparezca con distintas conjugaciones y formas, sin que cambie el significado del texto. Cuando esto ocurre un preprocesado que se realiza es convertir palabras a lemas, o eliminar categorías morfológicas que no aportan información. Se puede usar para ello la librería Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /Users/josemanuel/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: es_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.1.0/es_core_news_sm-2.1.0.tar.gz#egg=es_core_news_sm==2.1.0 in /Users/josemanuel/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/es_core_news_sm -->\n",
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/spacy/data/es\n",
      "You can now load the model via spacy.load('es')\n"
     ]
    }
   ],
   "source": [
    "# spaCy utiliza modelos morfosintácticos específicos para cada idioma. Por defecto la librería no incluye ningún \n",
    "# modelo, pero podemos instalarlo de manera sencilla con comandos a python. Las siguientes líneas ejecutan un comando \n",
    "# de sistema para instalar el modelo de spaCy para el idioma inglés y español.\n",
    "!python -m spacy download en\n",
    "!python -m spacy download es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar modelos en memoria\n",
    "nlp_eng = spacy.load('en')\n",
    "nlp_spa = spacy.load('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Phrase\n",
      "Token: detected\n",
      "Token: as\n",
      "Token: an\n",
      "Token: example\n",
      "Token: of\n",
      "Token: the\n",
      "Token: use\n",
      "Token: of\n",
      "Token: Spacy\n",
      "Token: library\n",
      "Token: to\n",
      "Token: analyze\n",
      "Token: texts\n",
      "Token: .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lema</th>\n",
       "      <th>POS</th>\n",
       "      <th>tag</th>\n",
       "      <th>shap</th>\n",
       "      <th>isalpha</th>\n",
       "      <th>isstop</th>\n",
       "      <th>padre</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phrase</td>\n",
       "      <td>phrase</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>detected</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detected</td>\n",
       "      <td>detect</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>detected</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>detected</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>example</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>example</td>\n",
       "      <td>example</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>as</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>example</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>use</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>use</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Spacy</td>\n",
       "      <td>Spacy</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>library</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>library</td>\n",
       "      <td>library</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "      <td>TO</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>analyze</td>\n",
       "      <td>aux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>analyze</td>\n",
       "      <td>analyze</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>use</td>\n",
       "      <td>acl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>texts</td>\n",
       "      <td>text</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>analyze</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>detected</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token     lema    POS  tag   shap  isalpha  isstop     padre       dep\n",
       "0     Phrase   phrase   NOUN   NN  Xxxxx     True   False  detected     nsubj\n",
       "1   detected   detect   VERB  VBD   xxxx     True   False  detected      ROOT\n",
       "2         as       as    ADP   IN     xx     True    True  detected      prep\n",
       "3         an       an    DET   DT     xx     True    True   example       det\n",
       "4    example  example   NOUN   NN   xxxx     True   False        as      pobj\n",
       "5         of       of    ADP   IN     xx     True    True   example      prep\n",
       "6        the      the    DET   DT    xxx     True    True       use       det\n",
       "7        use      use   NOUN   NN    xxx     True   False        of      pobj\n",
       "8         of       of    ADP   IN     xx     True    True       use      prep\n",
       "9      Spacy    Spacy  PROPN  NNP  Xxxxx     True   False   library  compound\n",
       "10   library  library   NOUN   NN   xxxx     True   False        of      pobj\n",
       "11        to       to   PART   TO     xx     True    True   analyze       aux\n",
       "12   analyze  analyze   VERB   VB   xxxx     True   False       use       acl\n",
       "13     texts     text   NOUN  NNS   xxxx     True   False   analyze      dobj\n",
       "14         .        .  PUNCT    .      .    False   False  detected     punct"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de uso de Spacy. Lo más interesante son los diferentes campos con información extra que contiene cada token\n",
    "frase = \"Phrase detected as an example of the use of Spacy library to analyze texts.\"\n",
    "doc = nlp_eng(frase)\n",
    "\n",
    "for token in doc:\n",
    "    print(\"Token:\", token)\n",
    "    \n",
    "pd.DataFrame(columns=[\"token\", \"lema\", \"POS\", \"tag\", \"shap\", \"isalpha\", \"isstop\", \"padre\", \"dep\"],\n",
    "    data=[[token.text, token.lemma_, token.pos_, token.tag_,\n",
    "        token.shape_, token.is_alpha, token.is_stop, token.head, token.dep_]\n",
    "        for token in doc]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visto el funcionamiento de spaCy, vamos a pasar a ejecutar el análisis morfosintáctico para cada texto de nuestros datos. Primero para los datos en inglés y luego para los datos en español.\n",
    "\n",
    "El número de datos en inglés es muy grande, para evitar problemas por tiempo y coste computacional a la hora de realizar los procesamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_english = df_clean_english.sample(frac=0.1, replace=False, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 175931\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "print(\"num_rows: %d\\tColumnas: %d\\n\" % (df_sample_english.shape[0], df_sample_english.shape[1]) )\n",
    "print(\"Columnas:\\n\", list(df_sample_english.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores dataset:\n",
      " 2    87560\n",
      "0    86882\n",
      "1     1489\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# reparto de valores del atributo sentimiento del sample generado de los datos en inglés\n",
    "print(\"Valores dataset:\\n\",pd.value_counts(df_sample_english['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>analyzed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819606</th>\n",
       "      <td>wants a macbook  windows sucks for creating be...</td>\n",
       "      <td>0</td>\n",
       "      <td>(wants, a, macbook,  , windows, sucks, for, cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688869</th>\n",
       "      <td>music does something that is indescribable but...</td>\n",
       "      <td>2</td>\n",
       "      <td>(music, does, something, that, is, indescribab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884831</th>\n",
       "      <td>williams oh  it was  i didn t get done until...</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , williams, oh,  , it, was,  , i, didn, t, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754246</th>\n",
       "      <td>spa felt good  and now its time to sleep  soo ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(spa, felt, good,  , and, now, its, time, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269213</th>\n",
       "      <td>she looks sad  is it bcos her gay brother was ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(she, looks, sad,  , is, it, bcos, her, gay, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment  \\\n",
       "819606   wants a macbook  windows sucks for creating be...          0   \n",
       "1688869  music does something that is indescribable but...          2   \n",
       "884831     williams oh  it was  i didn t get done until...          0   \n",
       "754246   spa felt good  and now its time to sleep  soo ...          0   \n",
       "269213   she looks sad  is it bcos her gay brother was ...          0   \n",
       "\n",
       "                                                  analyzed  \n",
       "819606   (wants, a, macbook,  , windows, sucks, for, cr...  \n",
       "1688869  (music, does, something, that, is, indescribab...  \n",
       "884831   (  , williams, oh,  , it, was,  , i, didn, t, ...  \n",
       "754246   (spa, felt, good,  , and, now, its, time, to, ...  \n",
       "269213   (she, looks, sad,  , is, it, bcos, her, gay, b...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilizamos una función para ejecutar dicho análisis morfosintáctico a cada texto de los dataframes\n",
    "# lo aplicamos al dataframe de datos en inglés\n",
    "def add_analyzed(df):\n",
    "    analyzed = [nlp_eng(text) for text in df[\"text\"]]\n",
    "    df[\"analyzed\"] = pd.Series(analyzed, index = df.index)\n",
    "    \n",
    "add_analyzed(df_sample_english)\n",
    "df_sample_english.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo aplicamos a los datos en español\n",
    "def add_analyzed_spa(df):\n",
    "    analyzed = [nlp_spa(text) for text in df[\"text\"]]\n",
    "    df[\"analyzed\"] = pd.Series(analyzed, index = df.index)\n",
    "    \n",
    "add_analyzed_spa(df_clean_spanish)\n",
    "df_clean_spanish.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a sacar partido a la información morfológica que nos proporciona spaCy para mejorar el modelo predictivo. Para ello realizaremos las operaciones:\n",
    "\n",
    "* Filtrar los textos para quedarnos solo con aquellas palabras de las categorías morfológicas con más carga de emoción.\n",
    "* Filtrar los textos para no incluir palabras stopwords.\n",
    "* Sustituir cada token por su lema, para así reducir el tamaño del vocabulario y simplificar el problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>analyzed</th>\n",
       "      <th>posfilter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128038</th>\n",
       "      <td>seems i always end up at tx schl  email lori...</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , seems, i, always, end, up, at, tx, schl, ...</td>\n",
       "      <td>end tx schl email lori com add mail list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491756</th>\n",
       "      <td>yeah  don t worry  you will   there s still ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , yeah,  , don, t, worry,  , you, will,   ,...</td>\n",
       "      <td>don t worry s week half hit road weekend s ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470925</th>\n",
       "      <td>excited that my email works reliably now with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(excited, that, my, email, works, reliably, no...</td>\n",
       "      <td>excited email work reliably secondary dns back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491264</th>\n",
       "      <td>yea that s the sad part</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , yea, that, s, the, sad, part)</td>\n",
       "      <td>s sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836490</th>\n",
       "      <td>you let me know  i think you too busy for li...</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , you, let, me, know,  , i, think, you, too...</td>\n",
       "      <td>let know think busy lil old netta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  sentiment  \\\n",
       "128038    seems i always end up at tx schl  email lori...          2   \n",
       "491756    yeah  don t worry  you will   there s still ...          0   \n",
       "470925  excited that my email works reliably now with ...          0   \n",
       "491264                           yea that s the sad part           0   \n",
       "836490    you let me know  i think you too busy for li...          0   \n",
       "\n",
       "                                                 analyzed  \\\n",
       "128038  (  , seems, i, always, end, up, at, tx, schl, ...   \n",
       "491756  (  , yeah,  , don, t, worry,  , you, will,   ,...   \n",
       "470925  (excited, that, my, email, works, reliably, no...   \n",
       "491264                 (  , yea, that, s, the, sad, part)   \n",
       "836490  (  , you, let, me, know,  , i, think, you, too...   \n",
       "\n",
       "                                                posfilter  \n",
       "128038           end tx schl email lori com add mail list  \n",
       "491756  don t worry s week half hit road weekend s ful...  \n",
       "470925  excited email work reliably secondary dns back...  \n",
       "491264                                              s sad  \n",
       "836490                  let know think busy lil old netta  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilizamos otra función para aplicar a los textos los filtrados y transformaciones comentados\n",
    "# primero a los textos en inglés y luego a los textos en español igual que anteriormente\n",
    "def add_posfilter(df):\n",
    "    posfilter = [\" \".join([token.lemma_ for token in text \n",
    "                           if token.pos_ in {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"} and not token.is_stop]) \n",
    "                 for text in df[\"analyzed\"]]\n",
    "    df[\"posfilter\"] = pd.Series(posfilter, index = df.index)\n",
    "    \n",
    "add_posfilter(df_sample_english)\n",
    "df_sample_english.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>analyzed</th>\n",
       "      <th>posfilter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no te libraras de ayudar me nos  besos y gra...</td>\n",
       "      <td>1</td>\n",
       "      <td>(  , no, te, libraras, de, ayudar, me, nos,  ,...</td>\n",
       "      <td>librar ayudar beso gracia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gracias mar</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , gracias, mar)</td>\n",
       "      <td>gracia mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>off pensando en el regalito sinde  la que se v...</td>\n",
       "      <td>0</td>\n",
       "      <td>(off, pensando, en, el, regalito, sinde,  , la...</td>\n",
       "      <td>pensar regalito sinde sgae corrupto sacar conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conozco a alguien q es adicto al drama  ja ja ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(conozco, a, alguien, q, es, adicto, al, drama...</td>\n",
       "      <td>conocer adicto drama sonar d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toca     grabacion dl especial navideno  mari ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(toca,     , grabacion, dl, especial, navideno...</td>\n",
       "      <td>tocar grabacion especial navideno mari crisma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0    no te libraras de ayudar me nos  besos y gra...          1   \n",
       "1                                        gracias mar          2   \n",
       "2  off pensando en el regalito sinde  la que se v...          0   \n",
       "3  conozco a alguien q es adicto al drama  ja ja ...          2   \n",
       "4  toca     grabacion dl especial navideno  mari ...          2   \n",
       "\n",
       "                                            analyzed  \\\n",
       "0  (  , no, te, libraras, de, ayudar, me, nos,  ,...   \n",
       "1                                 (  , gracias, mar)   \n",
       "2  (off, pensando, en, el, regalito, sinde,  , la...   \n",
       "3  (conozco, a, alguien, q, es, adicto, al, drama...   \n",
       "4  (toca,     , grabacion, dl, especial, navideno...   \n",
       "\n",
       "                                           posfilter  \n",
       "0                          librar ayudar beso gracia  \n",
       "1                                         gracia mar  \n",
       "2  pensar regalito sinde sgae corrupto sacar conc...  \n",
       "3                       conocer adicto drama sonar d  \n",
       "4      tocar grabacion especial navideno mari crisma  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_posfilter(df_clean_spanish)\n",
    "df_clean_spanish.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rellenamos con espacios los datos vacíos por si hubiera alguno y pudiera dar problemas posteriormente\n",
    "df_sample_english = df_sample_english.fillna(' ')\n",
    "df_clean_spanish = df_clean_spanish.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>analyzed</th>\n",
       "      <th>posfilter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128038</th>\n",
       "      <td>seems i always end up at tx schl  email lori...</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , seems, i, always, end, up, at, tx, schl, ...</td>\n",
       "      <td>end tx schl email lori com add mail list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491756</th>\n",
       "      <td>yeah  don t worry  you will   there s still ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , yeah,  , don, t, worry,  , you, will,   ,...</td>\n",
       "      <td>don t worry s week half hit road weekend s ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470925</th>\n",
       "      <td>excited that my email works reliably now with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(excited, that, my, email, works, reliably, no...</td>\n",
       "      <td>excited email work reliably secondary dns back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491264</th>\n",
       "      <td>yea that s the sad part</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , yea, that, s, the, sad, part)</td>\n",
       "      <td>s sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836490</th>\n",
       "      <td>you let me know  i think you too busy for li...</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , you, let, me, know,  , i, think, you, too...</td>\n",
       "      <td>let know think busy lil old netta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371404</th>\n",
       "      <td>well   susan didn t make it    runner up to...</td>\n",
       "      <td>0</td>\n",
       "      <td>(   , well,   , susan, didn, t, make, it,    ,...</td>\n",
       "      <td>susan didn t runner bunch dancer gobsmacke s word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73350</th>\n",
       "      <td>before the cool runs out  ima be trying my b...</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , before, the, cool, runs, out,  , i, m, a,...</td>\n",
       "      <td>cool run be try best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166160</th>\n",
       "      <td>lmao not today sir sorry sir i did go yester...</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , lmao, not, today, sir, sorry, sir, i, did...</td>\n",
       "      <td>lmao today sir sorry sir yesterday sir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070017</th>\n",
       "      <td>loves that lubbock is wet  its about time  no ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(loves, that, lubbock, is, wet,  , its, about,...</td>\n",
       "      <td>love lubbock wet time strip run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229521</th>\n",
       "      <td>i want a golden retriever puppy   soo cute   d...</td>\n",
       "      <td>0</td>\n",
       "      <td>(i, want, a, golden, retriever, puppy,   , soo...</td>\n",
       "      <td>want golden retriever puppy soo cute damn want</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment  \\\n",
       "128038     seems i always end up at tx schl  email lori...          2   \n",
       "491756     yeah  don t worry  you will   there s still ...          0   \n",
       "470925   excited that my email works reliably now with ...          0   \n",
       "491264                            yea that s the sad part           0   \n",
       "836490     you let me know  i think you too busy for li...          0   \n",
       "371404      well   susan didn t make it    runner up to...          0   \n",
       "73350      before the cool runs out  ima be trying my b...          2   \n",
       "1166160    lmao not today sir sorry sir i did go yester...          2   \n",
       "1070017  loves that lubbock is wet  its about time  no ...          2   \n",
       "229521   i want a golden retriever puppy   soo cute   d...          0   \n",
       "\n",
       "                                                  analyzed  \\\n",
       "128038   (  , seems, i, always, end, up, at, tx, schl, ...   \n",
       "491756   (  , yeah,  , don, t, worry,  , you, will,   ,...   \n",
       "470925   (excited, that, my, email, works, reliably, no...   \n",
       "491264                  (  , yea, that, s, the, sad, part)   \n",
       "836490   (  , you, let, me, know,  , i, think, you, too...   \n",
       "371404   (   , well,   , susan, didn, t, make, it,    ,...   \n",
       "73350    (  , before, the, cool, runs, out,  , i, m, a,...   \n",
       "1166160  (  , lmao, not, today, sir, sorry, sir, i, did...   \n",
       "1070017  (loves, that, lubbock, is, wet,  , its, about,...   \n",
       "229521   (i, want, a, golden, retriever, puppy,   , soo...   \n",
       "\n",
       "                                                 posfilter  \n",
       "128038            end tx schl email lori com add mail list  \n",
       "491756   don t worry s week half hit road weekend s ful...  \n",
       "470925   excited email work reliably secondary dns back...  \n",
       "491264                                               s sad  \n",
       "836490                   let know think busy lil old netta  \n",
       "371404   susan didn t runner bunch dancer gobsmacke s word  \n",
       "73350                                 cool run be try best  \n",
       "1166160             lmao today sir sorry sir yesterday sir  \n",
       "1070017                    love lubbock wet time strip run  \n",
       "229521      want golden retriever puppy soo cute damn want  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>analyzed</th>\n",
       "      <th>posfilter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no te libraras de ayudar me nos  besos y gra...</td>\n",
       "      <td>1</td>\n",
       "      <td>(  , no, te, libraras, de, ayudar, me, nos,  ,...</td>\n",
       "      <td>librar ayudar beso gracia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gracias mar</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , gracias, mar)</td>\n",
       "      <td>gracia mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>off pensando en el regalito sinde  la que se v...</td>\n",
       "      <td>0</td>\n",
       "      <td>(off, pensando, en, el, regalito, sinde,  , la...</td>\n",
       "      <td>pensar regalito sinde sgae corrupto sacar conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conozco a alguien q es adicto al drama  ja ja ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(conozco, a, alguien, q, es, adicto, al, drama...</td>\n",
       "      <td>conocer adicto drama sonar d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toca     grabacion dl especial navideno  mari ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(toca,     , grabacion, dl, especial, navideno...</td>\n",
       "      <td>tocar grabacion especial navideno mari crisma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>buen dia todos  lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "      <td>(buen, dia, todos,  , lo, primero, mandar, un,...</td>\n",
       "      <td>mandar abrazar grande miguel familia grandeza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>desde el escano  todo listo para empezar endia...</td>\n",
       "      <td>2</td>\n",
       "      <td>(desde, el, escano,  , todo, listo, para, empe...</td>\n",
       "      <td>escano listar empezar endiascomohoy congreso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bdias  em no se ira de puente  si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(bdias,  , em, no, se, ira, de, puente,  , si,...</td>\n",
       "      <td>bdias ira puente tableta pc orbyt decir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>un sistema economico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "      <td>(un, sistema, economico, q, recorta, dinero, p...</td>\n",
       "      <td>sistema economico recortar dinero prestación s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "      <td>(programascambiados, caca, d, ajuste)</td>\n",
       "      <td>programascambiados caca ajustar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0    no te libraras de ayudar me nos  besos y gra...          1   \n",
       "1                                        gracias mar          2   \n",
       "2  off pensando en el regalito sinde  la que se v...          0   \n",
       "3  conozco a alguien q es adicto al drama  ja ja ...          2   \n",
       "4  toca     grabacion dl especial navideno  mari ...          2   \n",
       "5  buen dia todos  lo primero mandar un abrazo gr...          2   \n",
       "6  desde el escano  todo listo para empezar endia...          2   \n",
       "7  bdias  em no se ira de puente  si vosotros os ...          2   \n",
       "8  un sistema economico q recorta dinero para pre...          2   \n",
       "9                   programascambiados caca d ajuste          0   \n",
       "\n",
       "                                            analyzed  \\\n",
       "0  (  , no, te, libraras, de, ayudar, me, nos,  ,...   \n",
       "1                                 (  , gracias, mar)   \n",
       "2  (off, pensando, en, el, regalito, sinde,  , la...   \n",
       "3  (conozco, a, alguien, q, es, adicto, al, drama...   \n",
       "4  (toca,     , grabacion, dl, especial, navideno...   \n",
       "5  (buen, dia, todos,  , lo, primero, mandar, un,...   \n",
       "6  (desde, el, escano,  , todo, listo, para, empe...   \n",
       "7  (bdias,  , em, no, se, ira, de, puente,  , si,...   \n",
       "8  (un, sistema, economico, q, recorta, dinero, p...   \n",
       "9              (programascambiados, caca, d, ajuste)   \n",
       "\n",
       "                                           posfilter  \n",
       "0                          librar ayudar beso gracia  \n",
       "1                                         gracia mar  \n",
       "2  pensar regalito sinde sgae corrupto sacar conc...  \n",
       "3                       conocer adicto drama sonar d  \n",
       "4      tocar grabacion especial navideno mari crisma  \n",
       "5  mandar abrazar grande miguel familia grandeza ...  \n",
       "6       escano listar empezar endiascomohoy congreso  \n",
       "7            bdias ira puente tableta pc orbyt decir  \n",
       "8  sistema economico recortar dinero prestación s...  \n",
       "9                    programascambiados caca ajustar  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_spanish.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separamos los datos en training y test para los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eng = df_sample_english['posfilter']\n",
    "y_eng = df_sample_english['sentiment']\n",
    "\n",
    "X_eng_train, X_eng_test, y_eng_train, y_eng_test = train_test_split(X_eng, y_eng, test_size=0.3, random_state=42)\n",
    "\n",
    "X_spa = df_clean_spanish['posfilter']\n",
    "y_spa = df_clean_spanish['sentiment']\n",
    "\n",
    "X_spa_train, X_spa_test, y_spa_train, y_spa_test = train_test_split(X_spa, y_spa, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7705191360363774"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# montamos un modelo basado en el análisis de palabras\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LinearSVC())\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'vectorizer__analyzer' : ['word'],\n",
    "    'vectorizer__ngram_range' : [(1, 1), (1,2), (1,3)]\n",
    "}\n",
    "\n",
    "# lo entrenamos y evaluamos con los datos en inglés\n",
    "model = GridSearchCV(pipeline, params, n_jobs = 7)\n",
    "model.fit(X_eng_train.values, y_eng_train)\n",
    "model.score(X_eng_test.values, y_eng_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8068228524455405"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenamos y evaluamos otro modelo con los datos en español\n",
    "model_spa = GridSearchCV(pipeline, params, n_jobs = 7)\n",
    "model_spa.fit(X_spa_train.values, y_spa_train)\n",
    "model_spa.score(X_spa_test.values, y_spa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este problema es desbalanceado, ya que existen más datos de un sentimiento en concreto, que de otros. Por ello, usar la precisión o accuracy como métrica de la calidad del modelo no es aconsejable, ya que un modelo que clasifique todas los textos de un sentimiento en concreto podrá tener una precisión muy alta, a pesar de su falta de utilidad en práctica. En su lugar deberá emplearse la métrica roc_auc_score, que tiene en cuenta la importancia de ambas clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos del dataframe los registros con sentimiento == 1, que serían los datos con sentimiento neutro, ya\n",
    "# que esta métrica sólo debe usarse con clases que tengan dos posibles valores.\n",
    "df_sample_english_copy = df_sample_english.copy()\n",
    "df_clean_spanish_copy = df_clean_spanish.copy()\n",
    "\n",
    "df_sample_english_copy = df_sample_english_copy[df_sample_english_copy.sentiment != 1]\n",
    "df_clean_spanish_copy = df_clean_spanish_copy[df_clean_spanish_copy.sentiment != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores dataset english:\n",
      " 2    87553\n",
      "0    86966\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "Valores dataset spanish:\n",
      " 2    26204\n",
      "0    19344\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores dataset english:\\n\",pd.value_counts(df_sample_english_copy['sentiment']))\n",
    "print(\"\\n\")\n",
    "print(\"Valores dataset spanish:\\n\",pd.value_counts(df_clean_spanish_copy['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>analyzed</th>\n",
       "      <th>posfilter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128038</th>\n",
       "      <td>seems i always end up at tx schl  email lori...</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , seems, i, always, end, up, at, tx, schl, ...</td>\n",
       "      <td>end tx schl email lori com add mail list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491756</th>\n",
       "      <td>yeah  don t worry  you will   there s still ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , yeah,  , don, t, worry,  , you, will,   ,...</td>\n",
       "      <td>don t worry s week half hit road weekend s ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470925</th>\n",
       "      <td>excited that my email works reliably now with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(excited, that, my, email, works, reliably, no...</td>\n",
       "      <td>excited email work reliably secondary dns back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491264</th>\n",
       "      <td>yea that s the sad part</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , yea, that, s, the, sad, part)</td>\n",
       "      <td>s sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836490</th>\n",
       "      <td>you let me know  i think you too busy for li...</td>\n",
       "      <td>0</td>\n",
       "      <td>(  , you, let, me, know,  , i, think, you, too...</td>\n",
       "      <td>let know think busy lil old netta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371404</th>\n",
       "      <td>well   susan didn t make it    runner up to...</td>\n",
       "      <td>0</td>\n",
       "      <td>(   , well,   , susan, didn, t, make, it,    ,...</td>\n",
       "      <td>susan didn t runner bunch dancer gobsmacke s word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73350</th>\n",
       "      <td>before the cool runs out  ima be trying my b...</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , before, the, cool, runs, out,  , i, m, a,...</td>\n",
       "      <td>cool run be try best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166160</th>\n",
       "      <td>lmao not today sir sorry sir i did go yester...</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , lmao, not, today, sir, sorry, sir, i, did...</td>\n",
       "      <td>lmao today sir sorry sir yesterday sir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070017</th>\n",
       "      <td>loves that lubbock is wet  its about time  no ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(loves, that, lubbock, is, wet,  , its, about,...</td>\n",
       "      <td>love lubbock wet time strip run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229521</th>\n",
       "      <td>i want a golden retriever puppy   soo cute   d...</td>\n",
       "      <td>0</td>\n",
       "      <td>(i, want, a, golden, retriever, puppy,   , soo...</td>\n",
       "      <td>want golden retriever puppy soo cute damn want</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment  \\\n",
       "128038     seems i always end up at tx schl  email lori...          2   \n",
       "491756     yeah  don t worry  you will   there s still ...          0   \n",
       "470925   excited that my email works reliably now with ...          0   \n",
       "491264                            yea that s the sad part           0   \n",
       "836490     you let me know  i think you too busy for li...          0   \n",
       "371404      well   susan didn t make it    runner up to...          0   \n",
       "73350      before the cool runs out  ima be trying my b...          2   \n",
       "1166160    lmao not today sir sorry sir i did go yester...          2   \n",
       "1070017  loves that lubbock is wet  its about time  no ...          2   \n",
       "229521   i want a golden retriever puppy   soo cute   d...          0   \n",
       "\n",
       "                                                  analyzed  \\\n",
       "128038   (  , seems, i, always, end, up, at, tx, schl, ...   \n",
       "491756   (  , yeah,  , don, t, worry,  , you, will,   ,...   \n",
       "470925   (excited, that, my, email, works, reliably, no...   \n",
       "491264                  (  , yea, that, s, the, sad, part)   \n",
       "836490   (  , you, let, me, know,  , i, think, you, too...   \n",
       "371404   (   , well,   , susan, didn, t, make, it,    ,...   \n",
       "73350    (  , before, the, cool, runs, out,  , i, m, a,...   \n",
       "1166160  (  , lmao, not, today, sir, sorry, sir, i, did...   \n",
       "1070017  (loves, that, lubbock, is, wet,  , its, about,...   \n",
       "229521   (i, want, a, golden, retriever, puppy,   , soo...   \n",
       "\n",
       "                                                 posfilter  \n",
       "128038            end tx schl email lori com add mail list  \n",
       "491756   don t worry s week half hit road weekend s ful...  \n",
       "470925   excited email work reliably secondary dns back...  \n",
       "491264                                               s sad  \n",
       "836490                   let know think busy lil old netta  \n",
       "371404   susan didn t runner bunch dancer gobsmacke s word  \n",
       "73350                                 cool run be try best  \n",
       "1166160             lmao today sir sorry sir yesterday sir  \n",
       "1070017                    love lubbock wet time strip run  \n",
       "229521      want golden retriever puppy soo cute damn want  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_english_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>analyzed</th>\n",
       "      <th>posfilter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gracias mar</td>\n",
       "      <td>2</td>\n",
       "      <td>(  , gracias, mar)</td>\n",
       "      <td>gracia mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>off pensando en el regalito sinde  la que se v...</td>\n",
       "      <td>0</td>\n",
       "      <td>(off, pensando, en, el, regalito, sinde,  , la...</td>\n",
       "      <td>pensar regalito sinde sgae corrupto sacar conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conozco a alguien q es adicto al drama  ja ja ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(conozco, a, alguien, q, es, adicto, al, drama...</td>\n",
       "      <td>conocer adicto drama sonar d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toca     grabacion dl especial navideno  mari ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(toca,     , grabacion, dl, especial, navideno...</td>\n",
       "      <td>tocar grabacion especial navideno mari crisma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>buen dia todos  lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "      <td>(buen, dia, todos,  , lo, primero, mandar, un,...</td>\n",
       "      <td>mandar abrazar grande miguel familia grandeza ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>desde el escano  todo listo para empezar endia...</td>\n",
       "      <td>2</td>\n",
       "      <td>(desde, el, escano,  , todo, listo, para, empe...</td>\n",
       "      <td>escano listar empezar endiascomohoy congreso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bdias  em no se ira de puente  si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "      <td>(bdias,  , em, no, se, ira, de, puente,  , si,...</td>\n",
       "      <td>bdias ira puente tableta pc orbyt decir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>un sistema economico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "      <td>(un, sistema, economico, q, recorta, dinero, p...</td>\n",
       "      <td>sistema economico recortar dinero prestación s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "      <td>(programascambiados, caca, d, ajuste)</td>\n",
       "      <td>programascambiados caca ajustar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>buen viernes</td>\n",
       "      <td>2</td>\n",
       "      <td>(buen, viernes)</td>\n",
       "      <td>viernes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment  \\\n",
       "1                                         gracias mar          2   \n",
       "2   off pensando en el regalito sinde  la que se v...          0   \n",
       "3   conozco a alguien q es adicto al drama  ja ja ...          2   \n",
       "4   toca     grabacion dl especial navideno  mari ...          2   \n",
       "5   buen dia todos  lo primero mandar un abrazo gr...          2   \n",
       "6   desde el escano  todo listo para empezar endia...          2   \n",
       "7   bdias  em no se ira de puente  si vosotros os ...          2   \n",
       "8   un sistema economico q recorta dinero para pre...          2   \n",
       "9                    programascambiados caca d ajuste          0   \n",
       "10                                       buen viernes          2   \n",
       "\n",
       "                                             analyzed  \\\n",
       "1                                  (  , gracias, mar)   \n",
       "2   (off, pensando, en, el, regalito, sinde,  , la...   \n",
       "3   (conozco, a, alguien, q, es, adicto, al, drama...   \n",
       "4   (toca,     , grabacion, dl, especial, navideno...   \n",
       "5   (buen, dia, todos,  , lo, primero, mandar, un,...   \n",
       "6   (desde, el, escano,  , todo, listo, para, empe...   \n",
       "7   (bdias,  , em, no, se, ira, de, puente,  , si,...   \n",
       "8   (un, sistema, economico, q, recorta, dinero, p...   \n",
       "9               (programascambiados, caca, d, ajuste)   \n",
       "10                                    (buen, viernes)   \n",
       "\n",
       "                                            posfilter  \n",
       "1                                          gracia mar  \n",
       "2   pensar regalito sinde sgae corrupto sacar conc...  \n",
       "3                        conocer adicto drama sonar d  \n",
       "4       tocar grabacion especial navideno mari crisma  \n",
       "5   mandar abrazar grande miguel familia grandeza ...  \n",
       "6        escano listar empezar endiascomohoy congreso  \n",
       "7             bdias ira puente tableta pc orbyt decir  \n",
       "8   sistema economico recortar dinero prestación s...  \n",
       "9                     programascambiados caca ajustar  \n",
       "10                                            viernes  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_spanish_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formación de los dataframes de training y de test para sacar los datos de test a pasarle a la métrica\n",
    "X_eng = df_sample_english_copy['posfilter']\n",
    "y_eng = df_sample_english_copy['sentiment']\n",
    "\n",
    "X_eng_train, X_eng_test, y_eng_train, y_eng_test = train_test_split(X_eng, y_eng, test_size=0.3, random_state=42)\n",
    "\n",
    "X_spa = df_clean_spanish_copy['posfilter']\n",
    "y_spa = df_clean_spanish_copy['sentiment']\n",
    "\n",
    "X_spa_train, X_spa_test, y_spa_train, y_spa_test = train_test_split(X_spa, y_spa, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.775097410038964"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lo entrenamos y evaluamos con los datos en inglés sin valores catalogados como neutros\n",
    "model_eng = GridSearchCV(pipeline, params, n_jobs = 7)\n",
    "model_eng.fit(X_eng_train.values, y_eng_train)\n",
    "model_eng.score(X_eng_test.values, y_eng_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7749656890080583"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluamos resultados del modelo con la métrica roc_auc_score con los datos en inglés\n",
    "y_pred = model_eng.predict(X_eng_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_eng_test, y_pred)\n",
    "\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8611781924624954"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lo entrenamos y evaluamos con los datos en español sin valores catalogados como neutros\n",
    "model_spa_noNeutro = GridSearchCV(pipeline, params, n_jobs = 7)\n",
    "model_spa_noNeutro.fit(X_spa_train.values, y_spa_train)\n",
    "model_spa_noNeutro.score(X_spa_test.values, y_spa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524064451013114"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluamos resultados del modelo con la métrica roc_auc_score con los datos en español\n",
    "y_pred_spa = model_spa_noNeutro.predict(X_spa_test)\n",
    "\n",
    "roc_auc_spa = roc_auc_score(y_spa_test, y_pred_spa)\n",
    "\n",
    "roc_auc_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones del uso de Spacy\n",
    "\n",
    "Se obtienen mejores resultados con Spacy y el modelo utilizado que con las pruebas que se hicieron con textblob, aunque muy similares a las medidas obtenidas con el clasificador Naive Bayes. \n",
    "\n",
    "Se mide con una métrica más fiable como \"roc_auc_score\". Aunque para poder evaluar el modelo con esta métrica, deben ser solo dos clases, por lo que eliminamos los datos con sentimiento neutro, y así es como se obtienen los mejores resultados. Suponemos que con menos clases es más sencillo el aprendizaje.\n",
    "\n",
    "Se vuelve a obtener mejores resultados al entrenar y testear con los datos en español que con los datos en inglés, como ya pasaba en pruebas anteriores con el clasificador de textblob."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
