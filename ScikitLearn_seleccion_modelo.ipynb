{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de ScikitLearn y prueba del modelo con distintos algoritmos de clasificación para hacer el análisis de sentimiento de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports y configuraciones necesarias\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "conf = (SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"twitter\")\\\n",
    "          .master(\"spark://MacBook-Pro-de-Jose.local:7077\")\\\n",
    "          .config(\"spark.io.compression.codec\", \"snappy\")\\\n",
    "          .getOrCreate())  \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de funciones necesarias, intentando modular y limpiar el notebook de código repetitivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de carga de datos desde MongoDB de los datos almacenados desde Apache Nifi, tanto en inglés como español\n",
    "def carga_datos_mongo():\n",
    "    # conectamos con las tablas de Mongo desde donde cargamos los datos\n",
    "    df_mongo_english = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_english\").load()\n",
    "    df_mongo_spanish = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_spanish\").load()\n",
    "    \n",
    "    # nos quedamos solo con el atributo texto que es la información que nos interesa de cada dataset\n",
    "    df_text_english = df_mongo_english[['text']]\n",
    "    df_text_spanish = df_mongo_spanish[['text']]\n",
    "    \n",
    "    # pasamos a DF pandas\n",
    "    df_pandas_english = df_text_english.toPandas()\n",
    "    df_pandas_spanish = df_text_spanish.toPandas()\n",
    "    \n",
    "    return df_pandas_english, df_pandas_spanish\n",
    "\n",
    "\n",
    "# Función de carga de datos desde los csv generados con anterioridad con registros ya con el sentimiento anotado\n",
    "def carga_datos_csv(csv):\n",
    "    # cargamos el csv\n",
    "    if(csv=='english_full'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_english.csv', sep=',')\n",
    "    elif(csv=='spanish_full'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_spanish.csv', sep=',')\n",
    "    elif(csv=='english_neutro'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_english_neutral.csv', sep=',')\n",
    "    elif(csv=='spanish_neutro'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_spanish_neutral.csv', sep=',')\n",
    "    elif(csv=='english_noNeutro'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_english_noNeutral.csv', sep=',')\n",
    "    elif(csv=='spanish_noNeutro'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_spanish_noNeutral.csv', sep=',')\n",
    "        \n",
    "    # eliminamos posibles valores de sentimiento vacio, pasándo el valor a -1 y eliminando el registro después\n",
    "    df_anotados = df_anotados.fillna(-1)\n",
    "    df_anotados = df_anotados[df_anotados['sentiment']!=-1]\n",
    "    \n",
    "    # el atributo sentimiento nos aseguramos que sea tipo int en lugar de float\n",
    "    col_float = df_anotados.columns[df_anotados.dtypes == float]\n",
    "    df_anotados[col_float] = df_anotados[col_float].astype(int)\n",
    "\n",
    "    # como los datos en inglés del df full son muy grandes, para evitar problemas computacionales nos quedamos \n",
    "    # con una parte similar al tamaño de los datos que tenemos en español\n",
    "    if(csv=='english_full'):\n",
    "        df_anotados = df_anotados.sample(frac=0.04, replace=False, random_state=1)\n",
    "    \n",
    "    return df_anotados\n",
    "\n",
    "\n",
    "# Función de visualización de DFs: número de registros y columnas, lista de columnas, recuento de los distintos\n",
    "# valores de la columna sentimiento.\n",
    "def visualizar_datos_df(df,tipo):\n",
    "    print(\"num_rows: %d\\tColumnas: %d\\n\" % (df.shape[0], df.shape[1]) )\n",
    "    print(\"Columnas:\\n\", list(df.columns))\n",
    "    print(\"\\n\")\n",
    "    if(tipo==0):\n",
    "        print(\"Recuento valores columna sentimiento:\\n\", pd.value_counts(df['sentiment']))\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Función de limpieza de los tweets\n",
    "def limpiar_tweet(tweet):\n",
    "    # quitamos RT, @nombre_usuario, links y urls, hashtags, menciones, caracteres extraños o emoticonos\n",
    "    tweet = re.sub('  +', ' ', tweet)\n",
    "    # eliminar acentos\n",
    "    tweet = ''.join((c for c in unicodedata.normalize('NFD', tweet) if unicodedata.category(c) != 'Mn'))  \n",
    "    # convertir la repetición de una letra más de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "    # eliminar \"RT\", \"@usuario\", o los enlaces que es información que no sería útil analizar\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub('','',tweet).lower() \n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet)\n",
    "    tweet = re.sub(r'[0-9]', '', tweet) \n",
    "\n",
    "    return tweet \n",
    "\n",
    "\n",
    "# Función que le pasamos un Dataframe y nos lo devuelve con los datos preprocesados con la función anterior\n",
    "def limpiar_df(df):\n",
    "    df_clean = df.copy()\n",
    "    df_clean['text'] = df_clean['text'].apply(limpiar_tweet)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Función que genera los conjuntos de train y test a partir de un dataframe. Reparto 90% train - 10% test\n",
    "def split_df(df):\n",
    "    X_eng = df['text']\n",
    "    y_eng = df['sentiment']\n",
    "\n",
    "    X, X_test, y, y_test = train_test_split(X_eng, y_eng, test_size=0.1, random_state=42)\n",
    "\n",
    "    return X, X_test, y, y_test\n",
    "\n",
    "\n",
    "# Función que genera el pipeline y parámetros del modelo, y lo entrena.\n",
    "# Se centra la prueba en ver el resultado con distintos clasificadores para ver que tipo de algoritmo da los \n",
    "# mejores resultados. En cuanto al vectorizador se prueba tanto con CountVectorizer como con TDIDFVectorizer.\n",
    "# No se definen muchos otros parámetros por cuestión de tiempo y del elevado coste computacional en el caso de\n",
    "# jugar con muchos parámetros y varias posibilidades para cada uno.\n",
    "def generar_modelo(clasificador, stopwords, X_train, y_train):\n",
    "    # uso de tokenizer que es una buena práctica en el tratamiento de textos\n",
    "    tokenizer = TweetTokenizer().tokenize\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', None),\n",
    "        ('classifier', clasificador)]\n",
    "    )\n",
    "    \n",
    "    # también se prueba a usar o no stopwords, que serían palabras muy comunes en el lenguaje, que no tienen valor\n",
    "    # real a la hora de analizar el sentimiento de un texto y pueden no tenerse en cuenta.\n",
    "    params = {\n",
    "        'vectorizer': [CountVectorizer(binary=True,tokenizer=tokenizer),\\\n",
    "                       CountVectorizer(binary=False,tokenizer=tokenizer),\\\n",
    "                       TfidfVectorizer(use_idf=False, tokenizer=tokenizer),\\\n",
    "                       TfidfVectorizer(use_idf=True, tokenizer=tokenizer)],\n",
    "        'vectorizer__stop_words': [None, stopwords],\n",
    "    }\n",
    "\n",
    "    # uso también de StratifiedKFold que es otra buena práctica\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    modelo = GridSearchCV(pipeline, params, n_jobs=-1, cv=skf, refit='f1_weighted')\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    return modelo\n",
    "\n",
    "\n",
    "# Función que evalua el modelo que le pasamos y saca las métricas que se quieren observar\n",
    "def evaluar_modelos(modelo1, modelo2, x_eng_test, x_spa_test, y_eng_test, y_spa_test):\n",
    "    # pintamos los mejores parámetros de cada modelo\n",
    "    print(\"Mejores parámetros del modelo para datos en inglés: \\n\", modelo1.best_params_)\n",
    "    print(\"\\n\")\n",
    "    print(\"Mejores parámetros del modelo para datos en español: \\n\", modelo2.best_params_)\n",
    "\n",
    "    # se usan los modelos entrenados para predecir los datos de test\n",
    "    y_pred_eng = modelo1.predict(x_eng_test)\n",
    "    y_pred_spa = modelo2.predict(x_spa_test)\n",
    "\n",
    "    # pintamos las métricas de cada modelo\n",
    "    print(\"\\n\")\n",
    "    print(\"Métricas del modelo con datos en inglés:\\n\")\n",
    "    print(classification_report(y_eng_test, y_pred_eng, target_names=None))\n",
    "    print(\"\\n\")\n",
    "    print(\"Métricas del modelo con datos en español:\\n\")\n",
    "    print(classification_report(y_spa_test, y_pred_spa, target_names=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos almacenados en MongoDB, de datos sin sentimiento calculado, por si probamos a utilizarlos con el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 322364\tColumnas: 1\n",
      "\n",
      "Columnas:\n",
      " ['text']\n",
      "\n",
      "\n",
      "num_rows: 79036\tColumnas: 1\n",
      "\n",
      "Columnas:\n",
      " ['text']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mongo_english, df_mongo_spanish = carga_datos_mongo()\n",
    "\n",
    "visualizar_datos_df(df_mongo_english,1)\n",
    "visualizar_datos_df(df_mongo_spanish,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@EJFC26 Y Messi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @btsmoonchild64: These group photos deserve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@rehankkhanNDS Overacting *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pay day play day🤑🤑🤑🤑🤑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@pandaeyed1 Thank you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @liamyoung: Strange that Tony Blair has sud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hard work puts you where good luck can find you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @xCiphxr: When creative kids try playing co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @bonang_m: I’m working on one as we speak. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @akashbanerjee: After #PulwamaAttack, terro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                    @EJFC26 Y Messi\n",
       "1  RT @btsmoonchild64: These group photos deserve...\n",
       "2                        @rehankkhanNDS Overacting *\n",
       "3                              Pay day play day🤑🤑🤑🤑🤑\n",
       "4                             @pandaeyed1 Thank you!\n",
       "5  RT @liamyoung: Strange that Tony Blair has sud...\n",
       "6   Hard work puts you where good luck can find you.\n",
       "7  RT @xCiphxr: When creative kids try playing co...\n",
       "8  RT @bonang_m: I’m working on one as we speak. ...\n",
       "9  RT @akashbanerjee: After #PulwamaAttack, terro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mongo_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@EJFC26 Y Messi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @MBelenAlegre: Hermoso y sensual viernes ❣️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Foro_TV: Suman 47 muertos y 640 heridos po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @revistaetcetera: .@lopezobrador_ dice que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me retracto de mi respuesta , él gobierno se s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anda a saber si te esta eligiendo o sos lo úni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @JCTrujilloCPCCS: Hay que propiciar una Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@RicciuP Terraplanistas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @oriolguellipuig: Jeje jeje https://t.co/NU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas? no quiero ser part...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                    @EJFC26 Y Messi\n",
       "1     RT @MBelenAlegre: Hermoso y sensual viernes ❣️\n",
       "2  RT @Foro_TV: Suman 47 muertos y 640 heridos po...\n",
       "3  RT @revistaetcetera: .@lopezobrador_ dice que ...\n",
       "4  Me retracto de mi respuesta , él gobierno se s...\n",
       "5  Anda a saber si te esta eligiendo o sos lo úni...\n",
       "6  RT @JCTrujilloCPCCS: Hay que propiciar una Con...\n",
       "7                            @RicciuP Terraplanistas\n",
       "8  RT @oriolguellipuig: Jeje jeje https://t.co/NU...\n",
       "9  quienes son las veganarcas? no quiero ser part..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mongo_spanish.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos desde csv generados, con el sentimiento de cada texto anotado, tanto con datos en inglés como en español, para utilizarlos en el entrenamiento y testeo de los modelos y ver el mejor clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 70373\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    35092\n",
      "0    34714\n",
      "1      567\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 48658\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    26204\n",
      "0    19344\n",
      "1     3110\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv_english = carga_datos_csv('english_full')\n",
    "df_csv_spanish = carga_datos_csv('spanish_full')\n",
    "\n",
    "visualizar_datos_df(df_csv_english,0)\n",
    "visualizar_datos_df(df_csv_spanish,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128038</th>\n",
       "      <td>@BradshawPhotogr  seems i always end up at tx ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491756</th>\n",
       "      <td>@unitechy yeah, don't worry, you will!!! there...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470925</th>\n",
       "      <td>Excited that my email works reliably now with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491264</th>\n",
       "      <td>@MF213 yea that's the sad part</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836490</th>\n",
       "      <td>@modeladrienne you let me know! I think you to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371404</th>\n",
       "      <td>2155 - Well - Susan didn't make it ...   Runne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73350</th>\n",
       "      <td>..Before the cool runs out..Ima be trying my B...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166160</th>\n",
       "      <td>@dbdc LMAO not today sir sorry sir I did go ye...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070017</th>\n",
       "      <td>LOVES that lubbock is wet..its about time..no ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229521</th>\n",
       "      <td>I want a golden retriever puppy!! soooo cute!!...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment\n",
       "128038   @BradshawPhotogr  seems i always end up at tx ...          2\n",
       "491756   @unitechy yeah, don't worry, you will!!! there...          0\n",
       "470925   Excited that my email works reliably now with ...          0\n",
       "491264                     @MF213 yea that's the sad part           0\n",
       "836490   @modeladrienne you let me know! I think you to...          0\n",
       "371404   2155 - Well - Susan didn't make it ...   Runne...          0\n",
       "73350    ..Before the cool runs out..Ima be trying my B...          2\n",
       "1166160  @dbdc LMAO not today sir sorry sir I did go ye...          2\n",
       "1070017  LOVES that lubbock is wet..its about time..no ...          2\n",
       "229521   I want a golden retriever puppy!! soooo cute!!...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Desde el escaño. Todo listo para empezar #endi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bdías. EM no se ira de puente. Si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Un sistema económico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  @PauladeLasHeras No te libraras de ayudar me/n...          1\n",
       "1                          @marodriguezb Gracias MAR          2\n",
       "2  Off pensando en el regalito Sinde, la que se v...          0\n",
       "3  Conozco a alguien q es adicto al drama! Ja ja ...          2\n",
       "4  Toca @crackoviadeTV3 . Grabación dl especial N...          2\n",
       "5  Buen día todos! Lo primero mandar un abrazo gr...          2\n",
       "6  Desde el escaño. Todo listo para empezar #endi...          2\n",
       "7  Bdías. EM no se ira de puente. Si vosotros os ...          2\n",
       "8  Un sistema económico q recorta dinero para pre...          2\n",
       "9                  #programascambiados caca d ajuste          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv_spanish.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizamos una limpieza de los textos para eliminar caracteres que no tengan valor para el análisis a realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 70373\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    35092\n",
      "0    34714\n",
      "1      567\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 48658\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    26204\n",
      "0    19344\n",
      "1     3110\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean_english = limpiar_df(df_csv_english)\n",
    "df_clean_spanish = limpiar_df(df_csv_spanish)\n",
    "\n",
    "visualizar_datos_df(df_clean_english,0)\n",
    "visualizar_datos_df(df_clean_spanish,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128038</th>\n",
       "      <td>seems i always end up at tx schl  email lori...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491756</th>\n",
       "      <td>yeah  don t worry  you will   there s still ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470925</th>\n",
       "      <td>excited that my email works reliably now with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491264</th>\n",
       "      <td>yea that s the sad part</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836490</th>\n",
       "      <td>you let me know  i think you too busy for li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371404</th>\n",
       "      <td>well   susan didn t make it    runner up to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73350</th>\n",
       "      <td>before the cool runs out  ima be trying my b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166160</th>\n",
       "      <td>lmao not today sir sorry sir i did go yester...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070017</th>\n",
       "      <td>loves that lubbock is wet  its about time  no ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229521</th>\n",
       "      <td>i want a golden retriever puppy   soo cute   d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment\n",
       "128038     seems i always end up at tx schl  email lori...          2\n",
       "491756     yeah  don t worry  you will   there s still ...          0\n",
       "470925   excited that my email works reliably now with ...          0\n",
       "491264                            yea that s the sad part           0\n",
       "836490     you let me know  i think you too busy for li...          0\n",
       "371404      well   susan didn t make it    runner up to...          0\n",
       "73350      before the cool runs out  ima be trying my b...          2\n",
       "1166160    lmao not today sir sorry sir i did go yester...          2\n",
       "1070017  loves that lubbock is wet  its about time  no ...          2\n",
       "229521   i want a golden retriever puppy   soo cute   d...          0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no te libraras de ayudar me nos  besos y gra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gracias mar</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>off pensando en el regalito sinde  la que se v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conozco a alguien q es adicto al drama  ja ja ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toca     grabacion dl especial navideno  mari ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>buen dia todos  lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>desde el escano  todo listo para empezar endia...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bdias  em no se ira de puente  si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>un sistema economico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0    no te libraras de ayudar me nos  besos y gra...          1\n",
       "1                                        gracias mar          2\n",
       "2  off pensando en el regalito sinde  la que se v...          0\n",
       "3  conozco a alguien q es adicto al drama  ja ja ...          2\n",
       "4  toca     grabacion dl especial navideno  mari ...          2\n",
       "5  buen dia todos  lo primero mandar un abrazo gr...          2\n",
       "6  desde el escano  todo listo para empezar endia...          2\n",
       "7  bdias  em no se ira de puente  si vosotros os ...          2\n",
       "8  un sistema economico q recorta dinero para pre...          2\n",
       "9                   programascambiados caca d ajuste          0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_spanish.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separamos los datos tanto en inglés como en español en conjuntos de entrenamiento y de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eng_train, X_eng_test, y_eng_train, y_eng_test = split_df(df_clean_english)\n",
    "X_spa_train, X_spa_test, y_spa_train, y_spa_test = split_df(df_clean_spanish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba modelo final.\n",
    "A la hora de aplicar machine learning para abordar problemas como éste del análisis de sentimiento, hay dos tipos de aprendizaje: supervisado y no supervisado.\n",
    "En este trabajo optamos por aplicar aprendizaje supervisado, que son métodos basados en algoritmos de aprendizaje automático que necesitan para entrenarse un conjunto de datos ya etiquetados.\n",
    "\n",
    "Vamos a probar con el modelo final y sus parámetros que probamos anteriormente, con 4 algoritmos de clasificación distintos, usando los siguientes: Naive Bayes, Máquinas de vector soporte, K vecinos más cercanos, y árboles de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos las stopwords para cada idioma\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los modelos para datos en inglés y en español\n",
    "clasificador = MultinomialNB()\n",
    "\n",
    "model_eng_naive = generar_modelo(clasificador, english_stopwords, X_eng_train, y_eng_train)\n",
    "model_spa_naive = generar_modelo(clasificador, spanish_stopwords, X_spa_train, y_spa_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros del modelo para datos en inglés: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a1ebf82b0>>,\n",
      "        use_idf=False, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores parámetros del modelo para datos en español: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a1ebf8358>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en inglés:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78      3497\n",
      "           1       0.00      0.00      0.00        62\n",
      "           2       0.78      0.73      0.76      3479\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      7038\n",
      "   macro avg       0.51      0.51      0.51      7038\n",
      "weighted avg       0.76      0.76      0.76      7038\n",
      "\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.80      1881\n",
      "           1       0.57      0.03      0.05       310\n",
      "           2       0.87      0.86      0.86      2675\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      4866\n",
      "   macro avg       0.73      0.59      0.57      4866\n",
      "weighted avg       0.80      0.81      0.79      4866\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos\n",
    "evaluar_modelos(model_eng_naive, model_spa_naive, X_eng_test, X_spa_test, y_eng_test, y_spa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo K-vecinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los modelos para datos en inglés y en español\n",
    "clasificador = KNeighborsClassifier()\n",
    "\n",
    "model_eng_vecinos = generar_modelo(clasificador, english_stopwords, X_eng_train, y_eng_train)\n",
    "model_spa_vecinos = generar_modelo(clasificador, spanish_stopwords, X_spa_train, y_spa_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros del modelo para datos en inglés: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a2471b160>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores parámetros del modelo para datos en español: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a2471b208>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en inglés:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67      3497\n",
      "           1       0.60      0.05      0.09        62\n",
      "           2       0.66      0.69      0.67      3479\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      7038\n",
      "   macro avg       0.65      0.46      0.48      7038\n",
      "weighted avg       0.67      0.67      0.67      7038\n",
      "\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.40      0.50      1881\n",
      "           1       0.23      0.03      0.06       310\n",
      "           2       0.64      0.89      0.75      2675\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      4866\n",
      "   macro avg       0.51      0.44      0.43      4866\n",
      "weighted avg       0.63      0.65      0.61      4866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos\n",
    "evaluar_modelos(model_eng_vecinos, model_spa_vecinos, X_eng_test, X_spa_test, y_eng_test, y_spa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo Máquinas vector soporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los modelos para datos en inglés y en español\n",
    "clasificador = LinearSVC()\n",
    "\n",
    "model_eng_svms = generar_modelo(clasificador, english_stopwords, X_eng_train, y_eng_train)\n",
    "model_spa_svms = generar_modelo(clasificador, spanish_stopwords, X_spa_train, y_spa_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros del modelo para datos en inglés: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a28d09710>>,\n",
      "        use_idf=False, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores parámetros del modelo para datos en español: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a23335358>>,\n",
      "        use_idf=False, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en inglés:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      3497\n",
      "           1       0.75      0.10      0.17        62\n",
      "           2       0.76      0.78      0.77      3479\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      7038\n",
      "   macro avg       0.76      0.55      0.57      7038\n",
      "weighted avg       0.77      0.77      0.77      7038\n",
      "\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      1881\n",
      "           1       0.55      0.10      0.16       310\n",
      "           2       0.86      0.91      0.89      2675\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      4866\n",
      "   macro avg       0.74      0.62      0.63      4866\n",
      "weighted avg       0.82      0.84      0.82      4866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos\n",
    "evaluar_modelos(model_eng_svms, model_spa_svms, X_eng_test, X_spa_test, y_eng_test, y_spa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los modelos para datos en inglés y en español\n",
    "clasificador = DecisionTreeClassifier()\n",
    "\n",
    "model_eng_tree = generar_modelo(clasificador, english_stopwords, X_eng_train, y_eng_train)\n",
    "model_spa_tree = generar_modelo(clasificador, spanish_stopwords, X_spa_train, y_spa_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros del modelo para datos en inglés: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a2531f860>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores parámetros del modelo para datos en español: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde'...tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened'],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a291da630>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']}\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en inglés:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70      3497\n",
      "           1       0.32      0.16      0.22        62\n",
      "           2       0.69      0.69      0.69      3479\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      7038\n",
      "   macro avg       0.57      0.52      0.54      7038\n",
      "weighted avg       0.69      0.69      0.69      7038\n",
      "\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70      1881\n",
      "           1       0.24      0.13      0.17       310\n",
      "           2       0.79      0.83      0.81      2675\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      4866\n",
      "   macro avg       0.58      0.56      0.56      4866\n",
      "weighted avg       0.72      0.74      0.73      4866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos\n",
    "evaluar_modelos(model_eng_tree, model_spa_tree, X_eng_test, X_spa_test, y_eng_test, y_spa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados del modelo tras probarlo con los 4 algoritmos de clasificación.\n",
    "\n",
    "Los mejores resultados vuelven a obtenerse de forma general, teniendo en cuenta las clases en conjunto y las distintas métricas que se obtienen en cada evaluación, con las máquinas vector soporte, en este caso el algoritmo LinearSVC.\n",
    "\n",
    "Los resultados suelen rondar aproximadamente sobre el 75% con los datos en Inglés, y el 80% con los datos en Español, llegando a sus máximos valores con LinearSVC como hemos dicho, donde se alcanza un 77% y un 82% respectivamente.\n",
    "\n",
    "Como ya se dijo anteriormente en el trabajo, se podrían probar muchos parámetros más, con distintos valores y combinaciones, para optimizar el modelo y poder subir los % para cada algoritmo, pero requiere un tiempo y un coste computacional que no se puede asumir.\n",
    "\n",
    "Nos quedamos con el clasificador LinearSVC y un modelo como el probado aquí, para aplicarlo en streaming a los tweets que se vayan recogiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a cargar los csv que se generaron en inglés y español con el mismo número de registros positivos y negativos en cada dataset, y los que tienen el mismo número de registros positivos, negativos y neutros por dataset.\n",
    "\n",
    "Probaremos el modelo anterior con el clasificador que mejor resultado se ha obtenido con estos datasets, para ver como influye el reparto de clases por dataset, y el número de registros, además de si hay 2 o 3 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 42000\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    14000\n",
      "1    14000\n",
      "0    14000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 9000\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    3000\n",
      "1    3000\n",
      "0    3000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 80000\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    40000\n",
      "0    40000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 38000\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    19000\n",
      "0    19000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cargamos los 4 datasets\n",
    "df_csv_english_neutro = carga_datos_csv('english_neutro')\n",
    "df_csv_spanish_neutro = carga_datos_csv('spanish_neutro')\n",
    "df_csv_english_noNeutro = carga_datos_csv('english_noNeutro')\n",
    "df_csv_spanish_noNeutro = carga_datos_csv('spanish_noNeutro')\n",
    "\n",
    "visualizar_datos_df(df_csv_english_neutro,0)\n",
    "visualizar_datos_df(df_csv_spanish_neutro,0)\n",
    "visualizar_datos_df(df_csv_english_noNeutro,0)\n",
    "visualizar_datos_df(df_csv_spanish_noNeutro,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpiamos los datos\n",
    "df_clean_english_neutro = limpiar_df(df_csv_english_neutro)\n",
    "df_clean_spanish_neutro = limpiar_df(df_csv_spanish_neutro)\n",
    "df_clean_english_noNeutro = limpiar_df(df_csv_english_noNeutro)\n",
    "df_clean_spanish_noNeutro = limpiar_df(df_csv_spanish_noNeutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos el reparto de datos para entrenamiento y testing\n",
    "X_eng_train_neu, X_eng_test_neu, y_eng_train_neu, y_eng_test_neu = split_df(df_clean_english_neutro)\n",
    "X_spa_train_neu, X_spa_test_neu, y_spa_train_neu, y_spa_test_neu = split_df(df_clean_spanish_neutro)\n",
    "X_eng_train_noNeu, X_eng_test_noNeu, y_eng_train_noNeu, y_eng_test_noNeu = split_df(df_clean_english_noNeutro)\n",
    "X_spa_train_noNeu, X_spa_test_noNeu, y_spa_train_noNeu, y_spa_test_noNeu = split_df(df_clean_spanish_noNeutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos el modelo con los 4 datasets anteriores\n",
    "clasificador = LinearSVC()\n",
    "\n",
    "model_eng_svms_neu = generar_modelo(clasificador, english_stopwords, X_eng_train_neu, y_eng_train_neu)\n",
    "model_spa_svms_neu = generar_modelo(clasificador, spanish_stopwords, X_spa_train_neu, y_spa_train_neu)\n",
    "model_eng_svms_noNeu = generar_modelo(clasificador, english_stopwords, X_eng_train_noNeu, y_eng_train_noNeu)\n",
    "model_spa_svms_noNeu = generar_modelo(clasificador, spanish_stopwords, X_spa_train_noNeu, y_spa_train_noNeu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros del modelo para datos en inglés: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a3a23f470>>,\n",
      "        use_idf=True, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores parámetros del modelo para datos en español: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a3a23f518>>,\n",
      "        use_idf=True, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en inglés:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.66      1398\n",
      "           1       0.67      0.68      0.68      1375\n",
      "           2       0.65      0.62      0.63      1427\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      4200\n",
      "   macro avg       0.66      0.66      0.66      4200\n",
      "weighted avg       0.66      0.66      0.66      4200\n",
      "\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       282\n",
      "           1       0.72      0.79      0.76       302\n",
      "           2       0.83      0.79      0.81       316\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       900\n",
      "   macro avg       0.77      0.77      0.77       900\n",
      "weighted avg       0.77      0.77      0.77       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos con 3 clases: positivos, negativos y neutros\n",
    "evaluar_modelos(model_eng_svms_neu, model_spa_svms_neu, X_eng_test_neu, X_spa_test_neu,\\\n",
    "                y_eng_test_neu, y_spa_test_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros del modelo para datos en inglés: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a3a23f7b8>>,\n",
      "        use_idf=False, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores parámetros del modelo para datos en español: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a3cdd0f98>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en inglés:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78      4025\n",
      "           2       0.78      0.79      0.78      3975\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      8000\n",
      "   macro avg       0.78      0.78      0.78      8000\n",
      "weighted avg       0.78      0.78      0.78      8000\n",
      "\n",
      "\n",
      "\n",
      "Métricas del modelo con datos en español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      1902\n",
      "           2       0.94      0.94      0.94      1898\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      3800\n",
      "   macro avg       0.94      0.94      0.94      3800\n",
      "weighted avg       0.94      0.94      0.94      3800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos con 2 clases: positivos y negativos\n",
    "evaluar_modelos(model_eng_svms_noNeu, model_spa_svms_noNeu, X_eng_test_noNeu, X_spa_test_noNeu,\\\n",
    "                y_eng_test_noNeu, y_spa_test_noNeu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen los siguientes resultados:\n",
    "- Modelo entrenado y testeado con los datasets con 3 clases y equilibrados en el número de registros por clase: 66% con datos en inglés, y 77% con datos en español.\n",
    "- Modelo entrenado y testeado con los datasets con 2 clases (positivo y negativo) y equilibrados en el número de registros por clase: 78% y 94%, para inglés y español respectivamente.\n",
    "\n",
    "Como era de suponer se obtienen mejores resultados sólo con 2 clases, y se sigue manteniendo la tendencia de mejor clasificación con datos en español que en inglés. De todos modos también hay que pensar que los datasets son más pequeños y con menor número de datos los resultados pueden ser menos fiables.\n",
    "Además la realidad es que al menos deben tener en cuenta 3 clases, ya que como mínimo siempre se deben clasificar los sentimientos en negativo, neutro o positivo.\n",
    "\n",
    "Se puede apreciar como entrenando con datasets equilibrados entre sus distintas clases, se obtienen resultados buenos y similares para cada clase, no como ocurría al entrenar y testear con un dataset desequilibrado, que la clase con menos registros obtiene unos pobres resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba a predecir el sentimiento de los datos cargados desde MongoDB con el modelo con mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos cargados desde MongoDB\n",
    "# limpiamos los textos\n",
    "df_clean_mongo_english = limpiar_df(df_mongo_english)\n",
    "df_clean_mongo_spanish = limpiar_df(df_mongo_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y messi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these group photos deserve more attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overacting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pay day play day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>strange that tony blair has suddenly become...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hard work puts you where good luck can find you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when creative kids try playing comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m  i m working on one as we speak  thank you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>after pulwamaattack  terrorists scored a bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                            y messi\n",
       "1         these group photos deserve more attention \n",
       "2                                       overacting  \n",
       "3                                 pay day play day  \n",
       "4                                         thank you \n",
       "5     strange that tony blair has suddenly become...\n",
       "6   hard work puts you where good luck can find you \n",
       "7               when creative kids try playing comp \n",
       "8    m  i m working on one as we speak  thank you...\n",
       "9     after pulwamaattack  terrorists scored a bi..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_mongo_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y messi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hermoso y sensual viernes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tv  suman  muertos y  heridos por explosion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dice que fue tan grande la gloria de be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me retracto de mi respuesta   el gobierno se s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anda a saber si te esta eligiendo o sos lo uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hay que propiciar una consulta popular para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>terraplanistas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jeje jeje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas  no quiero ser part...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                            y messi\n",
       "1                        hermoso y sensual viernes  \n",
       "2    tv  suman  muertos y  heridos por explosion ...\n",
       "3         dice que fue tan grande la gloria de be...\n",
       "4  me retracto de mi respuesta   el gobierno se s...\n",
       "5  anda a saber si te esta eligiendo o sos lo uni...\n",
       "6     hay que propiciar una consulta popular para...\n",
       "7                                     terraplanistas\n",
       "8                                         jeje jeje \n",
       "9  quienes son las veganarcas  no quiero ser part..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_mongo_spanish.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos usando el modelo de SVMs que ha obtenido los mejores resultados\n",
    "y_pred_mongo_eng = model_eng_svms.predict(df_clean_mongo_english.text)\n",
    "y_pred_mongo_spa = model_spa_svms.predict(df_clean_mongo_spanish.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos una copia de los dataframes con los textos limpios, y les añadimos la columna sentimiento con los valores\n",
    "# que se han predecido con el modelo.\n",
    "df_predict_english = df_clean_mongo_english.copy()\n",
    "df_predict_spanish = df_clean_mongo_spanish.copy()\n",
    "\n",
    "df_predict_english['sentiment'] = y_pred_mongo_eng\n",
    "df_predict_spanish['sentiment'] = y_pred_mongo_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y messi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these group photos deserve more attention</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overacting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pay day play day</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>strange that tony blair has suddenly become...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hard work puts you where good luck can find you</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when creative kids try playing comp</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m  i m working on one as we speak  thank you...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>after pulwamaattack  terrorists scored a bi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i am people</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vintage  piece hammered copper pot large bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>beyonce  world stop   world</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>no  it just needs money and connections  your ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the upcoming album from   boasts  new songs...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i see ya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>it s time to run our own country    retweet...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>this works  i m not even gonna front  ret...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>how many followers you want     retweet   f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>do me a favor right  follow this absolute top ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>parkland shooting survivor sydney aiello ta...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>trying to prove a point  lets settle this o...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nwofra why can t the feminist follow yo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>be among the top followers retweeting thi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>love you</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>allergies are already acting up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>activist asks if russia is living in the  ston...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>am shocked to find my name on the poster of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cronos group is set to report earnings tuesday...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "0                                             y messi          2\n",
       "1          these group photos deserve more attention           2\n",
       "2                                        overacting            2\n",
       "3                                  pay day play day            2\n",
       "4                                          thank you           2\n",
       "5      strange that tony blair has suddenly become...          0\n",
       "6    hard work puts you where good luck can find you           2\n",
       "7                when creative kids try playing comp           2\n",
       "8     m  i m working on one as we speak  thank you...          2\n",
       "9      after pulwamaattack  terrorists scored a bi...          2\n",
       "10                                       i am people           2\n",
       "11     vintage  piece hammered copper pot large bo...          0\n",
       "12                     beyonce  world stop   world             2\n",
       "13  no  it just needs money and connections  your ...          0\n",
       "14     the upcoming album from   boasts  new songs...          2\n",
       "15                                         i see ya            2\n",
       "16     it s time to run our own country    retweet...          2\n",
       "17       this works  i m not even gonna front  ret...          2\n",
       "18     how many followers you want     retweet   f...          2\n",
       "19  do me a favor right  follow this absolute top ...          2\n",
       "20     parkland shooting survivor sydney aiello ta...          2\n",
       "21     trying to prove a point  lets settle this o...          2\n",
       "22         nwofra why can t the feminist follow yo...          2\n",
       "23       be among the top followers retweeting thi...          2\n",
       "24                                          love you           2\n",
       "25                                                ...          0\n",
       "26                    allergies are already acting up          0\n",
       "27  activist asks if russia is living in the  ston...          2\n",
       "28     am shocked to find my name on the poster of...          2\n",
       "29  cronos group is set to report earnings tuesday...          2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict_english.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y messi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hermoso y sensual viernes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tv  suman  muertos y  heridos por explosion ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dice que fue tan grande la gloria de be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me retracto de mi respuesta   el gobierno se s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anda a saber si te esta eligiendo o sos lo uni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hay que propiciar una consulta popular para...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>terraplanistas</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jeje jeje</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas  no quiero ser part...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mas dias con este clima</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mood del fandom despues de haber recaudado ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>yo te quiero pa  mi no te quiero pa  mas na...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>me llego a poner eso y parezco un arrollado...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>un gran saludo a mis camaradas patriotas   ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>yo tambien vi a una chica army en la u pero ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cultura el festival de comic europeo de ubeda ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>solo entro en twitter por ti</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cas  que tan feo debes ser  para preferir un...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>portararmasjelo    estados unidos es el uni...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lastima  gracias</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>profe  hoy   de marzo de   las cortes espano...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>una falta de respeto q yo siga viva</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>te acostas con las patas asi en mi cama y t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vieron que taehyung anda super agresivo ult...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tara</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>preguntas fueron</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pero en cataluna quien cono trabaja  la em...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>amigos un amigo que era super socialista empez...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>un album con sonidos del oceano hechos con...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "0                                             y messi          2\n",
       "1                         hermoso y sensual viernes            2\n",
       "2     tv  suman  muertos y  heridos por explosion ...          0\n",
       "3          dice que fue tan grande la gloria de be...          0\n",
       "4   me retracto de mi respuesta   el gobierno se s...          0\n",
       "5   anda a saber si te esta eligiendo o sos lo uni...          0\n",
       "6      hay que propiciar una consulta popular para...          2\n",
       "7                                      terraplanistas          2\n",
       "8                                          jeje jeje           2\n",
       "9   quienes son las veganarcas  no quiero ser part...          0\n",
       "10                           mas dias con este clima           2\n",
       "11     mood del fandom despues de haber recaudado ...          2\n",
       "12     yo te quiero pa  mi no te quiero pa  mas na...          2\n",
       "13     me llego a poner eso y parezco un arrollado...          2\n",
       "14     un gran saludo a mis camaradas patriotas   ...          2\n",
       "15    yo tambien vi a una chica army en la u pero ...          2\n",
       "16  cultura el festival de comic europeo de ubeda ...          2\n",
       "17                       solo entro en twitter por ti          2\n",
       "18    cas  que tan feo debes ser  para preferir un...          0\n",
       "19     portararmasjelo    estados unidos es el uni...          2\n",
       "20                                  lastima  gracias           2\n",
       "21    profe  hoy   de marzo de   las cortes espano...          2\n",
       "22               una falta de respeto q yo siga viva           2\n",
       "23     te acostas con las patas asi en mi cama y t...          2\n",
       "24     vieron que taehyung anda super agresivo ult...          0\n",
       "25                                             tara            2\n",
       "26                                   preguntas fueron          0\n",
       "27      pero en cataluna quien cono trabaja  la em...          0\n",
       "28  amigos un amigo que era super socialista empez...          2\n",
       "29      un album con sonidos del oceano hechos con...          2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict_spanish.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan los primeros 30 tweets con el sentimiento predecido por el modelo, y se pueden ver varios ejemplos de un funcionamiento aceptablemente correcto.\n",
    "\n",
    "También se puede ver como se predicen más etiquetas con valor positivo o 2, que del resto. Esto es debido a que si se ve al inicio el reparto de valores en los datasets usados para entrenar los modelos, el valor positivo es el que más número de datos tiene. Además, no se ve ningún valor neutral, ya que los datasets tienen muy pocos valores neutrales, y por tanto el modelo no va a predecir fácilmente esta etiqueta.\n",
    "\n",
    "Para obtener un modelo con el mejor funcionamiento posible, haría falta que estuviera totalmente balanceado entre todas sus clases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
