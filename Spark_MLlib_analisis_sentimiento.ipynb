{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Spark Mlib y prueba del modelo con distintos algoritmos de clasificación para hacer el análisis de sentimiento de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports y configuraciones necesarias\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# cargamos las stopwords para cada idioma\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "conf = (SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"twitter\")\\\n",
    "          .master(\"spark://MacBook-Pro-de-Jose.local:7077\")\\\n",
    "          .config(\"spark.io.compression.codec\", \"snappy\")\\\n",
    "          .getOrCreate())  \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones necesarias para la preparación de los datos y el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de carga de datos desde MongoDB de los datos almacenados desde Apache Nifi, tanto en inglés como español\n",
    "def carga_datos_mongo():\n",
    "    # conectamos con las tablas de Mongo desde donde cargamos los datos\n",
    "    df_mongo_english = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_english\").load()\n",
    "    df_mongo_spanish = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_spanish\").load()\n",
    "    \n",
    "    # nos quedamos solo con el atributo texto que es la información que nos interesa de cada dataset\n",
    "    df_text_english = df_mongo_english[['text']]\n",
    "    df_text_spanish = df_mongo_spanish[['text']]\n",
    "    \n",
    "    return df_text_english, df_text_spanish\n",
    "\n",
    "\n",
    "# Función de carga de datos desde los csv generados con anterioridad con registros ya con el sentimiento anotado\n",
    "def carga_datos_csv(csv):\n",
    "    schema_csv = StructType([ \n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"sentiment\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    # cargamos los csv\n",
    "    if(csv=='english_full'): fichero = './data/df_result_english.csv'\n",
    "    elif(csv=='spanish_full'): fichero = './data/df_result_spanish.csv'\n",
    "    elif(csv=='english_neutro'): fichero = './data/df_result_english_neutral.csv'\n",
    "    elif(csv=='spanish_neutro'): fichero = './data/df_result_spanish_neutral.csv'\n",
    "    elif(csv=='english_noNeutro'): fichero = './data/df_result_english_noNeutral.csv'\n",
    "    elif(csv=='spanish_noNeutro'): fichero = './data/df_result_spanish_noNeutral.csv'\n",
    "    \n",
    "    df_csv = sqlContext.\\\n",
    "        read.format(\"com.databricks.spark.csv\").\\\n",
    "        option(\"header\", \"true\").\\\n",
    "        option(\"inferschema\", \"true\").\\\n",
    "        option(\"mode\", \"DROPMALFORMED\").\\\n",
    "        schema(schema_csv).\\\n",
    "        load(fichero).\\\n",
    "        cache()\n",
    "    \n",
    "    return df_csv\n",
    "\n",
    "\n",
    "# Funciones de visualización de DFs para ver las columnas, el número de registros o dimensiones, y el recuento\n",
    "# de valores del atributo sentimiento en caso de tener la columna.\n",
    "def visualizar_datos_csv(df):\n",
    "    print(\"Columnas del dataframe: \", df.columns)\n",
    "    print(\"Numero de registros = %d\" % df.count())\n",
    "    print(\"\\n\")\n",
    "    print(df.limit(10).toPandas())\n",
    "    print(\"\\n\")\n",
    "    recuento_sentiment = df.select('sentiment').groupBy(\"sentiment\").count().show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "def visualizar_datos_mongo(df):\n",
    "    df_pandas = df.toPandas()\n",
    "    print(\"num_rows: %d\\tColumnas: %d\\n\" % (df_pandas.shape[0], df_pandas.shape[1]) )\n",
    "    print(\"Columnas:\\n\", list(df_pandas.columns))\n",
    "    print(df_pandas.head(10))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# función para eliminar palabras que no queramos analizar\n",
    "def eliminar_stopwords(texto, palabras_eliminar):\n",
    "    tok = nltk.tokenize\n",
    "    palabras = tok.word_tokenize(texto)\n",
    "        \n",
    "    palabras_salida = []\n",
    "        \n",
    "    for palabra in palabras:\n",
    "        if palabra not in palabras_eliminar:\n",
    "            palabras_salida.append(palabra)\n",
    "        \n",
    "    salida = \"\"\n",
    "    for i in range(len(palabras_salida)):\n",
    "        if palabras_salida[i] in string.punctuation:\n",
    "            salida = salida.strip()+palabras_salida[i] + \" \"\n",
    "        else:\n",
    "            salida += palabras_salida[i] + \" \"\n",
    "\n",
    "    return salida\n",
    "\n",
    "\n",
    "# Funciones de limpieza de los tweets\n",
    "def limpieza_tweets_spanish(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    \n",
    "    # convertir la repetición de una letra más de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "    # eliminar acentos\n",
    "    tweet = [''.join((c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')) for s in tweet]\n",
    "        \n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\º\\'\\’\\‘\\|()%/\\\"{}@;:+\\[\\]\\–\\”\\…\\“\\】\\【=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, spanish_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "# a los datos en inglés le aplicamos sus stopwords correspondientes y no les quitamos los acentos\n",
    "def limpieza_tweets_english(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    \n",
    "    # convertir la repetición de una letra más de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "\n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\º\\'\\’\\‘\\|()%/\\\"{}@;:+\\[\\]\\–\\”\\…\\“\\】\\【=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, english_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "# Función de preprocesado de los datos\n",
    "def preprocesado_dataframe(df1, df2):\n",
    "    # primero usamos tokenizer y vamos a partir los tweets por palabras\n",
    "    tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"token\")\n",
    "\n",
    "    df_tokens_english = tokenizer.transform(df1)\n",
    "    df_tokens_spanish = tokenizer.transform(df2)  \n",
    "    \n",
    "    # usamos las funciones de limpieza y preprocesado con ambos DFs ya con los textos divididos en tokens\n",
    "    limpiezaUDF_english = F.udf(limpieza_tweets_english, ArrayType(StringType()))\n",
    "    limpiezaUDF_spanish = F.udf(limpieza_tweets_spanish, ArrayType(StringType()))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.withColumn(\"tokens_clean\", limpiezaUDF_english(df_tokens_english[\"token\"]))\n",
    "    df_tokens_spanish = df_tokens_spanish.withColumn(\"tokens_clean\", limpiezaUDF_spanish(df_tokens_spanish[\"token\"]))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.drop(\"token\")\n",
    "    df_tokens_spanish = df_tokens_spanish.drop(\"token\")\n",
    "\n",
    "    df_tokens_english_clean = df_tokens_english.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    df_tokens_spanish_clean = df_tokens_spanish.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    \n",
    "    return df_tokens_english_clean, df_tokens_spanish_clean\n",
    "\n",
    "\n",
    "# Función evaluación de modelo\n",
    "def evaluar_modelo(metric, predCol, labelCol, pred1, pred2):\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric, predictionCol=predCol, labelCol=labelCol)\n",
    "\n",
    "    eval_eng_clean = evaluator.evaluate(pred1)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Medidas de rendimiento datos en inglés\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"F1-score = %f\" % eval_eng_clean)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    eval_spa_clean = evaluator.evaluate(pred2)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Medidas de rendimiento datos en español\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"F1-score = %f\" % eval_spa_clean)\n",
    "    print(\"\\n\") \n",
    "\n",
    "\n",
    "# Función para ver los mejores parámetros de un modelo al usar cross-validation\n",
    "def mejores_parametros(model1, model2):\n",
    "    bestPipeline_eng = model1.bestModel\n",
    "    bestPipeline_spa = model2.bestModel\n",
    "\n",
    "    bestVectorizer_eng = bestPipeline_eng.stages[0]\n",
    "    bestVectorizer_spa = bestPipeline_spa.stages[0]\n",
    "\n",
    "    bestParamsVect_eng = bestVectorizer_eng.extractParamMap()\n",
    "    print (\"Vectorizer parameters datos inglés:\")\n",
    "    for k in bestParamsVect_eng.keys():\n",
    "        print (\"  \", k, bestParamsVect_eng[k])\n",
    "    \n",
    "    bestParamsVect_spa = bestVectorizer_spa.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print (\"Vectorizer parameters datos español:\")\n",
    "    for k in bestParamsVect_spa.keys():\n",
    "        print (\"  \", k, bestParamsVect_spa[k])\n",
    "\n",
    "    bestModel_eng = bestPipeline_eng.stages[2]\n",
    "    bestModel_spa = bestPipeline_spa.stages[2]\n",
    "\n",
    "    bestParamsModel_eng = bestModel_eng.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print(\"Model parameters datos inglés:\")\n",
    "    for k in bestParamsModel_eng.keys():\n",
    "        print(\"  \", k, bestParamsModel_eng[k])\n",
    "\n",
    "    bestParamsModel_spa = bestModel_spa.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print(\"Model parameters datos español:\")\n",
    "    for k in bestParamsModel_spa.keys():\n",
    "        print(\"  \", k, bestParamsModel_spa[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos los datos que pueden ser necesarios tanto para entrenar los modelos como para testearlos.\n",
    "\n",
    "Los datos almacenados en MongoDB que no tienen sentimiento calculado, se pueden utilizar para probar luego el modelo a predecirlo.\n",
    "\n",
    "Los datos que se cargan desde los csv generados con datos en inglés y español ya con sentimiento calculado, se van a usar para entrenar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datos de MongoDB que podrían usarse para probar el modelo final\n",
    "df_mongo_english, df_mongo_spanish = carga_datos_mongo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 609133\tColumnas: 1\n",
      "\n",
      "Columnas:\n",
      " ['text']\n",
      "                                                text\n",
      "0                                    @EJFC26 Y Messi\n",
      "1  RT @btsmoonchild64: These group photos deserve...\n",
      "2                        @rehankkhanNDS Overacting *\n",
      "3                              Pay day play day🤑🤑🤑🤑🤑\n",
      "4                             @pandaeyed1 Thank you!\n",
      "5  RT @liamyoung: Strange that Tony Blair has sud...\n",
      "6   Hard work puts you where good luck can find you.\n",
      "7  RT @xCiphxr: When creative kids try playing co...\n",
      "8  RT @bonang_m: I’m working on one as we speak. ...\n",
      "9  RT @akashbanerjee: After #PulwamaAttack, terro...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_mongo(df_mongo_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 153619\tColumnas: 1\n",
      "\n",
      "Columnas:\n",
      " ['text']\n",
      "                                                text\n",
      "0                                    @EJFC26 Y Messi\n",
      "1     RT @MBelenAlegre: Hermoso y sensual viernes ❣️\n",
      "2  RT @Foro_TV: Suman 47 muertos y 640 heridos po...\n",
      "3  RT @revistaetcetera: .@lopezobrador_ dice que ...\n",
      "4  Me retracto de mi respuesta , él gobierno se s...\n",
      "5  Anda a saber si te esta eligiendo o sos lo úni...\n",
      "6  RT @JCTrujilloCPCCS: Hay que propiciar una Con...\n",
      "7                            @RicciuP Terraplanistas\n",
      "8  RT @oriolguellipuig: Jeje jeje https://t.co/NU...\n",
      "9  quienes son las veganarcas? no quiero ser part...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_mongo(df_mongo_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datos almacenados en ficheros csv con el sentimiento etiquetado para entrenar modelos\n",
    "df_csv_english_full = carga_datos_csv('english_full')\n",
    "df_csv_spanish_full = carga_datos_csv('spanish_full')\n",
    "df_csv_english_neutro = carga_datos_csv('english_neutro')\n",
    "df_csv_spanish_neutro = carga_datos_csv('spanish_neutro')\n",
    "df_csv_english_noNeutro = carga_datos_csv('english_noNeutro')\n",
    "df_csv_spanish_noNeutro = carga_datos_csv('spanish_noNeutro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 1759091\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0                @VirginAmerica What @dhepburn said.          1\n",
      "1  @VirginAmerica plus you've added commercials t...          2\n",
      "2  @VirginAmerica I didn't today... Must mean I n...          1\n",
      "3  \"@VirginAmerica it's really aggressive to blas...          0\n",
      "4  @VirginAmerica and it's a really big bad thing...          0\n",
      "5    it's really the only bad thing about flying VA\"          0\n",
      "6  @VirginAmerica yes, nearly every time I fly VX...          2\n",
      "7  @VirginAmerica Really missed a prime opportuni...          1\n",
      "8    @virginamerica Well, I didn't…but NOW I DO! :-D          2\n",
      "9  @VirginAmerica it was amazing, and arrived an ...          2\n",
      "\n",
      "\n",
      "+---------+------+\n",
      "|sentiment| count|\n",
      "+---------+------+\n",
      "|        1| 14424|\n",
      "|        2|872815|\n",
      "|        0|871852|\n",
      "+---------+------+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_english_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cogemos un sample del DF con datos en inglés ya que tiene muchos datos y da problemas de cómputo y tiempos\n",
    "df_csv_english_sample = df_csv_english_full.sample(False, 0.1, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 175818\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @VirginAmerica amazing to me that we can't get...          0\n",
      "1  @VirginAmerica View of downtown Los Angeles, t...          2\n",
      "2  @VirginAmerica plz help me win my bid upgrade ...          1\n",
      "3  @VirginAmerica I'm #elevategold for a good rea...          2\n",
      "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1\n",
      "5  @VirginAmerica I’m having trouble adding this ...          0\n",
      "6  @VirginAmerica you have the absolute best team...          2\n",
      "7  @VirginAmerica has flight number 276 from SFO ...          1\n",
      "8  @VirginAmerica Another delayed flight? #liking...          0\n",
      "9  @VirginAmerica Can you find us a flt out of LA...          1\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 1372|\n",
      "|        2|87372|\n",
      "|        0|87074|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_english_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 46787\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @PauladeLasHeras No te libraras de ayudar me/n...          1\n",
      "1                          @marodriguezb Gracias MAR          2\n",
      "2  Off pensando en el regalito Sinde, la que se v...          0\n",
      "3  Conozco a alguien q es adicto al drama! Ja ja ...          2\n",
      "4  Toca @crackoviadeTV3 . Grabación dl especial N...          2\n",
      "5  Buen día todos! Lo primero mandar un abrazo gr...          2\n",
      "6  Desde el escaño. Todo listo para empezar #endi...          2\n",
      "7  Bdías. EM no se ira de puente. Si vosotros os ...          2\n",
      "8  Un sistema económico q recorta dinero para pre...          2\n",
      "9                  #programascambiados caca d ajuste          0\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 2927|\n",
      "|        2|25392|\n",
      "|        0|18468|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_spanish_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de los textos antes de hacer el análisis de sentimiento.\n",
    "\n",
    "Se limpiarán los textos de elementos y caracteres que no tengan importancia para el análisis a realizar. Además se sacarán los textos tokenizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la función que engloba el preprocesado de los datos, con la tokenización y la limpieza de tweets\n",
    "df_tokens_english_clean, df_tokens_spanish_clean = preprocesado_dataframe(df_csv_english_sample,df_csv_spanish_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica amazing to me that we can't get...</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazing , cant , get , cold , air , vents , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica View of downtown Los Angeles, t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[view , downtown , los , angeles , hollywood ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[plz , help , win , bid , upgrade , flight , l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica I'm #elevategold for a good rea...</td>\n",
       "      <td>2</td>\n",
       "      <td>[im , elevategold , good , reason , rock ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood Juli...</td>\n",
       "      <td>1</td>\n",
       "      <td>[julie , andrews , first , lady , gaga , wowd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica I’m having trouble adding this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[im , trouble , adding , flight , wife , booke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica you have the absolute best team...</td>\n",
       "      <td>2</td>\n",
       "      <td>[absolute , best , team , customer , service ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@VirginAmerica has flight number 276 from SFO ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[flight , number , sfo , cabo , san , lucas , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@VirginAmerica Another delayed flight? #liking...</td>\n",
       "      <td>0</td>\n",
       "      <td>[another , delayed , flight , likingyoulessand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VirginAmerica Can you find us a flt out of LA...</td>\n",
       "      <td>1</td>\n",
       "      <td>[find , us , flt , lax , sooner , midnight , m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@VirginAmerica you have amazing staff &amp;amp; su...</td>\n",
       "      <td>2</td>\n",
       "      <td>[amazing , staff , amp , super , helpful , ran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@VirginAmerica is there special assistance if ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[special , assistance , travel , alone , w , k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@VirginAmerica having problems Flight Booking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[problems , flight , booking , problems , web ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@VirginAmerica How do I reschedule my Cancelle...</td>\n",
       "      <td>0</td>\n",
       "      <td>[reschedule , cancelled , flightled , flights ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@VirginAmerica is the website down?</td>\n",
       "      <td>0</td>\n",
       "      <td>[website ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica  I applied over 2 weeks ago. Ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>[applied , weeks , ago , havent , heard , back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@VirginAmerica Only way to fly! #Elevate #Gold</td>\n",
       "      <td>2</td>\n",
       "      <td>[way , fly , elevate , gold ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@VirginAmerica Having an issue finding a missi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[issue , finding , missing , item , plane , he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@VirginAmerica Because we never rec'd Cancelle...</td>\n",
       "      <td>0</td>\n",
       "      <td>[never , recd , cancelled , flightlation , not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@VirginAmerica why Cancelled Flight flights to...</td>\n",
       "      <td>0</td>\n",
       "      <td>[cancelled , flight , flights , today , precip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment  \\\n",
       "0   @VirginAmerica amazing to me that we can't get...          0   \n",
       "1   @VirginAmerica View of downtown Los Angeles, t...          2   \n",
       "2   @VirginAmerica plz help me win my bid upgrade ...          1   \n",
       "3   @VirginAmerica I'm #elevategold for a good rea...          2   \n",
       "4   @VirginAmerica @ladygaga @carrieunderwood Juli...          1   \n",
       "5   @VirginAmerica I’m having trouble adding this ...          0   \n",
       "6   @VirginAmerica you have the absolute best team...          2   \n",
       "7   @VirginAmerica has flight number 276 from SFO ...          1   \n",
       "8   @VirginAmerica Another delayed flight? #liking...          0   \n",
       "9   @VirginAmerica Can you find us a flt out of LA...          1   \n",
       "10  @VirginAmerica you have amazing staff &amp; su...          2   \n",
       "11  @VirginAmerica is there special assistance if ...          1   \n",
       "12  @VirginAmerica having problems Flight Booking ...          0   \n",
       "13  @VirginAmerica How do I reschedule my Cancelle...          0   \n",
       "14                @VirginAmerica is the website down?          0   \n",
       "15  @VirginAmerica  I applied over 2 weeks ago. Ha...          0   \n",
       "16     @VirginAmerica Only way to fly! #Elevate #Gold          2   \n",
       "17  @VirginAmerica Having an issue finding a missi...          1   \n",
       "18  @VirginAmerica Because we never rec'd Cancelle...          0   \n",
       "19  @VirginAmerica why Cancelled Flight flights to...          0   \n",
       "\n",
       "                                         tokens_clean  \n",
       "0   [amazing , cant , get , cold , air , vents , v...  \n",
       "1   [view , downtown , los , angeles , hollywood ,...  \n",
       "2   [plz , help , win , bid , upgrade , flight , l...  \n",
       "3          [im , elevategold , good , reason , rock ]  \n",
       "4   [julie , andrews , first , lady , gaga , wowd ...  \n",
       "5   [im , trouble , adding , flight , wife , booke...  \n",
       "6   [absolute , best , team , customer , service ,...  \n",
       "7   [flight , number , sfo , cabo , san , lucas , ...  \n",
       "8   [another , delayed , flight , likingyoulessand...  \n",
       "9   [find , us , flt , lax , sooner , midnight , m...  \n",
       "10  [amazing , staff , amp , super , helpful , ran...  \n",
       "11  [special , assistance , travel , alone , w , k...  \n",
       "12  [problems , flight , booking , problems , web ...  \n",
       "13  [reschedule , cancelled , flightled , flights ...  \n",
       "14                                         [website ]  \n",
       "15  [applied , weeks , ago , havent , heard , back...  \n",
       "16                      [way , fly , elevate , gold ]  \n",
       "17  [issue , finding , missing , item , plane , he...  \n",
       "18  [never , recd , cancelled , flightlation , not...  \n",
       "19  [cancelled , flight , flights , today , precip...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos un grupo de datos para ver el correcto preprocesamiento del texto\n",
    "df_tokens_english_clean.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>1</td>\n",
       "      <td>[libraras , ayudar , menos , besos , gracias ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2</td>\n",
       "      <td>[gracias , mar ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>0</td>\n",
       "      <td>[off , pensando , regalito , sinde , va , sgae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[conozco , alguien , q , adicto , drama , ja ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "      <td>2</td>\n",
       "      <td>[toca , grabacion , dl , especial , navideno m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "      <td>[buen , dia , primero , mandar , abrazo , gran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Desde el escaño. Todo listo para empezar #endi...</td>\n",
       "      <td>2</td>\n",
       "      <td>[escano , listo , empezar , endiascomohoy , co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bdías. EM no se ira de puente. Si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[bdias , em , ira , puente , si , vais , dejei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Un sistema económico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "      <td>[sistema , economico , q , recorta , dinero , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "      <td>[programascambiados , caca , d , ajuste ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Buen viernes</td>\n",
       "      <td>2</td>\n",
       "      <td>[buen , viernes ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>“@adri_22_22: #programascambiados es TT gracia...</td>\n",
       "      <td>2</td>\n",
       "      <td>[programascambiados , tt , gracias , gracias ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>La Universidad confía en De la Calle para enca...</td>\n",
       "      <td>2</td>\n",
       "      <td>[universidad , confia , calle , encarar , reto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>¿Me ayudáis a que #indultoneiro sea TT? Por si...</td>\n",
       "      <td>2</td>\n",
       "      <td>[ayudais , indultoneiro , tt , si , zapatero ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>abcdesevilla.es: Recio no tiene «indicios pote...</td>\n",
       "      <td>0</td>\n",
       "      <td>[abcdesevilla.es , recio , indicios , potentes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abcdesevilla.es: Cuatro altos cargos de Empleo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[abcdesevilla.es , cuatro , altos , cargos , e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>La marcha atrás del PP en posponer devolución ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[marcha , atras , pp , posponer , devolucion ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Viernes negro: Aumenta el paro en Noviembre y ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[viernes , negro , aumenta , paro , noviembre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Accidente en BUS-VAO A-6 km. 12. Motorista de ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[accidente , busvao , km , motorista , anos , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#FF a ti, que deseas desesperadamente hacerme ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ff , deseas , desesperadamente , hacerme , ff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment  \\\n",
       "0   @PauladeLasHeras No te libraras de ayudar me/n...          1   \n",
       "1                           @marodriguezb Gracias MAR          2   \n",
       "2   Off pensando en el regalito Sinde, la que se v...          0   \n",
       "3   Conozco a alguien q es adicto al drama! Ja ja ...          2   \n",
       "4   Toca @crackoviadeTV3 . Grabación dl especial N...          2   \n",
       "5   Buen día todos! Lo primero mandar un abrazo gr...          2   \n",
       "6   Desde el escaño. Todo listo para empezar #endi...          2   \n",
       "7   Bdías. EM no se ira de puente. Si vosotros os ...          2   \n",
       "8   Un sistema económico q recorta dinero para pre...          2   \n",
       "9                   #programascambiados caca d ajuste          0   \n",
       "10                                       Buen viernes          2   \n",
       "11  “@adri_22_22: #programascambiados es TT gracia...          2   \n",
       "12  La Universidad confía en De la Calle para enca...          2   \n",
       "13  ¿Me ayudáis a que #indultoneiro sea TT? Por si...          2   \n",
       "14  abcdesevilla.es: Recio no tiene «indicios pote...          0   \n",
       "15  abcdesevilla.es: Cuatro altos cargos de Empleo...          0   \n",
       "16  La marcha atrás del PP en posponer devolución ...          0   \n",
       "17  Viernes negro: Aumenta el paro en Noviembre y ...          0   \n",
       "18  Accidente en BUS-VAO A-6 km. 12. Motorista de ...          0   \n",
       "19  #FF a ti, que deseas desesperadamente hacerme ...          0   \n",
       "\n",
       "                                         tokens_clean  \n",
       "0      [libraras , ayudar , menos , besos , gracias ]  \n",
       "1                                    [gracias , mar ]  \n",
       "2   [off , pensando , regalito , sinde , va , sgae...  \n",
       "3   [conozco , alguien , q , adicto , drama , ja ,...  \n",
       "4   [toca , grabacion , dl , especial , navideno m...  \n",
       "5   [buen , dia , primero , mandar , abrazo , gran...  \n",
       "6   [escano , listo , empezar , endiascomohoy , co...  \n",
       "7   [bdias , em , ira , puente , si , vais , dejei...  \n",
       "8   [sistema , economico , q , recorta , dinero , ...  \n",
       "9           [programascambiados , caca , d , ajuste ]  \n",
       "10                                  [buen , viernes ]  \n",
       "11  [programascambiados , tt , gracias , gracias ,...  \n",
       "12  [universidad , confia , calle , encarar , reto...  \n",
       "13  [ayudais , indultoneiro , tt , si , zapatero ,...  \n",
       "14  [abcdesevilla.es , recio , indicios , potentes...  \n",
       "15  [abcdesevilla.es , cuatro , altos , cargos , e...  \n",
       "16  [marcha , atras , pp , posponer , devolucion ,...  \n",
       "17  [viernes , negro , aumenta , paro , noviembre ...  \n",
       "18  [accidente , busvao , km , motorista , anos , ...  \n",
       "19  [ff , deseas , desesperadamente , hacerme , ff...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_spanish_clean.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de Machine Learning.\n",
    "\n",
    "Vamos a generar los modelos de aprendizaje supervisado, utilizando distintos algoritmos igual que se realizó con la librería Scikit-learn.\n",
    "\n",
    "Usaremos los datos en inglés y español después del preprocesado y limpieza realizado anteriormente.\n",
    "\n",
    "Para los modelos, se hace uso de Pipeline igual que en Scikit-learn, que contendrá el flujo de transformers y estimators que son necesarios para el modelo.\n",
    "\n",
    "Para evaluar el modelo se utilizará \"MulticlassClassificationEvaluator\", ya que es un problema de clasificación con más de 2 clases. La métrica que se tendrá en cuenta es F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a partir del dataframe anterior, vamos a eliminar la columna texto en los datos que han pasado por la limpieza y\n",
    "# preprocesado.\n",
    "df_tokens_english_clean = df_tokens_english_clean.drop(\"text\")\n",
    "df_tokens_spanish_clean = df_tokens_spanish_clean.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[amazing , cant , get , cold , air , vents , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[view , downtown , los , angeles , hollywood ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[plz , help , win , bid , upgrade , flight , l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[im , elevategold , good , reason , rock ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[julie , andrews , first , lady , gaga , wowd ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                       tokens_clean\n",
       "0          0  [amazing , cant , get , cold , air , vents , v...\n",
       "1          2  [view , downtown , los , angeles , hollywood ,...\n",
       "2          1  [plz , help , win , bid , upgrade , flight , l...\n",
       "3          2         [im , elevategold , good , reason , rock ]\n",
       "4          1  [julie , andrews , first , lady , gaga , wowd ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_english_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos el nombre de la columna sentiment por label a los dataframes\n",
    "df_english_clean = df_tokens_english_clean.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean = df_tokens_spanish_clean.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generación de conjunto de training y test de los dataframes\n",
    "train_english, test_english = df_english_clean.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_spanish, test_spanish = df_spanish_clean.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 1205|\n",
      "|    2|78215|\n",
      "|    0|78116|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vemos el recuento de registros en los conjuntos de train y test por cada clase de sentimiento\n",
    "train_english.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  141|\n",
      "|    2| 8716|\n",
      "|    0| 8539|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_english.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 2619|\n",
      "|    2|22781|\n",
      "|    0|16612|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_spanish.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  308|\n",
      "|    2| 2589|\n",
      "|    0| 1856|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_spanish.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los pipeline para cada modelo con las etapas o pasos que correspondan.\n",
    "# primero probamos con Hashing y logisticRegression como algoritmo\n",
    "hashingTF = HashingTF(inputCol=\"tokens_clean\", outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_lr = pipeline_lr.fit(train_english)\n",
    "model_spa_lr = pipeline_lr.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_lr = model_eng_lr.transform(test_english)\n",
    "pred_spa_lr = model_spa_lr.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|                  tokens_clean|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|[yach , allah , tawne , saw...|[0.9999999745421377,2.54446...|    0|       0.0|\n",
      "|[grumpy , tomorrow , hes , ...|[0.999999974241276,2.566668...|    0|       0.0|\n",
      "|[boo , still , mia , printe...|[0.9999999538056606,4.61111...|    0|       0.0|\n",
      "|[chickens , chickens , damn...|[0.9999999238062758,7.59345...|    0|       0.0|\n",
      "|[pts , expired , made , prc...|[0.9999999049176175,9.50285...|    0|       0.0|\n",
      "|[today , im , really , amaz...|[0.9999998019933557,7.29863...|    0|       0.0|\n",
      "|[going , chumlees , chinese...|[0.9999997774096685,2.20251...|    0|       0.0|\n",
      "|[ohgosh , hate , recession ...|[0.9999997621053274,2.33153...|    0|       0.0|\n",
      "|[wow , feel , like , ish , ...|[0.9999997229447234,2.64365...|    0|       0.0|\n",
      "|[seriouslyy , really , hate...|[0.9999996278249694,3.04794...|    0|       0.0|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "\n",
    "pred_eng_lr.filter(pred_eng_lr['prediction'] == 0) \\\n",
    "    .select(\"tokens_clean\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|                  tokens_clean|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|[k , k , detuvieron , k , d...|[0.9999999987991242,9.12566...|    0|       0.0|\n",
      "|[patxi , lopez , anuncia , ...|[0.999999998509266,1.268585...|    0|       0.0|\n",
      "|[paramo , huye , preguntas ...|[0.9999999979516403,6.26268...|    0|       0.0|\n",
      "|[sabiais , grinan , generad...|[0.9999999974993734,1.67380...|    0|       0.0|\n",
      "|[muerto , heridos , graves ...|[0.9999999962963302,3.70298...|    0|       0.0|\n",
      "|[explosion , gas , posible ...|[0.9999999939276418,6.05849...|    0|       0.0|\n",
      "|[dicen , escuchamos , gemel...|[0.9999999923347495,5.98716...|    0|       0.0|\n",
      "|[deficit , ciento , recesio...|[0.9999999920444488,3.35025...|    0|       0.0|\n",
      "|[acusa , grinan , confundir...|[0.9999999898535853,9.83601...|    0|       0.0|\n",
      "|[parece , grave , traten , ...|[0.9999999864194309,1.33525...|    0|       0.0|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "\n",
    "pred_spa_lr.filter(pred_spa_lr['prediction'] == 0) \\\n",
    "    .select(\"tokens_clean\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.716218\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.758850\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_lr, pred_spa_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a probar cambiando HashingVectorizer por CountVectorizer\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline_countVec = Pipeline(stages=[countVec, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_countVec = pipeline_countVec.fit(train_english)\n",
    "model_spa_countVec = pipeline_countVec.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_countVec = model_eng_countVec.transform(test_english)\n",
    "pred_spa_countVec = model_spa_countVec.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.727583\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.765104\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_countVec, pred_spa_countVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a probar a añadir después del vectorizador \"countVectorizer\" el uso de TD-IDF\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline_idf = Pipeline(stages=[countVec, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_idf = pipeline_idf.fit(train_english)\n",
    "model_spa_idf = pipeline_idf.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_idf = model_eng_idf.transform(test_english)\n",
    "pred_spa_idf = model_spa_idf.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.738500\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.769939\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_idf, pred_spa_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probar distintos algoritmos.\n",
    "\n",
    "Vamos ahora a probar con el último pipeline generado, que se habían mejorado ligeramente los resultados, con los algoritmos de Naive Bayes y Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo Naive Bayes\n",
    "nb = NaiveBayes(smoothing=1, modelType=\"multinomial\")\n",
    "\n",
    "pipeline_nb = Pipeline(stages=[countVec, idf, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_nb = pipeline_nb.fit(train_english)\n",
    "model_spa_nb = pipeline_nb.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_nb = model_eng_nb.transform(test_english)\n",
    "pred_spa_nb = model_spa_nb.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.734045\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.762048\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_nb, pred_spa_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo Random Forest\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 20, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "pipeline_rf = Pipeline(stages=[countVec, idf, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_rf = pipeline_rf.fit(train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa_rf = pipeline_rf.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_rf = model_eng_rf.transform(test_english)\n",
    "pred_spa_rf = model_spa_rf.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.553909\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.384633\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_rf, pred_spa_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de modelos y ajuste de los hiperparámetros.\n",
    "\n",
    "Hasta ahora los mejores resultados se han conseguido usando el algoritmo de LogisticRegression y Naive Bayes. Ahora vamos a probar a realizar ajuste de hiperparámetros usando CrossValidator y TrainValidationSplit.\n",
    "Para ver los mejores hiperparámetros para el modelo se usan conjuntos de validación. \n",
    "\n",
    "Durante el proceso de validación, el conjunto de datos se divide en dos (entrenamiento y validación). El conjunto de entrenamiento se utiliza para entrenar modelos con cada uno de los posibles conjuntos de valores de los hiperparámetros. Cada uno de estos modelos se evalúa sobre el conjunto de validación, y el que da los mejores resultados (de acuerdo al Evaluator) es seleccionado. \n",
    "\n",
    "El modelo final se vuelve a entrenar con todos los datos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el pipeline probado con el algoritmo de clasificación Naive Bayes\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "nb = NaiveBayes(smoothing=1, modelType=\"multinomial\")\n",
    "\n",
    "pipeline = Pipeline(stages=[countVec, idf, nb])\n",
    "\n",
    "#\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "    \n",
    "#\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos cross-validation\n",
    "model_eng_cv = crossval.fit(train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa_cv = crossval.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer parameters datos inglés:\n",
      "   CountVectorizer_b616a494e8fb__binary False\n",
      "   CountVectorizer_b616a494e8fb__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_b616a494e8fb__minDF 1.0\n",
      "   CountVectorizer_b616a494e8fb__minTF 1.0\n",
      "   CountVectorizer_b616a494e8fb__outputCol countfeatures\n",
      "   CountVectorizer_b616a494e8fb__vocabSize 262144\n",
      "   CountVectorizer_b616a494e8fb__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Vectorizer parameters datos español:\n",
      "   CountVectorizer_b616a494e8fb__binary False\n",
      "   CountVectorizer_b616a494e8fb__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_b616a494e8fb__minDF 1.0\n",
      "   CountVectorizer_b616a494e8fb__minTF 1.0\n",
      "   CountVectorizer_b616a494e8fb__outputCol countfeatures\n",
      "   CountVectorizer_b616a494e8fb__vocabSize 262144\n",
      "   CountVectorizer_b616a494e8fb__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Model parameters datos inglés:\n",
      "   NaiveBayes_a63bf89f899a__featuresCol features\n",
      "   NaiveBayes_a63bf89f899a__labelCol label\n",
      "   NaiveBayes_a63bf89f899a__modelType multinomial\n",
      "   NaiveBayes_a63bf89f899a__predictionCol prediction\n",
      "   NaiveBayes_a63bf89f899a__probabilityCol probability\n",
      "   NaiveBayes_a63bf89f899a__rawPredictionCol rawPrediction\n",
      "   NaiveBayes_a63bf89f899a__smoothing 1.0\n",
      "\n",
      "\n",
      "Model parameters datos español:\n",
      "   NaiveBayes_a63bf89f899a__featuresCol features\n",
      "   NaiveBayes_a63bf89f899a__labelCol label\n",
      "   NaiveBayes_a63bf89f899a__modelType multinomial\n",
      "   NaiveBayes_a63bf89f899a__predictionCol prediction\n",
      "   NaiveBayes_a63bf89f899a__probabilityCol probability\n",
      "   NaiveBayes_a63bf89f899a__rawPredictionCol rawPrediction\n",
      "   NaiveBayes_a63bf89f899a__smoothing 1.0\n"
     ]
    }
   ],
   "source": [
    "# mostrar los mejores parámetros\n",
    "mejores_parametros(model_eng_cv, model_spa_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_cv = model_eng_cv.transform(test_english)\n",
    "pred_spa_cv = model_spa_cv.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.734045\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.762048\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_cv, pred_spa_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora probamos el uso de CrossValidator con el algoritmo de LogisticRegression\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline_cv_lr = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid_cv_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "crossval_cv_lr = CrossValidator(estimator=pipeline_cv_lr,\n",
    "                          estimatorParamMaps=paramGrid_cv_lr,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eng_cv_lr = crossval_cv_lr.fit(train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa_cv_lr = crossval_cv_lr.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer parameters datos inglés:\n",
      "   CountVectorizer_770593e2c78e__binary False\n",
      "   CountVectorizer_770593e2c78e__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_770593e2c78e__minDF 1.0\n",
      "   CountVectorizer_770593e2c78e__minTF 1.0\n",
      "   CountVectorizer_770593e2c78e__outputCol countfeatures\n",
      "   CountVectorizer_770593e2c78e__vocabSize 262144\n",
      "   CountVectorizer_770593e2c78e__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Vectorizer parameters datos español:\n",
      "   CountVectorizer_770593e2c78e__binary False\n",
      "   CountVectorizer_770593e2c78e__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_770593e2c78e__minDF 1.0\n",
      "   CountVectorizer_770593e2c78e__minTF 1.0\n",
      "   CountVectorizer_770593e2c78e__outputCol countfeatures\n",
      "   CountVectorizer_770593e2c78e__vocabSize 262144\n",
      "   CountVectorizer_770593e2c78e__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Model parameters datos inglés:\n",
      "   LogisticRegression_fcdb86672b4e__aggregationDepth 2\n",
      "   LogisticRegression_fcdb86672b4e__elasticNetParam 0.1\n",
      "   LogisticRegression_fcdb86672b4e__family auto\n",
      "   LogisticRegression_fcdb86672b4e__featuresCol features\n",
      "   LogisticRegression_fcdb86672b4e__fitIntercept True\n",
      "   LogisticRegression_fcdb86672b4e__labelCol label\n",
      "   LogisticRegression_fcdb86672b4e__maxIter 5\n",
      "   LogisticRegression_fcdb86672b4e__predictionCol prediction\n",
      "   LogisticRegression_fcdb86672b4e__probabilityCol probability\n",
      "   LogisticRegression_fcdb86672b4e__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_fcdb86672b4e__regParam 0.01\n",
      "   LogisticRegression_fcdb86672b4e__standardization True\n",
      "   LogisticRegression_fcdb86672b4e__threshold 0.5\n",
      "   LogisticRegression_fcdb86672b4e__tol 1e-06\n",
      "\n",
      "\n",
      "Model parameters datos español:\n",
      "   LogisticRegression_fcdb86672b4e__aggregationDepth 2\n",
      "   LogisticRegression_fcdb86672b4e__elasticNetParam 0.2\n",
      "   LogisticRegression_fcdb86672b4e__family auto\n",
      "   LogisticRegression_fcdb86672b4e__featuresCol features\n",
      "   LogisticRegression_fcdb86672b4e__fitIntercept True\n",
      "   LogisticRegression_fcdb86672b4e__labelCol label\n",
      "   LogisticRegression_fcdb86672b4e__maxIter 5\n",
      "   LogisticRegression_fcdb86672b4e__predictionCol prediction\n",
      "   LogisticRegression_fcdb86672b4e__probabilityCol probability\n",
      "   LogisticRegression_fcdb86672b4e__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_fcdb86672b4e__regParam 0.01\n",
      "   LogisticRegression_fcdb86672b4e__standardization True\n",
      "   LogisticRegression_fcdb86672b4e__threshold 0.5\n",
      "   LogisticRegression_fcdb86672b4e__tol 1e-06\n"
     ]
    }
   ],
   "source": [
    "# mostrar los mejores parámetros\n",
    "mejores_parametros(model_eng_cv_lr, model_spa_cv_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_cv_lr = model_eng_cv_lr.transform(test_english)\n",
    "pred_spa_cv_lr = model_spa_cv_lr.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.760526\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.806676\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_cv_lr, pred_spa_cv_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas con los otros datasets cargados desde ficheros csv con datos ya etiquetados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar los otros datasets cargados desde ficheros csv:\n",
    "- Con datos en inglés y español con el mismo balance de registros entre las 3 categorías.\n",
    "- Con datos en inglés y español con el mismo número de registros de solo 2 categorías: negativo y positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la función que engloba el preprocesado de los datos, con la tokenización y la limpieza de tweets\n",
    "df_tokens_english_clean_neutro, df_tokens_spanish_clean_neutro = preprocesado_dataframe(df_csv_english_neutro,\\\n",
    "                                                                          df_csv_spanish_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_english_clean_neutro = df_tokens_english_clean_neutro.drop(\"text\")\n",
    "df_tokens_spanish_clean_neutro = df_tokens_spanish_clean_neutro.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english_clean_neutro = df_tokens_english_clean_neutro.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean_neutro = df_tokens_spanish_clean_neutro.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_neutro, test_english_neutro = df_english_clean_neutro.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_spanish_neutro, test_spanish_neutro = df_spanish_clean_neutro.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|12360|\n",
      "|    2|12502|\n",
      "|    0|12552|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_english_neutro.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 2536|\n",
      "|    2| 2622|\n",
      "|    0| 2539|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_spanish_neutro.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a lanzar una prueba con crossvalidator y logisticRegression, con los datos que tienen practicamente el mismo\n",
    "# número de registros de las 3 clases distintas de sentimiento.\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline_cv_lr_neutro = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid_cv_lr_neutro = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "crossval_cv_lr_neutro = CrossValidator(estimator=pipeline_cv_lr_neutro,\n",
    "                          estimatorParamMaps=paramGrid_cv_lr_neutro,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eng_cv_lr_neutro = crossval_cv_lr_neutro.fit(train_english_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa_cv_lr_neutro = crossval_cv_lr_neutro.fit(train_spanish_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer parameters datos inglés:\n",
      "   CountVectorizer_9187c9b13a2b__binary False\n",
      "   CountVectorizer_9187c9b13a2b__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_9187c9b13a2b__minDF 1.0\n",
      "   CountVectorizer_9187c9b13a2b__minTF 1.0\n",
      "   CountVectorizer_9187c9b13a2b__outputCol countfeatures\n",
      "   CountVectorizer_9187c9b13a2b__vocabSize 262144\n",
      "   CountVectorizer_9187c9b13a2b__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Vectorizer parameters datos español:\n",
      "   CountVectorizer_9187c9b13a2b__binary False\n",
      "   CountVectorizer_9187c9b13a2b__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_9187c9b13a2b__minDF 1.0\n",
      "   CountVectorizer_9187c9b13a2b__minTF 1.0\n",
      "   CountVectorizer_9187c9b13a2b__outputCol countfeatures\n",
      "   CountVectorizer_9187c9b13a2b__vocabSize 262144\n",
      "   CountVectorizer_9187c9b13a2b__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Model parameters datos inglés:\n",
      "   LogisticRegression_35077ec1faf4__aggregationDepth 2\n",
      "   LogisticRegression_35077ec1faf4__elasticNetParam 0.2\n",
      "   LogisticRegression_35077ec1faf4__family auto\n",
      "   LogisticRegression_35077ec1faf4__featuresCol features\n",
      "   LogisticRegression_35077ec1faf4__fitIntercept True\n",
      "   LogisticRegression_35077ec1faf4__labelCol label\n",
      "   LogisticRegression_35077ec1faf4__maxIter 5\n",
      "   LogisticRegression_35077ec1faf4__predictionCol prediction\n",
      "   LogisticRegression_35077ec1faf4__probabilityCol probability\n",
      "   LogisticRegression_35077ec1faf4__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_35077ec1faf4__regParam 0.01\n",
      "   LogisticRegression_35077ec1faf4__standardization True\n",
      "   LogisticRegression_35077ec1faf4__threshold 0.5\n",
      "   LogisticRegression_35077ec1faf4__tol 1e-06\n",
      "\n",
      "\n",
      "Model parameters datos español:\n",
      "   LogisticRegression_35077ec1faf4__aggregationDepth 2\n",
      "   LogisticRegression_35077ec1faf4__elasticNetParam 0.1\n",
      "   LogisticRegression_35077ec1faf4__family auto\n",
      "   LogisticRegression_35077ec1faf4__featuresCol features\n",
      "   LogisticRegression_35077ec1faf4__fitIntercept True\n",
      "   LogisticRegression_35077ec1faf4__labelCol label\n",
      "   LogisticRegression_35077ec1faf4__maxIter 20\n",
      "   LogisticRegression_35077ec1faf4__predictionCol prediction\n",
      "   LogisticRegression_35077ec1faf4__probabilityCol probability\n",
      "   LogisticRegression_35077ec1faf4__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_35077ec1faf4__regParam 0.1\n",
      "   LogisticRegression_35077ec1faf4__standardization True\n",
      "   LogisticRegression_35077ec1faf4__threshold 0.5\n",
      "   LogisticRegression_35077ec1faf4__tol 1e-06\n"
     ]
    }
   ],
   "source": [
    "# mostrar los mejores parámetros\n",
    "mejores_parametros(model_eng_cv_lr_neutro, model_spa_cv_lr_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_cv_lr_neutro = model_eng_cv_lr_neutro.transform(test_english_neutro)\n",
    "pred_spa_cv_lr_neutro = model_spa_cv_lr_neutro.transform(test_spanish_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.590517\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.625248\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_cv_lr_neutro, pred_spa_cv_lr_neutro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba con TrainValidationSplit en lugar de CrossValidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos el modelo con los mejores resultados, y el uso de TrainValidationSplit en lugar de CrossValidation\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline_cv_lr = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid_cv_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline_cv_lr,\n",
    "                           estimatorParamMaps=paramGrid_cv_lr,\n",
    "                           evaluator=MulticlassClassificationEvaluator(),\n",
    "                           trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tvs_eng = tvs.fit(train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tvs_spa = tvs.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_tvs = model_tvs_eng.transform(test_english)\n",
    "pred_spa_tvs = model_tvs_spa.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.761035\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.801400\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_tvs, pred_spa_tvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probar el modelo para predecir los tweets sin etiquetar recogidos de MongoDB.\n",
    "\n",
    "Usando el modelo anterior, donde se han obtenido más o menos los mejores resultados que con el mejor anterior (sobre el 76% con datos en inglés, y un 80% con datos en español, vamos a predecir el sentimiento de los datos almacenados en MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cogemos un sample del DF con datos en inglés ya que tiene muchos datos y da problemas de cómputo y tiempos\n",
    "df_mongo_english_sample = df_mongo_english.sample(False, 0.4, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la función que engloba el preprocesado de los datos, con la tokenización y la limpieza de tweets\n",
    "df_tokens_english_clean_mongo, df_tokens_spanish_clean_mongo = preprocesado_dataframe(df_mongo_english_sample,\\\n",
    "                                                                          df_mongo_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @liamyoung: Strange that Tony Blair has sud...</td>\n",
       "      <td>[strange , tony , blair , suddenly , become , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hard work puts you where good luck can find you.</td>\n",
       "      <td>[hard , work , puts , good , luck , find ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @xCiphxr: When creative kids try playing co...</td>\n",
       "      <td>[creative , kids , try , playing , comp ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @bonang_m: I’m working on one as we speak. ...</td>\n",
       "      <td>[im , working , one , speak , thank , incredib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @akashbanerjee: After #PulwamaAttack, terro...</td>\n",
       "      <td>[pulwamaattack , terrorists , scored , bigger ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @godisgodisback: Beyoncé: WORLD STOP!\\n\\nWo...</td>\n",
       "      <td>[beyonc , world , stop , world ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No. It just needs money and connections. Your ...</td>\n",
       "      <td>[needs , money , connections , average , norma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @hopeoverfear01: It’s time to run our own c...</td>\n",
       "      <td>[time , run , country , retweet , agree ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @gugugaga__: this works. i’m not even gonna...</td>\n",
       "      <td>[works , im , even , gon na , front , retweete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @pushetblur: Trying to prove a point. Lets ...</td>\n",
       "      <td>[trying , prove , point , lets , settle , retw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT @liamyoung: Strange that Tony Blair has sud...   \n",
       "1   Hard work puts you where good luck can find you.   \n",
       "2  RT @xCiphxr: When creative kids try playing co...   \n",
       "3  RT @bonang_m: I’m working on one as we speak. ...   \n",
       "4  RT @akashbanerjee: After #PulwamaAttack, terro...   \n",
       "5  RT @godisgodisback: Beyoncé: WORLD STOP!\\n\\nWo...   \n",
       "6  No. It just needs money and connections. Your ...   \n",
       "7  RT @hopeoverfear01: It’s time to run our own c...   \n",
       "8  RT @gugugaga__: this works. i’m not even gonna...   \n",
       "9  RT @pushetblur: Trying to prove a point. Lets ...   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0  [strange , tony , blair , suddenly , become , ...  \n",
       "1         [hard , work , puts , good , luck , find ]  \n",
       "2          [creative , kids , try , playing , comp ]  \n",
       "3  [im , working , one , speak , thank , incredib...  \n",
       "4  [pulwamaattack , terrorists , scored , bigger ...  \n",
       "5                   [beyonc , world , stop , world ]  \n",
       "6  [needs , money , connections , average , norma...  \n",
       "7          [time , run , country , retweet , agree ]  \n",
       "8  [works , im , even , gon na , front , retweete...  \n",
       "9  [trying , prove , point , lets , settle , retw...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_english_clean_mongo.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@EJFC26 Y Messi</td>\n",
       "      <td>[messi ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @MBelenAlegre: Hermoso y sensual viernes ❣️</td>\n",
       "      <td>[hermoso , sensual , viernes ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Foro_TV: Suman 47 muertos y 640 heridos po...</td>\n",
       "      <td>[suman , muertos , heridos , explosion , china ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @revistaetcetera: .@lopezobrador_ dice que ...</td>\n",
       "      <td>[dice , tan , grande , gloria , benito , juare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me retracto de mi respuesta , él gobierno se s...</td>\n",
       "      <td>[retracto , respuesta , gobierno , acorralado ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anda a saber si te esta eligiendo o sos lo úni...</td>\n",
       "      <td>[anda , saber , si , eligiendo , sos , unico ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @JCTrujilloCPCCS: Hay que propiciar una Con...</td>\n",
       "      <td>[propiciar , consulta , popular , pueblo , ecu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@RicciuP Terraplanistas</td>\n",
       "      <td>[terraplanistas ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @oriolguellipuig: Jeje jeje https://t.co/NU...</td>\n",
       "      <td>[jeje , jeje ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas? no quiero ser part...</td>\n",
       "      <td>[veganarcas , quiero , ser , parte , team , ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                    @EJFC26 Y Messi   \n",
       "1     RT @MBelenAlegre: Hermoso y sensual viernes ❣️   \n",
       "2  RT @Foro_TV: Suman 47 muertos y 640 heridos po...   \n",
       "3  RT @revistaetcetera: .@lopezobrador_ dice que ...   \n",
       "4  Me retracto de mi respuesta , él gobierno se s...   \n",
       "5  Anda a saber si te esta eligiendo o sos lo úni...   \n",
       "6  RT @JCTrujilloCPCCS: Hay que propiciar una Con...   \n",
       "7                            @RicciuP Terraplanistas   \n",
       "8  RT @oriolguellipuig: Jeje jeje https://t.co/NU...   \n",
       "9  quienes son las veganarcas? no quiero ser part...   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0                                           [messi ]  \n",
       "1                     [hermoso , sensual , viernes ]  \n",
       "2   [suman , muertos , heridos , explosion , china ]  \n",
       "3  [dice , tan , grande , gloria , benito , juare...  \n",
       "4    [retracto , respuesta , gobierno , acorralado ]  \n",
       "5  [anda , saber , si , eligiendo , sos , unico ,...  \n",
       "6  [propiciar , consulta , popular , pueblo , ecu...  \n",
       "7                                  [terraplanistas ]  \n",
       "8                                     [jeje , jeje ]  \n",
       "9  [veganarcas , quiero , ser , parte , team , ex...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_spanish_clean_mongo.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_clean = model_tvs_eng.transform(df_tokens_english_clean_mongo)\n",
    "pred_spa_clean = model_tvs_spa.transform(df_tokens_spanish_clean_mongo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nos quedamos con las columnas del texto, tokens y la predicción calculada\n",
    "pred_eng_clean = pred_eng_clean.drop(\"countfeatures\",\"features\",\"rawPrediction\",\"probability\")\n",
    "pred_spa_clean = pred_spa_clean.drop(\"countfeatures\",\"features\",\"rawPrediction\",\"probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_clean</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @liamyoung: Strange that Tony Blair has sud...</td>\n",
       "      <td>[strange , tony , blair , suddenly , become , ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hard work puts you where good luck can find you.</td>\n",
       "      <td>[hard , work , puts , good , luck , find ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @xCiphxr: When creative kids try playing co...</td>\n",
       "      <td>[creative , kids , try , playing , comp ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @bonang_m: I’m working on one as we speak. ...</td>\n",
       "      <td>[im , working , one , speak , thank , incredib...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @akashbanerjee: After #PulwamaAttack, terro...</td>\n",
       "      <td>[pulwamaattack , terrorists , scored , bigger ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @godisgodisback: Beyoncé: WORLD STOP!\\n\\nWo...</td>\n",
       "      <td>[beyonc , world , stop , world ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No. It just needs money and connections. Your ...</td>\n",
       "      <td>[needs , money , connections , average , norma...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @hopeoverfear01: It’s time to run our own c...</td>\n",
       "      <td>[time , run , country , retweet , agree ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Do me a favor right? Follow this absolute top ...</td>\n",
       "      <td>[favor , right , follow , absolute , top , fuc...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @pushetblur: Trying to prove a point. Lets ...</td>\n",
       "      <td>[trying , prove , point , lets , settle , retw...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@TeilZeitMensch Love you.</td>\n",
       "      <td>[love ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>allergies are already acting up</td>\n",
       "      <td>[allergies , already , acting ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Well there was a fly in the milk and a over do...</td>\n",
       "      <td>[well , fly , milk , done , poached , egg , mu...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@factsionary Men tend to grunt more...</td>\n",
       "      <td>[men , tend , grunt ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RT @hasanminhaj: Everyone should have @ShashiT...</td>\n",
       "      <td>[everyone , order , taco , bell ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RT @UNSW: It's #WorldFrogDay! Here are the 7 k...</td>\n",
       "      <td>[worldfrogday , known , frog , amplexus , mati...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RT @printpuncakalam: CC ARAB  24 Hours for UiT...</td>\n",
       "      <td>[cc , arab , hours , uitm , student , cc , ara...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@TheKriten what’s the shop also do u know whic...</td>\n",
       "      <td>[whats , shop , also , u , know , artist ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>We are now up to two PS4’s, PS VR, a PS2, a Dr...</td>\n",
       "      <td>[two , ps4s , ps , vr , ps , dreamcast , sega ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RT @realDonaldTrump: It was announced today by...</td>\n",
       "      <td>[announced , today , u.s , treasury , addition...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   RT @liamyoung: Strange that Tony Blair has sud...   \n",
       "1    Hard work puts you where good luck can find you.   \n",
       "2   RT @xCiphxr: When creative kids try playing co...   \n",
       "3   RT @bonang_m: I’m working on one as we speak. ...   \n",
       "4   RT @akashbanerjee: After #PulwamaAttack, terro...   \n",
       "5   RT @godisgodisback: Beyoncé: WORLD STOP!\\n\\nWo...   \n",
       "6   No. It just needs money and connections. Your ...   \n",
       "7   RT @hopeoverfear01: It’s time to run our own c...   \n",
       "8   Do me a favor right? Follow this absolute top ...   \n",
       "9   RT @pushetblur: Trying to prove a point. Lets ...   \n",
       "10                          @TeilZeitMensch Love you.   \n",
       "11                    allergies are already acting up   \n",
       "12  Well there was a fly in the milk and a over do...   \n",
       "13             @factsionary Men tend to grunt more...   \n",
       "14  RT @hasanminhaj: Everyone should have @ShashiT...   \n",
       "15  RT @UNSW: It's #WorldFrogDay! Here are the 7 k...   \n",
       "16  RT @printpuncakalam: CC ARAB  24 Hours for UiT...   \n",
       "17  @TheKriten what’s the shop also do u know whic...   \n",
       "18  We are now up to two PS4’s, PS VR, a PS2, a Dr...   \n",
       "19  RT @realDonaldTrump: It was announced today by...   \n",
       "\n",
       "                                         tokens_clean  prediction  \n",
       "0   [strange , tony , blair , suddenly , become , ...         0.0  \n",
       "1          [hard , work , puts , good , luck , find ]         0.0  \n",
       "2           [creative , kids , try , playing , comp ]         2.0  \n",
       "3   [im , working , one , speak , thank , incredib...         2.0  \n",
       "4   [pulwamaattack , terrorists , scored , bigger ...         2.0  \n",
       "5                    [beyonc , world , stop , world ]         2.0  \n",
       "6   [needs , money , connections , average , norma...         0.0  \n",
       "7           [time , run , country , retweet , agree ]         2.0  \n",
       "8   [favor , right , follow , absolute , top , fuc...         2.0  \n",
       "9   [trying , prove , point , lets , settle , retw...         2.0  \n",
       "10                                            [love ]         2.0  \n",
       "11                    [allergies , already , acting ]         0.0  \n",
       "12  [well , fly , milk , done , poached , egg , mu...         2.0  \n",
       "13                              [men , tend , grunt ]         2.0  \n",
       "14                  [everyone , order , taco , bell ]         2.0  \n",
       "15  [worldfrogday , known , frog , amplexus , mati...         2.0  \n",
       "16  [cc , arab , hours , uitm , student , cc , ara...         0.0  \n",
       "17         [whats , shop , also , u , know , artist ]         2.0  \n",
       "18  [two , ps4s , ps , vr , ps , dreamcast , sega ...         2.0  \n",
       "19  [announced , today , u.s , treasury , addition...         2.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "pred_eng_clean.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_clean</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@EJFC26 Y Messi</td>\n",
       "      <td>[messi ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @MBelenAlegre: Hermoso y sensual viernes ❣️</td>\n",
       "      <td>[hermoso , sensual , viernes ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Foro_TV: Suman 47 muertos y 640 heridos po...</td>\n",
       "      <td>[suman , muertos , heridos , explosion , china ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @revistaetcetera: .@lopezobrador_ dice que ...</td>\n",
       "      <td>[dice , tan , grande , gloria , benito , juare...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me retracto de mi respuesta , él gobierno se s...</td>\n",
       "      <td>[retracto , respuesta , gobierno , acorralado ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anda a saber si te esta eligiendo o sos lo úni...</td>\n",
       "      <td>[anda , saber , si , eligiendo , sos , unico ,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @JCTrujilloCPCCS: Hay que propiciar una Con...</td>\n",
       "      <td>[propiciar , consulta , popular , pueblo , ecu...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@RicciuP Terraplanistas</td>\n",
       "      <td>[terraplanistas ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @oriolguellipuig: Jeje jeje https://t.co/NU...</td>\n",
       "      <td>[jeje , jeje ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas? no quiero ser part...</td>\n",
       "      <td>[veganarcas , quiero , ser , parte , team , ex...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT @NatalyArteta16: Más días con este clima❤</td>\n",
       "      <td>[mas , dias , clima ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RT @julxval: mood del fandom después de haber ...</td>\n",
       "      <td>[mood , fandom , despues , haber , recaudado ,...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RT @aguantaok: Yo te quiero pa' mi no te quier...</td>\n",
       "      <td>[quiero , pa , quiero , pa , mas , nadie , sol...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @zoecoria9: Me llego a poner eso y parezco ...</td>\n",
       "      <td>[llego , poner , parezco , arrollado , pollo ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RT @fagundezcertad1: Un gran saludo a mis cama...</td>\n",
       "      <td>[gran , saludo , camaradas , patriotas ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@gcfwithmochi Yo también vi a una chica army e...</td>\n",
       "      <td>[tambien , vi , chica , army , u , quise , hab...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#Cultura\\nEl Festival de #Cómic Europeo de #Úb...</td>\n",
       "      <td>[cultura , festival , comic , europeo , ubeda ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@COOPERMENDI Solo entro en twitter por ti</td>\n",
       "      <td>[solo , entro , twitter ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RT @gil_cas: Que tan feo debes ser, para prefe...</td>\n",
       "      <td>[tan , feo , debes , ser , preferir , emoji , ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RT @Juliaenlaonda: #PortarArmasJELO @noeliacla...</td>\n",
       "      <td>[portararmasjelo , unidos , unico , pais , mun...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0                                     @EJFC26 Y Messi   \n",
       "1      RT @MBelenAlegre: Hermoso y sensual viernes ❣️   \n",
       "2   RT @Foro_TV: Suman 47 muertos y 640 heridos po...   \n",
       "3   RT @revistaetcetera: .@lopezobrador_ dice que ...   \n",
       "4   Me retracto de mi respuesta , él gobierno se s...   \n",
       "5   Anda a saber si te esta eligiendo o sos lo úni...   \n",
       "6   RT @JCTrujilloCPCCS: Hay que propiciar una Con...   \n",
       "7                             @RicciuP Terraplanistas   \n",
       "8   RT @oriolguellipuig: Jeje jeje https://t.co/NU...   \n",
       "9   quienes son las veganarcas? no quiero ser part...   \n",
       "10       RT @NatalyArteta16: Más días con este clima❤   \n",
       "11  RT @julxval: mood del fandom después de haber ...   \n",
       "12  RT @aguantaok: Yo te quiero pa' mi no te quier...   \n",
       "13  RT @zoecoria9: Me llego a poner eso y parezco ...   \n",
       "14  RT @fagundezcertad1: Un gran saludo a mis cama...   \n",
       "15  @gcfwithmochi Yo también vi a una chica army e...   \n",
       "16  #Cultura\\nEl Festival de #Cómic Europeo de #Úb...   \n",
       "17          @COOPERMENDI Solo entro en twitter por ti   \n",
       "18  RT @gil_cas: Que tan feo debes ser, para prefe...   \n",
       "19  RT @Juliaenlaonda: #PortarArmasJELO @noeliacla...   \n",
       "\n",
       "                                         tokens_clean  prediction  \n",
       "0                                            [messi ]         2.0  \n",
       "1                      [hermoso , sensual , viernes ]         2.0  \n",
       "2    [suman , muertos , heridos , explosion , china ]         0.0  \n",
       "3   [dice , tan , grande , gloria , benito , juare...         0.0  \n",
       "4     [retracto , respuesta , gobierno , acorralado ]         0.0  \n",
       "5   [anda , saber , si , eligiendo , sos , unico ,...         0.0  \n",
       "6   [propiciar , consulta , popular , pueblo , ecu...         2.0  \n",
       "7                                   [terraplanistas ]         0.0  \n",
       "8                                      [jeje , jeje ]         2.0  \n",
       "9   [veganarcas , quiero , ser , parte , team , ex...         2.0  \n",
       "10                              [mas , dias , clima ]         2.0  \n",
       "11  [mood , fandom , despues , haber , recaudado ,...         2.0  \n",
       "12  [quiero , pa , quiero , pa , mas , nadie , sol...         2.0  \n",
       "13     [llego , poner , parezco , arrollado , pollo ]         0.0  \n",
       "14           [gran , saludo , camaradas , patriotas ]         2.0  \n",
       "15  [tambien , vi , chica , army , u , quise , hab...         1.0  \n",
       "16  [cultura , festival , comic , europeo , ubeda ...         2.0  \n",
       "17                          [solo , entro , twitter ]         2.0  \n",
       "18  [tan , feo , debes , ser , preferir , emoji , ...         0.0  \n",
       "19  [portararmasjelo , unidos , unico , pais , mun...         2.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "pred_spa_clean.limit(20).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
