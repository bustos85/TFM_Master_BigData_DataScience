{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Structured streaming con Spark ML.\n",
    "\n",
    "Se recogen los datos en streaming desde Kafka, y se les aplicará el análisis de sentimiento para etiquetar el sentimiento de los tweets recogidos en streaming, utilizando Spark ML. También se almacenarán los tweets y su predicción en distintos formatos como serían una tabla de MongoDB, ficheros csv y en ElasticSearch para su posterior visualización con Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports y configuraciones necesarias\n",
    "# Spark Streaming\n",
    "from pyspark.streaming import StreamingContext  \n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "from pyspark.mllib.util import *\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# cargamos las stopwords para cada idioma\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# !pip install elasticsearch --si fuera necesario instalarlo\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"twitter\")\\\n",
    "        .master(\"spark://MacBook-Pro-de-Jose.local:7077\")\\\n",
    "        .config(\"spark.io.compression.codec\", \"snappy\")\\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\\\n",
    "        .config(\"spark.mongodb.input.uri\",\"mongodb://localhost:27017/tfm_twitter\")\\\n",
    "        .config(\"spark.mongodb.output.uri\",\"mongodb://localhost:27017/tfm_twitter\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos las funciones necesarias para los procesos posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de carga de datos desde los csv generados con anterioridad con registros ya con el sentimiento anotado.\n",
    "# Estos datos servirán para entrenar el modelo con el que predecir los tweets en streaming.\n",
    "def carga_datos_csv(csv):\n",
    "    schema_csv = StructType([ \n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"sentiment\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    # cargamos los csv\n",
    "    if(csv=='english_full'): fichero = './data/df_result_english.csv'\n",
    "    elif(csv=='spanish_full'): fichero = './data/df_result_spanish.csv'\n",
    "    elif(csv=='english_neutro'): fichero = './data/df_result_english_neutral.csv'\n",
    "    elif(csv=='spanish_neutro'): fichero = './data/df_result_spanish_neutral.csv'\n",
    "    elif(csv=='english_noNeutro'): fichero = './data/df_result_english_noNeutral.csv'\n",
    "    elif(csv=='spanish_noNeutro'): fichero = './data/df_result_spanish_noNeutral.csv'\n",
    "    \n",
    "    df_csv = sqlContext.\\\n",
    "        read.format(\"com.databricks.spark.csv\").\\\n",
    "        option(\"header\", \"true\").\\\n",
    "        option(\"inferschema\", \"true\").\\\n",
    "        option(\"mode\", \"DROPMALFORMED\").\\\n",
    "        schema(schema_csv).\\\n",
    "        load(fichero).\\\n",
    "        cache()\n",
    "    \n",
    "    return df_csv\n",
    "\n",
    "\n",
    "# Funciones de visualización de DFs para ver las columnas, el número de registros o dimensiones, y el recuento\n",
    "# de valores del atributo sentimiento.\n",
    "def visualizar_datos_csv(df):\n",
    "    print(\"Columnas del dataframe: \", df.columns)\n",
    "    print(\"Numero de registros = %d\" % df.count())\n",
    "    print(\"\\n\")\n",
    "    print(df.limit(10).toPandas())\n",
    "    print(\"\\n\")\n",
    "    recuento_sentiment = df.select('sentiment').groupBy(\"sentiment\").count().show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# Función para eliminar palabras que no queramos analizar. Serán las palabras más comunes de cada lenguaje, que no\n",
    "# tienen valor para hacer un análisis de sentimiento, como podrían ser artículos o pronombres.\n",
    "def eliminar_stopwords(texto, palabras_eliminar):\n",
    "    tok = nltk.tokenize\n",
    "    palabras = tok.word_tokenize(texto)\n",
    "        \n",
    "    palabras_salida = []\n",
    "        \n",
    "    for palabra in palabras:\n",
    "        if palabra not in palabras_eliminar:\n",
    "            palabras_salida.append(palabra)\n",
    "        \n",
    "    salida = \"\"\n",
    "    for i in range(len(palabras_salida)):\n",
    "        if palabras_salida[i] in string.punctuation:\n",
    "            salida = salida.strip()+palabras_salida[i] + \" \"\n",
    "        else:\n",
    "            salida += palabras_salida[i] + \" \"\n",
    "\n",
    "    return salida\n",
    "\n",
    "\n",
    "# Funciones de limpieza de los tweets, para limpiar los textos de caracteres extraños, números, urls o emoticonos.\n",
    "def limpieza_tweets_spanish(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    # eliminar full_text que es como comienzan los textos que son extendidos\n",
    "    tweet = [re.sub(r'full_text', '', s) for s in tweet] \n",
    "    \n",
    "    # convertir la repetición de una letra más de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "    # eliminar acentos\n",
    "    tweet = [''.join((c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')) for s in tweet]\n",
    "        \n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\º\\'\\’\\‘\\|()%/\\\"{}@;:+\\[\\]\\–\\”\\…\\“\\】\\【=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, spanish_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "# A los datos en inglés le aplicamos sus stopwords correspondientes y no les quitamos los acentos\n",
    "def limpieza_tweets_english(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    # eliminar full_text que es como comienzan los textos que son extendidos\n",
    "    tweet = [re.sub(r'full_text', '', s) for s in tweet] \n",
    "    \n",
    "    # convertir la repetición de una letra más de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "\n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\º\\'\\’\\‘\\|()%/\\\"{}@;:+\\[\\]\\–\\”\\…\\“\\】\\【=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, english_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "# Función de preprocesado de los datos que tokeniza los textos y llama a las funciones de limpieza anteriores.\n",
    "# Devolverá los dos dataframes, con datos en inglés y español, con los datos listos para aplicar el modelo.\n",
    "def preprocesado_dataframe(df1, df2, tipo):\n",
    "    # primero usamos tokenizer y vamos a partir los tweets por palabras\n",
    "    if(tipo==0): tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"token\")\n",
    "    elif(tipo==1): tokenizer = Tokenizer(inputCol = \"texto\", outputCol = \"token\")\n",
    "\n",
    "    df_tokens_english = tokenizer.transform(df1)\n",
    "    df_tokens_spanish = tokenizer.transform(df2)  \n",
    "    \n",
    "    # usamos las funciones de limpieza y preprocesado con ambos DFs ya con los textos divididos en tokens\n",
    "    limpiezaUDF_english = F.udf(limpieza_tweets_english, ArrayType(StringType()))\n",
    "    limpiezaUDF_spanish = F.udf(limpieza_tweets_spanish, ArrayType(StringType()))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.withColumn(\"tokens_clean\", limpiezaUDF_english(df_tokens_english[\"token\"]))\n",
    "    df_tokens_spanish = df_tokens_spanish.withColumn(\"tokens_clean\", limpiezaUDF_spanish(df_tokens_spanish[\"token\"]))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.drop(\"token\")\n",
    "    df_tokens_spanish = df_tokens_spanish.drop(\"token\")\n",
    "\n",
    "    df_tokens_english_clean = df_tokens_english.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    df_tokens_spanish_clean = df_tokens_spanish.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    \n",
    "    return df_tokens_english_clean, df_tokens_spanish_clean\n",
    "\n",
    "\n",
    "# Función que según el valor del atributo Truncated, se queda con el valor de Text o de Extended_tweet, si el texto\n",
    "# del tweet es demasiado largo y está extendido.\n",
    "def udf_cambiar_texto(col1,col2,col3):\n",
    "    if(col1==True):\n",
    "        col2=col3\n",
    "    return col2\n",
    "\n",
    "\n",
    "# Función udf para calcular fecha y hora y añadirla posteriormente al dataframe.\n",
    "def add_timestamp():\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return timestamp\n",
    "\n",
    "\n",
    "# Función que pasa del tipo DenseVector a un array, que es mejor para tratar con estos datos.\n",
    "def sparse_to_array(v):\n",
    "    v = DenseVector(v)\n",
    "    new_array = list([float(x) for x in v])\n",
    "    return new_array\n",
    "\n",
    "\n",
    "# Función udf para según el valor del sentimiento calculado por el modelo, quedarnos con el valor\n",
    "# del array de probabilidad que corresponde a esa etiqueta de sentimiento, y así ver en cada caso el % con el\n",
    "# cual el modelo ha etiquetado el sentimiento.\n",
    "def udf_probability(col1,col2):\n",
    "    if(col2==0.0):\n",
    "        col1=col1[0]\n",
    "    elif(col2==1.0):\n",
    "        col1=col1[1]\n",
    "    elif(col2==2.0):\n",
    "        col1=col1[2]\n",
    "        \n",
    "    return col1\n",
    "\n",
    "\n",
    "# Función que engloba las transformaciones y predicciones a realizar a los datos en streaming, para acabar teniendo\n",
    "# las columnas que queremos de los datos y el sentimiento calculado.\n",
    "def generacion_dataframe_streaming(df_1, df_2, model_1, model_2):\n",
    "    # sumamos las 4 columnas del recuento de replies, favorites, quotes y retweet para tener una sola columna con la\n",
    "    # suma de todas estas interacciones de un tweet.\n",
    "    df_english = df_1.withColumn('interacciones', df_1.favorite_count\\\n",
    "                + df_1.quote_count + df_1.reply_count + df_1.retweet_count)\n",
    "    df_spanish = df_2.withColumn('interacciones', df_2.favorite_count\\\n",
    "                + df_2.quote_count + df_2.reply_count + df_2.retweet_count)\n",
    "\n",
    "    # aplicamos la función para coger el texto de la columna extended_tweet si es necesario, y sustituir al texto.\n",
    "    cambiar_texto_udf=F.udf(udf_cambiar_texto)\n",
    "    df_spanish = df_spanish.select('*').withColumn(\"texto\", cambiar_texto_udf(\"truncated\",\"text\",\"extended_tweet\"))\n",
    "    df_english = df_english.select('*').withColumn(\"texto\", cambiar_texto_udf(\"truncated\",\"text\",\"extended_tweet\"))\n",
    "    \n",
    "    # llamamos a la función que engloba el preprocesado de los datos, con la tokenización y la limpieza de tweets.\n",
    "    df_tokens_english, df_tokens_spanish = preprocesado_dataframe(df_english,df_spanish,1)\n",
    "    df_english_clean = df_tokens_english.withColumnRenamed(\"sentiment\", \"label\")\n",
    "    df_spanish_clean = df_tokens_spanish.withColumnRenamed(\"sentiment\", \"label\")\n",
    "\n",
    "    # usamos el modelo para predecir el sentimiento de los tweets en los distintos DFs por idioma.\n",
    "    pred_eng = model_1.transform(df_english_clean)\n",
    "    pred_spa = model_2.transform(df_spanish_clean)\n",
    "\n",
    "    # usamos la función para añadir la fecha y hora a cada registro\n",
    "    add_timestamp_udf = F.udf(add_timestamp, StringType())\n",
    "    # añadimos la columna con el timestamp a los datos\n",
    "    pred_eng = pred_eng.withColumn(\"timestamp\", add_timestamp_udf())\n",
    "    pred_spa = pred_spa.withColumn(\"timestamp\", add_timestamp_udf())\n",
    "\n",
    "    # pasamos a array el DenseVector con la probabilidad calculada por el modelo al predecir el sentimiento.\n",
    "    sparse_to_array_udf = F.udf(sparse_to_array, ArrayType(FloatType()))\n",
    "    pred_eng = pred_eng.withColumn('probability_array', sparse_to_array_udf('probability'))\n",
    "    pred_spa = pred_spa.withColumn('probability_array', sparse_to_array_udf('probability'))\n",
    "    # del array de probabilidad nos quedamos solo con la probabilidad del sentimiento predecido en cada registro.\n",
    "    probability_udf=F.udf(udf_probability)\n",
    "    pred_eng = pred_eng.withColumn('probabilidad', probability_udf('probability_array','prediction'))\n",
    "    pred_spa = pred_spa.withColumn('probabilidad', probability_udf('probability_array','prediction'))\n",
    "\n",
    "    # eliminamos las columnas que no queremos sacar con los datos, y posteriormente las ordenamos según se desea.\n",
    "    columns_to_drop = ['favorite_count', 'quote_count', 'reply_count', 'retweet_count', 'label',\\\n",
    "                       'countfeatures', 'features', 'rawPrediction', 'tokens_clean', 'sentiment']\n",
    "    pred_spa = pred_spa.drop(*columns_to_drop)\n",
    "    pred_eng = pred_eng.drop(*columns_to_drop)\n",
    "\n",
    "    columns = ['id', 'texto', 'interacciones', 'probabilidad', 'prediction', 'timestamp']\n",
    "    pred_eng = pred_eng[columns] \n",
    "    pred_spa = pred_spa[columns]\n",
    "    \n",
    "    return pred_eng, pred_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de los datos guardados en formato .csv para entrenar el modelo a usar con los tweets que vienen en streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los distintos csv generados con anterioridad y con el sentimiento ya etiquetado.\n",
    "df_csv_english_full = carga_datos_csv('english_full')\n",
    "df_csv_spanish_full = carga_datos_csv('spanish_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 175818\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @VirginAmerica amazing to me that we can't get...          0\n",
      "1  @VirginAmerica View of downtown Los Angeles, t...          2\n",
      "2  @VirginAmerica plz help me win my bid upgrade ...          1\n",
      "3  @VirginAmerica I'm #elevategold for a good rea...          2\n",
      "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1\n",
      "5  @VirginAmerica I’m having trouble adding this ...          0\n",
      "6  @VirginAmerica you have the absolute best team...          2\n",
      "7  @VirginAmerica has flight number 276 from SFO ...          1\n",
      "8  @VirginAmerica Another delayed flight? #liking...          0\n",
      "9  @VirginAmerica Can you find us a flt out of LA...          1\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 1372|\n",
      "|        2|87372|\n",
      "|        0|87074|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cogemos un sample del DF con datos en inglés ya que tiene muchos datos y da problemas de cómputo y tiempos.\n",
    "df_csv_english_sample = df_csv_english_full.sample(False, 0.1, 45)\n",
    "\n",
    "visualizar_datos_csv(df_csv_english_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 46787\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @PauladeLasHeras No te libraras de ayudar me/n...          1\n",
      "1                          @marodriguezb Gracias MAR          2\n",
      "2  Off pensando en el regalito Sinde, la que se v...          0\n",
      "3  Conozco a alguien q es adicto al drama! Ja ja ...          2\n",
      "4  Toca @crackoviadeTV3 . Grabación dl especial N...          2\n",
      "5  Buen día todos! Lo primero mandar un abrazo gr...          2\n",
      "6  Desde el escaño. Todo listo para empezar #endi...          2\n",
      "7  Bdías. EM no se ira de puente. Si vosotros os ...          2\n",
      "8  Un sistema económico q recorta dinero para pre...          2\n",
      "9                  #programascambiados caca d ajuste          0\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 2927|\n",
      "|        2|25392|\n",
      "|        0|18468|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_spanish_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generamos el modelo que se probó anteriormente y dio los mejores resultados, y lo entrenamos con los datos recogidos de los csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la función que engloba el preprocesado de los datos, con la tokenización y la limpieza de tweets.\n",
    "df_tokens_english_clean, df_tokens_spanish_clean = preprocesado_dataframe(df_csv_english_sample,df_csv_spanish_full,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica amazing to me that we can't get...</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazing , cant , get , cold , air , vents , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica View of downtown Los Angeles, t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[view , downtown , los , angeles , hollywood ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[plz , help , win , bid , upgrade , flight , l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica I'm #elevategold for a good rea...</td>\n",
       "      <td>2</td>\n",
       "      <td>[im , elevategold , good , reason , rock ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood Juli...</td>\n",
       "      <td>1</td>\n",
       "      <td>[julie , andrews , first , lady , gaga , wowd ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  @VirginAmerica amazing to me that we can't get...          0   \n",
       "1  @VirginAmerica View of downtown Los Angeles, t...          2   \n",
       "2  @VirginAmerica plz help me win my bid upgrade ...          1   \n",
       "3  @VirginAmerica I'm #elevategold for a good rea...          2   \n",
       "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0  [amazing , cant , get , cold , air , vents , v...  \n",
       "1  [view , downtown , los , angeles , hollywood ,...  \n",
       "2  [plz , help , win , bid , upgrade , flight , l...  \n",
       "3         [im , elevategold , good , reason , rock ]  \n",
       "4  [julie , andrews , first , lady , gaga , wowd ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_english_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>1</td>\n",
       "      <td>[libraras , ayudar , menos , besos , gracias ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2</td>\n",
       "      <td>[gracias , mar ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>0</td>\n",
       "      <td>[off , pensando , regalito , sinde , va , sgae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[conozco , alguien , q , adicto , drama , ja ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "      <td>2</td>\n",
       "      <td>[toca , grabacion , dl , especial , navideno m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  @PauladeLasHeras No te libraras de ayudar me/n...          1   \n",
       "1                          @marodriguezb Gracias MAR          2   \n",
       "2  Off pensando en el regalito Sinde, la que se v...          0   \n",
       "3  Conozco a alguien q es adicto al drama! Ja ja ...          2   \n",
       "4  Toca @crackoviadeTV3 . Grabación dl especial N...          2   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0     [libraras , ayudar , menos , besos , gracias ]  \n",
       "1                                   [gracias , mar ]  \n",
       "2  [off , pensando , regalito , sinde , va , sgae...  \n",
       "3  [conozco , alguien , q , adicto , drama , ja ,...  \n",
       "4  [toca , grabacion , dl , especial , navideno m...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_spanish_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a partir del dataframe anterior, vamos a eliminar la columna texto ya que nos interesa quedarnos con los textos en\n",
    "# limpio y tras pasar por el preprocesado para el modelo.\n",
    "df_tokens_english_clean = df_tokens_english_clean.drop(\"text\")\n",
    "df_tokens_spanish_clean = df_tokens_spanish_clean.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos el nombre de la columna sentiment por label a los dataframes para los modelos.\n",
    "df_english_clean = df_tokens_english_clean.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean = df_tokens_spanish_clean.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generación de conjunto de training y test de los dataframes.\n",
    "train_english_clean, test_english_clean = df_english_clean.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_spanish_clean, test_spanish_clean = df_spanish_clean.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos el pipeline del modelo con sus etapas y parámetros, además de utilizar TrainValidationSplit,\n",
    "# sería la configuración del modelo con el que se obtuvieron los mejores resultados en el notebook de pruebas de ML.\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=MulticlassClassificationEvaluator(),\n",
    "                           trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng = tvs.fit(train_english_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa = tvs.fit(train_spanish_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conectamos con el streaming para recoger los datos de Kafka y tenerlos en un dataframe, al que poder aplicar los modelos para calcular el sentimiento de cada tweet en streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el schema de los datos que queremos guardar de los datos de entrada al sistema.\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"truncated\", BooleanType(), True),\n",
    "    StructField(\"extended_tweet\", StringType(), True),\n",
    "    StructField(\"favorite_count\", IntegerType(), True),\n",
    "    StructField(\"quote_count\", IntegerType(), True),\n",
    "    StructField(\"reply_count\", IntegerType(), True),\n",
    "    StructField(\"retweet_count\", IntegerType(), True),\n",
    "    StructField(\"sentiment\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtienen desde Kafka los datos del topic en español mediante el schema.\n",
    "df_spanish = spark\\\n",
    ".readStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    ".option(\"startingOffsets\", \"latest\")\\\n",
    ".option(\"subscribe\", \"TopicSpanish\")\\\n",
    ".option(\"failOnDataLoss\", \"false\") \\\n",
    ".load()\n",
    "\n",
    "df_spanish = df_spanish.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_spanish = df_spanish.select(F.from_json(F.col(\"value\"), schema).alias(\"data\")).select(\"data.*\")           \n",
    "                                                                                          \n",
    "df_spanish.createOrReplaceTempView(\"spanish_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, truncated: boolean, extended_tweet: string, favorite_count: int, quote_count: int, reply_count: int, retweet_count: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtienen desde Kafka los datos del topic en inglés mediante el schema.\n",
    "df_english = spark\\\n",
    ".readStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9093\")\\\n",
    ".option(\"startingOffsets\", \"latest\")\\\n",
    ".option(\"subscribe\", \"TopicEnglish\")\\\n",
    ".option(\"failOnDataLoss\", \"false\") \\\n",
    ".load()\n",
    "\n",
    "df_english = df_english.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_english = df_english.select(F.from_json(F.col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "df_english.createOrReplaceTempView(\"english_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, truncated: boolean, extended_tweet: string, favorite_count: int, quote_count: int, reply_count: int, retweet_count: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Is the stream ready?\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "# chequeamos los datos en streaming\n",
    "print(\" \")\n",
    "print(\"Is the stream ready?\")\n",
    "print(df_spanish.isStreaming, df_english.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la función que engloba todo el preprocesado y análisis a aplicar a los datos, y nos devuelve los datos\n",
    "# recogidos en streaming con el sentimiento ya calculado usando los modelos entrenados anteriormente.\n",
    "pred_eng, pred_spa = generacion_dataframe_streaming(df_english, df_spanish, model_eng, model_spa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos en los dataframes con las siguientes columnas:\n",
    "- ID: id único del tweet\n",
    "- Texto: el texto del tweet\n",
    "- Interacciones: sería la suma de respuestas, citas, retweets y favoritos del tweet.\n",
    "- Probability: probabilidad con la que se obtiene el valor del sentimiento de cada tweet.\n",
    "- Prediction: etiqueta de sentimiento elegida por el modelo para el tweet.\n",
    "- Timestamp: añadimos fecha y hora del procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, texto: string, interacciones: int, probabilidad: string, prediction: double, timestamp: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, texto: string, interacciones: int, probabilidad: string, prediction: double, timestamp: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pred_eng)\n",
    "display(pred_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por si acaso hubiera duplicados los eliminamos.\n",
    "pred_eng = pred_eng.dropDuplicates(subset=['id'])\n",
    "pred_spa = pred_spa.dropDuplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una función a la que pasamos los datos, y la configuración para lanzar el streaming, y poder ver por\n",
    "# consola el procesamiento de datos en streaming. Procesamos cada segundo, y usando time.sleep paramos luego la \n",
    "# query de forma programada y cómoda.\n",
    "def mostrar_datos(df, outputMode, formato, tiempo, query_name):\n",
    "    query = df.writeStream.outputMode(outputMode).queryName(query_name).format(formato)\\\n",
    "            .option(\"truncate\", \"False\").trigger(processingTime='1 second').start()\n",
    "    time.sleep(tiempo)\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos con los datos en inglés y modo append, donde en cada ejecución sólo saldrán los nuevos registros.\n",
    "mostrar_datos(pred_eng, 'append', 'console', 60, 'query_eng_append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos con los datos en español y modo append, donde en cada ejecución sólo saldrán los nuevos registros.\n",
    "mostrar_datos(pred_spa, 'append', 'console', 60, 'query_spa_append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos con los datos en inglés y modo update, donde en cada ejecución sólo saldrán los nuevos registros y \n",
    "# los que hayan sido actualizados.\n",
    "mostrar_datos(pred_eng, 'update', 'console', 60, 'query_eng_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos con los datos en español y modo update, donde en cada ejecución sólo saldrán los nuevos registros y\n",
    "# los que hayan sido actualizados.\n",
    "mostrar_datos(pred_spa, 'update', 'console', 60, 'query_spa_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para poder insertar en streaming los datos en MongoDB debemos hacer uso de funciones foreachBatch.\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .option(\"database\", \"tfm_twitter\")\\\n",
    "        .option(\"collection\", \"tweets_streaming_english\").save()\n",
    "    \n",
    "def foreach_batch_function_spa(df, epoch_id):\n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .option(\"database\", \"tfm_twitter\")\\\n",
    "        .option(\"collection\", \"tweets_streaming_spanish\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insertamos los datos en inglés\n",
    "mongo_eng = pred_eng.writeStream\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .start()\n",
    "\n",
    "time.sleep(60)\n",
    "mongo_eng.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insertamos los datos en español\n",
    "mongo_spa = pred_spa.writeStream\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .foreachBatch(foreach_batch_function_spa)\\\n",
    "        .start()\n",
    "\n",
    "time.sleep(60)\n",
    "mongo_spa.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para generar el índice\n",
    "def generateIndexName(name):\n",
    "    return(name + time.strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se insertan los datos en inglés en streaming en elasticsearch\n",
    "elastic_eng = pred_eng.writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"es\")\\\n",
    "    .option(\"es.nodes\",\"localhost\")\\\n",
    "    .option(\"es.port\",\"9200\")\\\n",
    "    .option(\"checkpointLocation\",\"/tmp/\")\\\n",
    "    .option(\"es.resource\", generateIndexName('tweets_sentiment_eng_') + \"/tweets_sentiment_eng_\") \\\n",
    "    .option(\"es.codec\", \"best_compression\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(60)\n",
    "elastic_eng.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se insertan los datos en español en streaming en elasticsearch\n",
    "elastic_spa = pred_spa.writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"es\")\\\n",
    "    .option(\"es.nodes\",\"localhost\")\\\n",
    "    .option(\"es.port\",\"9200\")\\\n",
    "    .option(\"checkpointLocation\",\"/tmp/\")\\\n",
    "    .option(\"es.resource\", generateIndexName('tweets_sentiment_spa_') + \"/tweets_sentiment_spa_\") \\\n",
    "    .option(\"es.codec\", \"best_compression\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(60)\n",
    "elastic_spa.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se prueba a hacer las consultas en memoria, obteniendo un dataframe por idioma e insertándolo posteriormente en Mongo y ElasticSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Getting offsets from KafkaV2[Subscribe[TopicSpanish]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[TopicEnglish]]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "# vamos a obtener los datos en memoria\n",
    "query_spanish_memory = pred_spa.writeStream.outputMode(\"append\").queryName(\"spanish_memory\").format(\"memory\")\\\n",
    ".option(\"truncate\", \"False\").start()\n",
    "\n",
    "query_english_memory = pred_eng.writeStream.outputMode(\"append\").queryName(\"english_memory\").format(\"memory\")\\\n",
    ".option(\"truncate\", \"False\").start()\n",
    "\n",
    "print(query_spanish_memory.status)\n",
    "print(query_english_memory.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_spanish = spark.table(\"spanish_memory\")\n",
    "result_english = spark.table(\"english_memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>probabilidad</th>\n",
       "      <th>prediction</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167084394420592642</td>\n",
       "      <td>RT @LegalAppMJ: El Estado obliga a las entidad...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8828351497650146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167084394395459584</td>\n",
       "      <td>RT @Zuanna_Sol: Buenos días 😈😈 https://t.co/tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8228078484535217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167084394395394051</td>\n",
       "      <td>RT @petrogustavo: Por ahora los violentos gana...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9783394932746887</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167084394399580165</td>\n",
       "      <td>Fuck el desayuno merezco el almuerzo</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4916864037513733</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167084394407956483</td>\n",
       "      <td>@Poetadelciel0 Amén 🙏</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7538945078849792</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167084394387050497</td>\n",
       "      <td>RT @anacastelan_: Donde no te buscan, no haces...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5875048637390137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              texto  \\\n",
       "0  1167084394420592642  RT @LegalAppMJ: El Estado obliga a las entidad...   \n",
       "1  1167084394395459584  RT @Zuanna_Sol: Buenos días 😈😈 https://t.co/tr...   \n",
       "2  1167084394395394051  RT @petrogustavo: Por ahora los violentos gana...   \n",
       "3  1167084394399580165               Fuck el desayuno merezco el almuerzo   \n",
       "4  1167084394407956483                              @Poetadelciel0 Amén 🙏   \n",
       "5  1167084394387050497  RT @anacastelan_: Donde no te buscan, no haces...   \n",
       "\n",
       "   interacciones        probabilidad  prediction            timestamp  \n",
       "0              0  0.8828351497650146         0.0  2019-08-29 16:39:58  \n",
       "1              0  0.8228078484535217         2.0  2019-08-29 16:39:58  \n",
       "2              0  0.9783394932746887         2.0  2019-08-29 16:39:58  \n",
       "3              0  0.4916864037513733         1.0  2019-08-29 16:39:58  \n",
       "4              0  0.7538945078849792         2.0  2019-08-29 16:39:58  \n",
       "5              0  0.5875048637390137         0.0  2019-08-29 16:39:58  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos los datos en español\n",
    "result_spanish.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>probabilidad</th>\n",
       "      <th>prediction</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167084394382790657</td>\n",
       "      <td>@BTS_twt OMG MY LOVE😭😭</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6848946213722229</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167084394382856193</td>\n",
       "      <td>@EmmaSpoonley Yes! I know I'm as appalled at m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5481786727905273</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167084394399633410</td>\n",
       "      <td>RT @sawxcy: If you ain’t trynna b like this I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7860847115516663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167084394391265286</td>\n",
       "      <td>RT @nielo_2wit: You cannot always be right... ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6511359214782715</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167084394387054592</td>\n",
       "      <td>RT @AccidentalNut: I got y’all , rt Bc these h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5655750632286072</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              texto  \\\n",
       "0  1167084394382790657                             @BTS_twt OMG MY LOVE😭😭   \n",
       "1  1167084394382856193  @EmmaSpoonley Yes! I know I'm as appalled at m...   \n",
       "2  1167084394399633410  RT @sawxcy: If you ain’t trynna b like this I ...   \n",
       "3  1167084394391265286  RT @nielo_2wit: You cannot always be right... ...   \n",
       "4  1167084394387054592  RT @AccidentalNut: I got y’all , rt Bc these h...   \n",
       "\n",
       "   interacciones        probabilidad  prediction            timestamp  \n",
       "0              0  0.6848946213722229         2.0  2019-08-29 16:39:58  \n",
       "1              0  0.5481786727905273         2.0  2019-08-29 16:39:58  \n",
       "2              0  0.7860847115516663         0.0  2019-08-29 16:39:58  \n",
       "3              0  0.6511359214782715         2.0  2019-08-29 16:39:58  \n",
       "4              0  0.5655750632286072         2.0  2019-08-29 16:39:58  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos los datos en inglés\n",
    "result_english.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end streaming\n",
    "query_spanish_memory.stop()\n",
    "query_english_memory.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almacenar datos en CSV, MongoDB y ElasticSearch.\n",
    "\n",
    "Probamos a almacenar los datos etiquetados en diferentes formatos: archivos csv, MongoDB, y en ElasticSearch para su posterior visualización con Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190829-164021\n"
     ]
    }
   ],
   "source": [
    "# sacamos una cadena con fecha y hora para añadirla al nombre de los csv que se generan a continuación\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasamos los dataframes con el sentimiento calculado a pandas\n",
    "df_eng_pandas = result_english.toPandas()\n",
    "df_spa_pandas = result_spanish.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se guardan los dataframes con el sentimiento calculado en csv\n",
    "df_eng_pandas.to_csv('./data/predict_streaming_english_'+timestr+'.csv', index=False)\n",
    "df_spa_pandas.to_csv('./data/predict_streaming_spanish_'+timestr+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1a22f76388>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardar datos en MongoDB\n",
    "conexion = 'mongodb://localhost:27017'\n",
    "client = MongoClient(conexion)\n",
    "\n",
    "# accedemos a la base de datos\n",
    "db = client.tfm_twitter\n",
    "# insertamos los dataframes en la tabla correspondiente\n",
    "db.tweets_streaming_english.insert_many(df_eng_pandas.to_dict(\"records\"))\n",
    "db.tweets_streaming_spanish.insert_many(df_spa_pandas.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tweets_sentiment_spa_20190829-164021\"}{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tweets_sentiment_eng_20190829-164021\"}True True\n"
     ]
    }
   ],
   "source": [
    "# guardar datos en ElasticSearch\n",
    "es = Elasticsearch('http://localhost:9200/')\n",
    "\n",
    "# añadimos fecha y hora a los índices\n",
    "indice_spa = \"tweets_sentiment_spa_\"+timestr\n",
    "indice_eng = \"tweets_sentiment_eng_\"+timestr\n",
    "\n",
    "# primero comprobamos si ya existen los índices y se borran\n",
    "if es.indices.exists(indice_spa):\n",
    "    !curl -X DELETE localhost:9200/{indice_spa}\n",
    "if es.indices.exists(indice_eng):\n",
    "    !curl -X DELETE localhost:9200/{indice_eng}\n",
    "\n",
    "# generación de índices\n",
    "!curl -X PUT localhost:9200/{indice_spa}\n",
    "!curl -X PUT localhost:9200/{indice_eng}\n",
    "\n",
    "TYPE = \"record\"\n",
    "\n",
    "def rec_to_actions(df, lang):\n",
    "    for record in df.to_dict(orient=\"records\"):\n",
    "        if(lang==0): yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (indice_spa, TYPE))\n",
    "        elif(lang==1): yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (indice_eng, TYPE))\n",
    "        yield (json.dumps(record, default=int))\n",
    "\n",
    "if not es.indices.exists(indice_spa):\n",
    "    raise RuntimeError('index does not exists, use `curl -X PUT \"localhost:9200/%s\"` and try again'%indice_spa)\n",
    "if not es.indices.exists(indice_eng):\n",
    "    raise RuntimeError('index does not exists, use `curl -X PUT \"localhost:9200/%s\"` and try again'%indice_eng)\n",
    "\n",
    "r_spa = es.bulk(rec_to_actions(df_spa_pandas, 0))\n",
    "r_eng = es.bulk(rec_to_actions(df_eng_pandas, 1)) \n",
    "\n",
    "print(not r_spa[\"errors\"], not r_eng[\"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se guardan los modelos utilizados para la predicción\n",
    "model_eng_best = model_eng.bestModel\n",
    "model_spa_best = model_spa.bestModel\n",
    "model_eng_best.write().overwrite().save('./data/models/model_eng_'+timestr)\n",
    "model_spa_best.write().overwrite().save('./data/models/model_spa_'+timestr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
