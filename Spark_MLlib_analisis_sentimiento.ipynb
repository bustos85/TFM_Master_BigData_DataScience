{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Spark Mlib y prueba del modelo con distintos algoritmos de clasificaci√≥n para hacer el an√°lisis de sentimiento de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports y configuraciones necesarias\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# cargamos las stopwords para cada idioma\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "conf = (SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"twitter\")\\\n",
    "          .master(\"spark://MacBook-Pro-de-Jose.local:7077\")\\\n",
    "          .config(\"spark.io.compression.codec\", \"snappy\")\\\n",
    "          .getOrCreate())  \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones necesarias para la preparaci√≥n de los datos y el an√°lisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de carga de datos desde MongoDB de los datos almacenados desde Apache Nifi, tanto en ingl√©s como espa√±ol\n",
    "def carga_datos_mongo():\n",
    "    # conectamos con las tablas de Mongo desde donde cargamos los datos\n",
    "    df_mongo_english = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_english\").load()\n",
    "    df_mongo_spanish = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_spanish\").load()\n",
    "    \n",
    "    # nos quedamos solo con el atributo texto que es la informaci√≥n que nos interesa de cada dataset\n",
    "    df_text_english = df_mongo_english[['text']]\n",
    "    df_text_spanish = df_mongo_spanish[['text']]\n",
    "    \n",
    "    return df_text_english, df_text_spanish\n",
    "\n",
    "\n",
    "# Funci√≥n de carga de datos desde los csv generados con anterioridad con registros ya con el sentimiento anotado\n",
    "def carga_datos_csv(csv):\n",
    "    schema_csv = StructType([ \n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"sentiment\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    # cargamos los csv\n",
    "    if(csv=='english_full'): fichero = './data/df_result_english.csv'\n",
    "    elif(csv=='spanish_full'): fichero = './data/df_result_spanish.csv'\n",
    "    elif(csv=='english_neutro'): fichero = './data/df_result_english_neutral.csv'\n",
    "    elif(csv=='spanish_neutro'): fichero = './data/df_result_spanish_neutral.csv'\n",
    "    elif(csv=='english_noNeutro'): fichero = './data/df_result_english_noNeutral.csv'\n",
    "    elif(csv=='spanish_noNeutro'): fichero = './data/df_result_spanish_noNeutral.csv'\n",
    "    \n",
    "    df_csv = sqlContext.\\\n",
    "        read.format(\"com.databricks.spark.csv\").\\\n",
    "        option(\"header\", \"true\").\\\n",
    "        option(\"inferschema\", \"true\").\\\n",
    "        option(\"mode\", \"DROPMALFORMED\").\\\n",
    "        schema(schema_csv).\\\n",
    "        load(fichero).\\\n",
    "        cache()\n",
    "    \n",
    "    return df_csv\n",
    "\n",
    "\n",
    "# Funciones de visualizaci√≥n de DFs para ver las columnas, el n√∫mero de registros o dimensiones, y el recuento\n",
    "# de valores del atributo sentimiento en caso de tener la columna.\n",
    "def visualizar_datos_csv(df):\n",
    "    print(\"Columnas del dataframe: \", df.columns)\n",
    "    print(\"Numero de registros = %d\" % df.count())\n",
    "    print(\"\\n\")\n",
    "    print(df.limit(10).toPandas())\n",
    "    print(\"\\n\")\n",
    "    recuento_sentiment = df.select('sentiment').groupBy(\"sentiment\").count().show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "def visualizar_datos_mongo(df):\n",
    "    df_pandas = df.toPandas()\n",
    "    print(\"num_rows: %d\\tColumnas: %d\\n\" % (df_pandas.shape[0], df_pandas.shape[1]) )\n",
    "    print(\"Columnas:\\n\", list(df_pandas.columns))\n",
    "    print(df_pandas.head(10))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# funci√≥n para eliminar palabras que no queramos analizar\n",
    "def eliminar_stopwords(texto, palabras_eliminar):\n",
    "    tok = nltk.tokenize\n",
    "    palabras = tok.word_tokenize(texto)\n",
    "        \n",
    "    palabras_salida = []\n",
    "        \n",
    "    for palabra in palabras:\n",
    "        if palabra not in palabras_eliminar:\n",
    "            palabras_salida.append(palabra)\n",
    "        \n",
    "    salida = \"\"\n",
    "    for i in range(len(palabras_salida)):\n",
    "        if palabras_salida[i] in string.punctuation:\n",
    "            salida = salida.strip()+palabras_salida[i] + \" \"\n",
    "        else:\n",
    "            salida += palabras_salida[i] + \" \"\n",
    "\n",
    "    return salida\n",
    "\n",
    "\n",
    "# Funciones de limpieza de los tweets\n",
    "def limpieza_tweets_spanish(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    \n",
    "    # convertir la repetici√≥n de una letra m√°s de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "    # eliminar acentos\n",
    "    tweet = [''.join((c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')) for s in tweet]\n",
    "        \n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\¬∫\\'\\‚Äô\\‚Äò\\|()%/\\\"{}@;:+\\[\\]\\‚Äì\\‚Äù\\‚Ä¶\\‚Äú\\„Äë\\„Äê=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, spanish_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "# a los datos en ingl√©s le aplicamos sus stopwords correspondientes y no les quitamos los acentos\n",
    "def limpieza_tweets_english(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    \n",
    "    # convertir la repetici√≥n de una letra m√°s de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "\n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\¬∫\\'\\‚Äô\\‚Äò\\|()%/\\\"{}@;:+\\[\\]\\‚Äì\\‚Äù\\‚Ä¶\\‚Äú\\„Äë\\„Äê=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, english_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "# Funci√≥n de preprocesado de los datos\n",
    "def preprocesado_dataframe(df1, df2):\n",
    "    # primero usamos tokenizer y vamos a partir los tweets por palabras\n",
    "    tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"token\")\n",
    "\n",
    "    df_tokens_english = tokenizer.transform(df1)\n",
    "    df_tokens_spanish = tokenizer.transform(df2)  \n",
    "    \n",
    "    # usamos las funciones de limpieza y preprocesado con ambos DFs ya con los textos divididos en tokens\n",
    "    limpiezaUDF_english = F.udf(limpieza_tweets_english, ArrayType(StringType()))\n",
    "    limpiezaUDF_spanish = F.udf(limpieza_tweets_spanish, ArrayType(StringType()))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.withColumn(\"tokens_clean\", limpiezaUDF_english(df_tokens_english[\"token\"]))\n",
    "    df_tokens_spanish = df_tokens_spanish.withColumn(\"tokens_clean\", limpiezaUDF_spanish(df_tokens_spanish[\"token\"]))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.drop(\"token\")\n",
    "    df_tokens_spanish = df_tokens_spanish.drop(\"token\")\n",
    "\n",
    "    df_tokens_english_clean = df_tokens_english.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    df_tokens_spanish_clean = df_tokens_spanish.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    \n",
    "    return df_tokens_english_clean, df_tokens_spanish_clean\n",
    "\n",
    "\n",
    "# Funci√≥n evaluaci√≥n de modelo\n",
    "def evaluar_modelo(metric, predCol, labelCol, pred1, pred2):\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric, predictionCol=predCol, labelCol=labelCol)\n",
    "\n",
    "    eval_eng_clean = evaluator.evaluate(pred1)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Medidas de rendimiento datos en ingl√©s\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"F1-score = %f\" % eval_eng_clean)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    eval_spa_clean = evaluator.evaluate(pred2)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Medidas de rendimiento datos en espa√±ol\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"F1-score = %f\" % eval_spa_clean)\n",
    "    print(\"\\n\") \n",
    "\n",
    "\n",
    "# Funci√≥n para ver los mejores par√°metros de un modelo al usar cross-validation\n",
    "def mejores_parametros(model1, model2):\n",
    "    bestPipeline_eng = model1.bestModel\n",
    "    bestPipeline_spa = model2.bestModel\n",
    "\n",
    "    bestVectorizer_eng = bestPipeline_eng.stages[0]\n",
    "    bestVectorizer_spa = bestPipeline_spa.stages[0]\n",
    "\n",
    "    bestParamsVect_eng = bestVectorizer_eng.extractParamMap()\n",
    "    print (\"Vectorizer parameters datos ingl√©s:\")\n",
    "    for k in bestParamsVect_eng.keys():\n",
    "        print (\"  \", k, bestParamsVect_eng[k])\n",
    "    \n",
    "    bestParamsVect_spa = bestVectorizer_spa.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print (\"Vectorizer parameters datos espa√±ol:\")\n",
    "    for k in bestParamsVect_spa.keys():\n",
    "        print (\"  \", k, bestParamsVect_spa[k])\n",
    "\n",
    "    bestModel_eng = bestPipeline_eng.stages[2]\n",
    "    bestModel_spa = bestPipeline_spa.stages[2]\n",
    "\n",
    "    bestParamsModel_eng = bestModel_eng.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print(\"Model parameters datos ingl√©s:\")\n",
    "    for k in bestParamsModel_eng.keys():\n",
    "        print(\"  \", k, bestParamsModel_eng[k])\n",
    "\n",
    "    bestParamsModel_spa = bestModel_spa.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print(\"Model parameters datos espa√±ol:\")\n",
    "    for k in bestParamsModel_spa.keys():\n",
    "        print(\"  \", k, bestParamsModel_spa[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos los datos que pueden ser necesarios tanto para entrenar los modelos como para testearlos.\n",
    "\n",
    "Los datos almacenados en MongoDB que no tienen sentimiento calculado, se pueden utilizar para probar luego el modelo a predecirlo.\n",
    "\n",
    "Los datos que se cargan desde los csv generados con datos en ingl√©s y espa√±ol ya con sentimiento calculado, se van a usar para entrenar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datos de MongoDB que podr√≠an usarse para probar el modelo final\n",
    "df_mongo_english, df_mongo_spanish = carga_datos_mongo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 609133\tColumnas: 1\n",
      "\n",
      "Columnas:\n",
      " ['text']\n",
      "                                                text\n",
      "0                                    @EJFC26 Y Messi\n",
      "1  RT @btsmoonchild64: These group photos deserve...\n",
      "2                        @rehankkhanNDS Overacting *\n",
      "3                              Pay day play dayü§ëü§ëü§ëü§ëü§ë\n",
      "4                             @pandaeyed1 Thank you!\n",
      "5  RT @liamyoung: Strange that Tony Blair has sud...\n",
      "6   Hard work puts you where good luck can find you.\n",
      "7  RT @xCiphxr: When creative kids try playing co...\n",
      "8  RT @bonang_m: I‚Äôm working on one as we speak. ...\n",
      "9  RT @akashbanerjee: After #PulwamaAttack, terro...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_mongo(df_mongo_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 153619\tColumnas: 1\n",
      "\n",
      "Columnas:\n",
      " ['text']\n",
      "                                                text\n",
      "0                                    @EJFC26 Y Messi\n",
      "1     RT @MBelenAlegre: Hermoso y sensual viernes ‚ù£Ô∏è\n",
      "2  RT @Foro_TV: Suman 47 muertos y 640 heridos po...\n",
      "3  RT @revistaetcetera: .@lopezobrador_ dice que ...\n",
      "4  Me retracto de mi respuesta , √©l gobierno se s...\n",
      "5  Anda a saber si te esta eligiendo o sos lo √∫ni...\n",
      "6  RT @JCTrujilloCPCCS: Hay que propiciar una Con...\n",
      "7                            @RicciuP Terraplanistas\n",
      "8  RT @oriolguellipuig: Jeje jeje https://t.co/NU...\n",
      "9  quienes son las veganarcas? no quiero ser part...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_mongo(df_mongo_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datos almacenados en ficheros csv con el sentimiento etiquetado para entrenar modelos\n",
    "df_csv_english_full = carga_datos_csv('english_full')\n",
    "df_csv_spanish_full = carga_datos_csv('spanish_full')\n",
    "df_csv_english_neutro = carga_datos_csv('english_neutro')\n",
    "df_csv_spanish_neutro = carga_datos_csv('spanish_neutro')\n",
    "df_csv_english_noNeutro = carga_datos_csv('english_noNeutro')\n",
    "df_csv_spanish_noNeutro = carga_datos_csv('spanish_noNeutro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 1759091\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0                @VirginAmerica What @dhepburn said.          1\n",
      "1  @VirginAmerica plus you've added commercials t...          2\n",
      "2  @VirginAmerica I didn't today... Must mean I n...          1\n",
      "3  \"@VirginAmerica it's really aggressive to blas...          0\n",
      "4  @VirginAmerica and it's a really big bad thing...          0\n",
      "5    it's really the only bad thing about flying VA\"          0\n",
      "6  @VirginAmerica yes, nearly every time I fly VX...          2\n",
      "7  @VirginAmerica Really missed a prime opportuni...          1\n",
      "8    @virginamerica Well, I didn't‚Ä¶but NOW I DO! :-D          2\n",
      "9  @VirginAmerica it was amazing, and arrived an ...          2\n",
      "\n",
      "\n",
      "+---------+------+\n",
      "|sentiment| count|\n",
      "+---------+------+\n",
      "|        1| 14424|\n",
      "|        2|872815|\n",
      "|        0|871852|\n",
      "+---------+------+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_english_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cogemos un sample del DF con datos en ingl√©s ya que tiene muchos datos y da problemas de c√≥mputo y tiempos\n",
    "df_csv_english_sample = df_csv_english_full.sample(False, 0.1, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 175818\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @VirginAmerica amazing to me that we can't get...          0\n",
      "1  @VirginAmerica View of downtown Los Angeles, t...          2\n",
      "2  @VirginAmerica plz help me win my bid upgrade ...          1\n",
      "3  @VirginAmerica I'm #elevategold for a good rea...          2\n",
      "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1\n",
      "5  @VirginAmerica I‚Äôm having trouble adding this ...          0\n",
      "6  @VirginAmerica you have the absolute best team...          2\n",
      "7  @VirginAmerica has flight number 276 from SFO ...          1\n",
      "8  @VirginAmerica Another delayed flight? #liking...          0\n",
      "9  @VirginAmerica Can you find us a flt out of LA...          1\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 1372|\n",
      "|        2|87372|\n",
      "|        0|87074|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_english_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 46787\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @PauladeLasHeras No te libraras de ayudar me/n...          1\n",
      "1                          @marodriguezb Gracias MAR          2\n",
      "2  Off pensando en el regalito Sinde, la que se v...          0\n",
      "3  Conozco a alguien q es adicto al drama! Ja ja ...          2\n",
      "4  Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...          2\n",
      "5  Buen d√≠a todos! Lo primero mandar un abrazo gr...          2\n",
      "6  Desde el esca√±o. Todo listo para empezar #endi...          2\n",
      "7  Bd√≠as. EM no se ira de puente. Si vosotros os ...          2\n",
      "8  Un sistema econ√≥mico q recorta dinero para pre...          2\n",
      "9                  #programascambiados caca d ajuste          0\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 2927|\n",
      "|        2|25392|\n",
      "|        0|18468|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_spanish_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de los textos antes de hacer el an√°lisis de sentimiento.\n",
    "\n",
    "Se limpiar√°n los textos de elementos y caracteres que no tengan importancia para el an√°lisis a realizar. Adem√°s se sacar√°n los textos tokenizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la funci√≥n que engloba el preprocesado de los datos, con la tokenizaci√≥n y la limpieza de tweets\n",
    "df_tokens_english_clean, df_tokens_spanish_clean = preprocesado_dataframe(df_csv_english_sample,df_csv_spanish_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica amazing to me that we can't get...</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazing , cant , get , cold , air , vents , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica View of downtown Los Angeles, t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[view , downtown , los , angeles , hollywood ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[plz , help , win , bid , upgrade , flight , l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica I'm #elevategold for a good rea...</td>\n",
       "      <td>2</td>\n",
       "      <td>[im , elevategold , good , reason , rock ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood Juli...</td>\n",
       "      <td>1</td>\n",
       "      <td>[julie , andrews , first , lady , gaga , wowd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica I‚Äôm having trouble adding this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[im , trouble , adding , flight , wife , booke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica you have the absolute best team...</td>\n",
       "      <td>2</td>\n",
       "      <td>[absolute , best , team , customer , service ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@VirginAmerica has flight number 276 from SFO ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[flight , number , sfo , cabo , san , lucas , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@VirginAmerica Another delayed flight? #liking...</td>\n",
       "      <td>0</td>\n",
       "      <td>[another , delayed , flight , likingyoulessand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VirginAmerica Can you find us a flt out of LA...</td>\n",
       "      <td>1</td>\n",
       "      <td>[find , us , flt , lax , sooner , midnight , m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@VirginAmerica you have amazing staff &amp;amp; su...</td>\n",
       "      <td>2</td>\n",
       "      <td>[amazing , staff , amp , super , helpful , ran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@VirginAmerica is there special assistance if ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[special , assistance , travel , alone , w , k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@VirginAmerica having problems Flight Booking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[problems , flight , booking , problems , web ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@VirginAmerica How do I reschedule my Cancelle...</td>\n",
       "      <td>0</td>\n",
       "      <td>[reschedule , cancelled , flightled , flights ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@VirginAmerica is the website down?</td>\n",
       "      <td>0</td>\n",
       "      <td>[website ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica  I applied over 2 weeks ago. Ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>[applied , weeks , ago , havent , heard , back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@VirginAmerica Only way to fly! #Elevate #Gold</td>\n",
       "      <td>2</td>\n",
       "      <td>[way , fly , elevate , gold ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@VirginAmerica Having an issue finding a missi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[issue , finding , missing , item , plane , he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@VirginAmerica Because we never rec'd Cancelle...</td>\n",
       "      <td>0</td>\n",
       "      <td>[never , recd , cancelled , flightlation , not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@VirginAmerica why Cancelled Flight flights to...</td>\n",
       "      <td>0</td>\n",
       "      <td>[cancelled , flight , flights , today , precip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment  \\\n",
       "0   @VirginAmerica amazing to me that we can't get...          0   \n",
       "1   @VirginAmerica View of downtown Los Angeles, t...          2   \n",
       "2   @VirginAmerica plz help me win my bid upgrade ...          1   \n",
       "3   @VirginAmerica I'm #elevategold for a good rea...          2   \n",
       "4   @VirginAmerica @ladygaga @carrieunderwood Juli...          1   \n",
       "5   @VirginAmerica I‚Äôm having trouble adding this ...          0   \n",
       "6   @VirginAmerica you have the absolute best team...          2   \n",
       "7   @VirginAmerica has flight number 276 from SFO ...          1   \n",
       "8   @VirginAmerica Another delayed flight? #liking...          0   \n",
       "9   @VirginAmerica Can you find us a flt out of LA...          1   \n",
       "10  @VirginAmerica you have amazing staff &amp; su...          2   \n",
       "11  @VirginAmerica is there special assistance if ...          1   \n",
       "12  @VirginAmerica having problems Flight Booking ...          0   \n",
       "13  @VirginAmerica How do I reschedule my Cancelle...          0   \n",
       "14                @VirginAmerica is the website down?          0   \n",
       "15  @VirginAmerica  I applied over 2 weeks ago. Ha...          0   \n",
       "16     @VirginAmerica Only way to fly! #Elevate #Gold          2   \n",
       "17  @VirginAmerica Having an issue finding a missi...          1   \n",
       "18  @VirginAmerica Because we never rec'd Cancelle...          0   \n",
       "19  @VirginAmerica why Cancelled Flight flights to...          0   \n",
       "\n",
       "                                         tokens_clean  \n",
       "0   [amazing , cant , get , cold , air , vents , v...  \n",
       "1   [view , downtown , los , angeles , hollywood ,...  \n",
       "2   [plz , help , win , bid , upgrade , flight , l...  \n",
       "3          [im , elevategold , good , reason , rock ]  \n",
       "4   [julie , andrews , first , lady , gaga , wowd ...  \n",
       "5   [im , trouble , adding , flight , wife , booke...  \n",
       "6   [absolute , best , team , customer , service ,...  \n",
       "7   [flight , number , sfo , cabo , san , lucas , ...  \n",
       "8   [another , delayed , flight , likingyoulessand...  \n",
       "9   [find , us , flt , lax , sooner , midnight , m...  \n",
       "10  [amazing , staff , amp , super , helpful , ran...  \n",
       "11  [special , assistance , travel , alone , w , k...  \n",
       "12  [problems , flight , booking , problems , web ...  \n",
       "13  [reschedule , cancelled , flightled , flights ...  \n",
       "14                                         [website ]  \n",
       "15  [applied , weeks , ago , havent , heard , back...  \n",
       "16                      [way , fly , elevate , gold ]  \n",
       "17  [issue , finding , missing , item , plane , he...  \n",
       "18  [never , recd , cancelled , flightlation , not...  \n",
       "19  [cancelled , flight , flights , today , precip...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos un grupo de datos para ver el correcto preprocesamiento del texto\n",
    "df_tokens_english_clean.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>1</td>\n",
       "      <td>[libraras , ayudar , menos , besos , gracias ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2</td>\n",
       "      <td>[gracias , mar ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>0</td>\n",
       "      <td>[off , pensando , regalito , sinde , va , sgae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[conozco , alguien , q , adicto , drama , ja ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...</td>\n",
       "      <td>2</td>\n",
       "      <td>[toca , grabacion , dl , especial , navideno m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Buen d√≠a todos! Lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "      <td>[buen , dia , primero , mandar , abrazo , gran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Desde el esca√±o. Todo listo para empezar #endi...</td>\n",
       "      <td>2</td>\n",
       "      <td>[escano , listo , empezar , endiascomohoy , co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bd√≠as. EM no se ira de puente. Si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[bdias , em , ira , puente , si , vais , dejei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Un sistema econ√≥mico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "      <td>[sistema , economico , q , recorta , dinero , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "      <td>[programascambiados , caca , d , ajuste ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Buen viernes</td>\n",
       "      <td>2</td>\n",
       "      <td>[buen , viernes ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>‚Äú@adri_22_22: #programascambiados es TT gracia...</td>\n",
       "      <td>2</td>\n",
       "      <td>[programascambiados , tt , gracias , gracias ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>La Universidad conf√≠a en De la Calle para enca...</td>\n",
       "      <td>2</td>\n",
       "      <td>[universidad , confia , calle , encarar , reto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>¬øMe ayud√°is a que #indultoneiro sea TT? Por si...</td>\n",
       "      <td>2</td>\n",
       "      <td>[ayudais , indultoneiro , tt , si , zapatero ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>abcdesevilla.es: Recio no tiene ¬´indicios pote...</td>\n",
       "      <td>0</td>\n",
       "      <td>[abcdesevilla.es , recio , indicios , potentes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abcdesevilla.es: Cuatro altos cargos de Empleo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[abcdesevilla.es , cuatro , altos , cargos , e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>La marcha atr√°s del PP en posponer devoluci√≥n ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[marcha , atras , pp , posponer , devolucion ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Viernes negro: Aumenta el paro en Noviembre y ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[viernes , negro , aumenta , paro , noviembre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Accidente en BUS-VAO A-6 km. 12. Motorista de ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[accidente , busvao , km , motorista , anos , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#FF a ti, que deseas desesperadamente hacerme ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ff , deseas , desesperadamente , hacerme , ff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment  \\\n",
       "0   @PauladeLasHeras No te libraras de ayudar me/n...          1   \n",
       "1                           @marodriguezb Gracias MAR          2   \n",
       "2   Off pensando en el regalito Sinde, la que se v...          0   \n",
       "3   Conozco a alguien q es adicto al drama! Ja ja ...          2   \n",
       "4   Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...          2   \n",
       "5   Buen d√≠a todos! Lo primero mandar un abrazo gr...          2   \n",
       "6   Desde el esca√±o. Todo listo para empezar #endi...          2   \n",
       "7   Bd√≠as. EM no se ira de puente. Si vosotros os ...          2   \n",
       "8   Un sistema econ√≥mico q recorta dinero para pre...          2   \n",
       "9                   #programascambiados caca d ajuste          0   \n",
       "10                                       Buen viernes          2   \n",
       "11  ‚Äú@adri_22_22: #programascambiados es TT gracia...          2   \n",
       "12  La Universidad conf√≠a en De la Calle para enca...          2   \n",
       "13  ¬øMe ayud√°is a que #indultoneiro sea TT? Por si...          2   \n",
       "14  abcdesevilla.es: Recio no tiene ¬´indicios pote...          0   \n",
       "15  abcdesevilla.es: Cuatro altos cargos de Empleo...          0   \n",
       "16  La marcha atr√°s del PP en posponer devoluci√≥n ...          0   \n",
       "17  Viernes negro: Aumenta el paro en Noviembre y ...          0   \n",
       "18  Accidente en BUS-VAO A-6 km. 12. Motorista de ...          0   \n",
       "19  #FF a ti, que deseas desesperadamente hacerme ...          0   \n",
       "\n",
       "                                         tokens_clean  \n",
       "0      [libraras , ayudar , menos , besos , gracias ]  \n",
       "1                                    [gracias , mar ]  \n",
       "2   [off , pensando , regalito , sinde , va , sgae...  \n",
       "3   [conozco , alguien , q , adicto , drama , ja ,...  \n",
       "4   [toca , grabacion , dl , especial , navideno m...  \n",
       "5   [buen , dia , primero , mandar , abrazo , gran...  \n",
       "6   [escano , listo , empezar , endiascomohoy , co...  \n",
       "7   [bdias , em , ira , puente , si , vais , dejei...  \n",
       "8   [sistema , economico , q , recorta , dinero , ...  \n",
       "9           [programascambiados , caca , d , ajuste ]  \n",
       "10                                  [buen , viernes ]  \n",
       "11  [programascambiados , tt , gracias , gracias ,...  \n",
       "12  [universidad , confia , calle , encarar , reto...  \n",
       "13  [ayudais , indultoneiro , tt , si , zapatero ,...  \n",
       "14  [abcdesevilla.es , recio , indicios , potentes...  \n",
       "15  [abcdesevilla.es , cuatro , altos , cargos , e...  \n",
       "16  [marcha , atras , pp , posponer , devolucion ,...  \n",
       "17  [viernes , negro , aumenta , paro , noviembre ...  \n",
       "18  [accidente , busvao , km , motorista , anos , ...  \n",
       "19  [ff , deseas , desesperadamente , hacerme , ff...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_spanish_clean.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de Machine Learning.\n",
    "\n",
    "Vamos a generar los modelos de aprendizaje supervisado, utilizando distintos algoritmos igual que se realiz√≥ con la librer√≠a Scikit-learn.\n",
    "\n",
    "Usaremos los datos en ingl√©s y espa√±ol despu√©s del preprocesado y limpieza realizado anteriormente.\n",
    "\n",
    "Para los modelos, se hace uso de Pipeline igual que en Scikit-learn, que contendr√° el flujo de transformers y estimators que son necesarios para el modelo.\n",
    "\n",
    "Para evaluar el modelo se utilizar√° \"MulticlassClassificationEvaluator\", ya que es un problema de clasificaci√≥n con m√°s de 2 clases. La m√©trica que se tendr√° en cuenta es F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a partir del dataframe anterior, vamos a eliminar la columna texto en los datos que han pasado por la limpieza y\n",
    "# preprocesado.\n",
    "df_tokens_english_clean = df_tokens_english_clean.drop(\"text\")\n",
    "df_tokens_spanish_clean = df_tokens_spanish_clean.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[amazing , cant , get , cold , air , vents , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[view , downtown , los , angeles , hollywood ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[plz , help , win , bid , upgrade , flight , l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[im , elevategold , good , reason , rock ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[julie , andrews , first , lady , gaga , wowd ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                       tokens_clean\n",
       "0          0  [amazing , cant , get , cold , air , vents , v...\n",
       "1          2  [view , downtown , los , angeles , hollywood ,...\n",
       "2          1  [plz , help , win , bid , upgrade , flight , l...\n",
       "3          2         [im , elevategold , good , reason , rock ]\n",
       "4          1  [julie , andrews , first , lady , gaga , wowd ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_english_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos el nombre de la columna sentiment por label a los dataframes\n",
    "df_english_clean = df_tokens_english_clean.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean = df_tokens_spanish_clean.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generaci√≥n de conjunto de training y test de los dataframes\n",
    "train_english, test_english = df_english_clean.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_spanish, test_spanish = df_spanish_clean.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 1205|\n",
      "|    2|78215|\n",
      "|    0|78116|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vemos el recuento de registros en los conjuntos de train y test por cada clase de sentimiento\n",
    "train_english.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  141|\n",
      "|    2| 8716|\n",
      "|    0| 8539|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_english.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 2619|\n",
      "|    2|22781|\n",
      "|    0|16612|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_spanish.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|  308|\n",
      "|    2| 2589|\n",
      "|    0| 1856|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_spanish.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los pipeline para cada modelo con las etapas o pasos que correspondan.\n",
    "# primero probamos con Hashing y logisticRegression como algoritmo\n",
    "hashingTF = HashingTF(inputCol=\"tokens_clean\", outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_lr = pipeline_lr.fit(train_english)\n",
    "model_spa_lr = pipeline_lr.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_lr = model_eng_lr.transform(test_english)\n",
    "pred_spa_lr = model_spa_lr.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|                  tokens_clean|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|[yach , allah , tawne , saw...|[0.9999999745421377,2.54446...|    0|       0.0|\n",
      "|[grumpy , tomorrow , hes , ...|[0.999999974241276,2.566668...|    0|       0.0|\n",
      "|[boo , still , mia , printe...|[0.9999999538056606,4.61111...|    0|       0.0|\n",
      "|[chickens , chickens , damn...|[0.9999999238062758,7.59345...|    0|       0.0|\n",
      "|[pts , expired , made , prc...|[0.9999999049176175,9.50285...|    0|       0.0|\n",
      "|[today , im , really , amaz...|[0.9999998019933557,7.29863...|    0|       0.0|\n",
      "|[going , chumlees , chinese...|[0.9999997774096685,2.20251...|    0|       0.0|\n",
      "|[ohgosh , hate , recession ...|[0.9999997621053274,2.33153...|    0|       0.0|\n",
      "|[wow , feel , like , ish , ...|[0.9999997229447234,2.64365...|    0|       0.0|\n",
      "|[seriouslyy , really , hate...|[0.9999996278249694,3.04794...|    0|       0.0|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "\n",
    "pred_eng_lr.filter(pred_eng_lr['prediction'] == 0) \\\n",
    "    .select(\"tokens_clean\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|                  tokens_clean|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|[k , k , detuvieron , k , d...|[0.9999999987991242,9.12566...|    0|       0.0|\n",
      "|[patxi , lopez , anuncia , ...|[0.999999998509266,1.268585...|    0|       0.0|\n",
      "|[paramo , huye , preguntas ...|[0.9999999979516403,6.26268...|    0|       0.0|\n",
      "|[sabiais , grinan , generad...|[0.9999999974993734,1.67380...|    0|       0.0|\n",
      "|[muerto , heridos , graves ...|[0.9999999962963302,3.70298...|    0|       0.0|\n",
      "|[explosion , gas , posible ...|[0.9999999939276418,6.05849...|    0|       0.0|\n",
      "|[dicen , escuchamos , gemel...|[0.9999999923347495,5.98716...|    0|       0.0|\n",
      "|[deficit , ciento , recesio...|[0.9999999920444488,3.35025...|    0|       0.0|\n",
      "|[acusa , grinan , confundir...|[0.9999999898535853,9.83601...|    0|       0.0|\n",
      "|[parece , grave , traten , ...|[0.9999999864194309,1.33525...|    0|       0.0|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "\n",
    "pred_spa_lr.filter(pred_spa_lr['prediction'] == 0) \\\n",
    "    .select(\"tokens_clean\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.716218\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.758850\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_lr, pred_spa_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a probar cambiando HashingVectorizer por CountVectorizer\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline_countVec = Pipeline(stages=[countVec, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_countVec = pipeline_countVec.fit(train_english)\n",
    "model_spa_countVec = pipeline_countVec.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_countVec = model_eng_countVec.transform(test_english)\n",
    "pred_spa_countVec = model_spa_countVec.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.727583\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.765104\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_countVec, pred_spa_countVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a probar a a√±adir despu√©s del vectorizador \"countVectorizer\" el uso de TD-IDF\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline_idf = Pipeline(stages=[countVec, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_idf = pipeline_idf.fit(train_english)\n",
    "model_spa_idf = pipeline_idf.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_idf = model_eng_idf.transform(test_english)\n",
    "pred_spa_idf = model_spa_idf.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.738500\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.769939\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_idf, pred_spa_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probar distintos algoritmos.\n",
    "\n",
    "Vamos ahora a probar con el √∫ltimo pipeline generado, que se hab√≠an mejorado ligeramente los resultados, con los algoritmos de Naive Bayes y Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo Naive Bayes\n",
    "nb = NaiveBayes(smoothing=1, modelType=\"multinomial\")\n",
    "\n",
    "pipeline_nb = Pipeline(stages=[countVec, idf, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_nb = pipeline_nb.fit(train_english)\n",
    "model_spa_nb = pipeline_nb.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_nb = model_eng_nb.transform(test_english)\n",
    "pred_spa_nb = model_spa_nb.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.734045\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.762048\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_nb, pred_spa_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo Random Forest\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 20, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "pipeline_rf = Pipeline(stages=[countVec, idf, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng_rf = pipeline_rf.fit(train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa_rf = pipeline_rf.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_rf = model_eng_rf.transform(test_english)\n",
    "pred_spa_rf = model_spa_rf.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.553909\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.384633\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_rf, pred_spa_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecci√≥n de modelos y ajuste de los hiperpar√°metros.\n",
    "\n",
    "Hasta ahora los mejores resultados se han conseguido usando el algoritmo de LogisticRegression y Naive Bayes. Ahora vamos a probar a realizar ajuste de hiperpar√°metros usando CrossValidator y TrainValidationSplit.\n",
    "Para ver los mejores hiperpar√°metros para el modelo se usan conjuntos de validaci√≥n. \n",
    "\n",
    "Durante el proceso de validaci√≥n, el conjunto de datos se divide en dos (entrenamiento y validaci√≥n). El conjunto de entrenamiento se utiliza para entrenar modelos con cada uno de los posibles conjuntos de valores de los hiperpar√°metros. Cada uno de estos modelos se eval√∫a sobre el conjunto de validaci√≥n, y el que da los mejores resultados (de acuerdo al Evaluator) es seleccionado. \n",
    "\n",
    "El modelo final se vuelve a entrenar con todos los datos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el pipeline probado con el algoritmo de clasificaci√≥n Naive Bayes\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "nb = NaiveBayes(smoothing=1, modelType=\"multinomial\")\n",
    "\n",
    "pipeline = Pipeline(stages=[countVec, idf, nb])\n",
    "\n",
    "#\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]) \\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "    \n",
    "#\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos cross-validation\n",
    "model_eng_cv = crossval.fit(train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa_cv = crossval.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer parameters datos ingl√©s:\n",
      "   CountVectorizer_b616a494e8fb__binary False\n",
      "   CountVectorizer_b616a494e8fb__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_b616a494e8fb__minDF 1.0\n",
      "   CountVectorizer_b616a494e8fb__minTF 1.0\n",
      "   CountVectorizer_b616a494e8fb__outputCol countfeatures\n",
      "   CountVectorizer_b616a494e8fb__vocabSize 262144\n",
      "   CountVectorizer_b616a494e8fb__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Vectorizer parameters datos espa√±ol:\n",
      "   CountVectorizer_b616a494e8fb__binary False\n",
      "   CountVectorizer_b616a494e8fb__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_b616a494e8fb__minDF 1.0\n",
      "   CountVectorizer_b616a494e8fb__minTF 1.0\n",
      "   CountVectorizer_b616a494e8fb__outputCol countfeatures\n",
      "   CountVectorizer_b616a494e8fb__vocabSize 262144\n",
      "   CountVectorizer_b616a494e8fb__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Model parameters datos ingl√©s:\n",
      "   NaiveBayes_a63bf89f899a__featuresCol features\n",
      "   NaiveBayes_a63bf89f899a__labelCol label\n",
      "   NaiveBayes_a63bf89f899a__modelType multinomial\n",
      "   NaiveBayes_a63bf89f899a__predictionCol prediction\n",
      "   NaiveBayes_a63bf89f899a__probabilityCol probability\n",
      "   NaiveBayes_a63bf89f899a__rawPredictionCol rawPrediction\n",
      "   NaiveBayes_a63bf89f899a__smoothing 1.0\n",
      "\n",
      "\n",
      "Model parameters datos espa√±ol:\n",
      "   NaiveBayes_a63bf89f899a__featuresCol features\n",
      "   NaiveBayes_a63bf89f899a__labelCol label\n",
      "   NaiveBayes_a63bf89f899a__modelType multinomial\n",
      "   NaiveBayes_a63bf89f899a__predictionCol prediction\n",
      "   NaiveBayes_a63bf89f899a__probabilityCol probability\n",
      "   NaiveBayes_a63bf89f899a__rawPredictionCol rawPrediction\n",
      "   NaiveBayes_a63bf89f899a__smoothing 1.0\n"
     ]
    }
   ],
   "source": [
    "# mostrar los mejores par√°metros\n",
    "mejores_parametros(model_eng_cv, model_spa_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_cv = model_eng_cv.transform(test_english)\n",
    "pred_spa_cv = model_spa_cv.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.734045\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.762048\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_cv, pred_spa_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora probamos el uso de CrossValidator con el algoritmo de LogisticRegression\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline_cv_lr = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid_cv_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "crossval_cv_lr = CrossValidator(estimator=pipeline_cv_lr,\n",
    "                          estimatorParamMaps=paramGrid_cv_lr,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eng_cv_lr = crossval_cv_lr.fit(train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa_cv_lr = crossval_cv_lr.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer parameters datos ingl√©s:\n",
      "   CountVectorizer_770593e2c78e__binary False\n",
      "   CountVectorizer_770593e2c78e__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_770593e2c78e__minDF 1.0\n",
      "   CountVectorizer_770593e2c78e__minTF 1.0\n",
      "   CountVectorizer_770593e2c78e__outputCol countfeatures\n",
      "   CountVectorizer_770593e2c78e__vocabSize 262144\n",
      "   CountVectorizer_770593e2c78e__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Vectorizer parameters datos espa√±ol:\n",
      "   CountVectorizer_770593e2c78e__binary False\n",
      "   CountVectorizer_770593e2c78e__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_770593e2c78e__minDF 1.0\n",
      "   CountVectorizer_770593e2c78e__minTF 1.0\n",
      "   CountVectorizer_770593e2c78e__outputCol countfeatures\n",
      "   CountVectorizer_770593e2c78e__vocabSize 262144\n",
      "   CountVectorizer_770593e2c78e__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Model parameters datos ingl√©s:\n",
      "   LogisticRegression_fcdb86672b4e__aggregationDepth 2\n",
      "   LogisticRegression_fcdb86672b4e__elasticNetParam 0.1\n",
      "   LogisticRegression_fcdb86672b4e__family auto\n",
      "   LogisticRegression_fcdb86672b4e__featuresCol features\n",
      "   LogisticRegression_fcdb86672b4e__fitIntercept True\n",
      "   LogisticRegression_fcdb86672b4e__labelCol label\n",
      "   LogisticRegression_fcdb86672b4e__maxIter 5\n",
      "   LogisticRegression_fcdb86672b4e__predictionCol prediction\n",
      "   LogisticRegression_fcdb86672b4e__probabilityCol probability\n",
      "   LogisticRegression_fcdb86672b4e__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_fcdb86672b4e__regParam 0.01\n",
      "   LogisticRegression_fcdb86672b4e__standardization True\n",
      "   LogisticRegression_fcdb86672b4e__threshold 0.5\n",
      "   LogisticRegression_fcdb86672b4e__tol 1e-06\n",
      "\n",
      "\n",
      "Model parameters datos espa√±ol:\n",
      "   LogisticRegression_fcdb86672b4e__aggregationDepth 2\n",
      "   LogisticRegression_fcdb86672b4e__elasticNetParam 0.2\n",
      "   LogisticRegression_fcdb86672b4e__family auto\n",
      "   LogisticRegression_fcdb86672b4e__featuresCol features\n",
      "   LogisticRegression_fcdb86672b4e__fitIntercept True\n",
      "   LogisticRegression_fcdb86672b4e__labelCol label\n",
      "   LogisticRegression_fcdb86672b4e__maxIter 5\n",
      "   LogisticRegression_fcdb86672b4e__predictionCol prediction\n",
      "   LogisticRegression_fcdb86672b4e__probabilityCol probability\n",
      "   LogisticRegression_fcdb86672b4e__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_fcdb86672b4e__regParam 0.01\n",
      "   LogisticRegression_fcdb86672b4e__standardization True\n",
      "   LogisticRegression_fcdb86672b4e__threshold 0.5\n",
      "   LogisticRegression_fcdb86672b4e__tol 1e-06\n"
     ]
    }
   ],
   "source": [
    "# mostrar los mejores par√°metros\n",
    "mejores_parametros(model_eng_cv_lr, model_spa_cv_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_cv_lr = model_eng_cv_lr.transform(test_english)\n",
    "pred_spa_cv_lr = model_spa_cv_lr.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.760526\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.806676\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_cv_lr, pred_spa_cv_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas con los otros datasets cargados desde ficheros csv con datos ya etiquetados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar los otros datasets cargados desde ficheros csv:\n",
    "- Con datos en ingl√©s y espa√±ol con el mismo balance de registros entre las 3 categor√≠as.\n",
    "- Con datos en ingl√©s y espa√±ol con el mismo n√∫mero de registros de solo 2 categor√≠as: negativo y positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la funci√≥n que engloba el preprocesado de los datos, con la tokenizaci√≥n y la limpieza de tweets\n",
    "df_tokens_english_clean_neutro, df_tokens_spanish_clean_neutro = preprocesado_dataframe(df_csv_english_neutro,\\\n",
    "                                                                          df_csv_spanish_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_english_clean_neutro = df_tokens_english_clean_neutro.drop(\"text\")\n",
    "df_tokens_spanish_clean_neutro = df_tokens_spanish_clean_neutro.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english_clean_neutro = df_tokens_english_clean_neutro.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean_neutro = df_tokens_spanish_clean_neutro.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_english_neutro, test_english_neutro = df_english_clean_neutro.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_spanish_neutro, test_spanish_neutro = df_spanish_clean_neutro.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|12360|\n",
      "|    2|12502|\n",
      "|    0|12552|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_english_neutro.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 2536|\n",
      "|    2| 2622|\n",
      "|    0| 2539|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_spanish_neutro.select('label').groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a lanzar una prueba con crossvalidator y logisticRegression, con los datos que tienen practicamente el mismo\n",
    "# n√∫mero de registros de las 3 clases distintas de sentimiento.\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline_cv_lr_neutro = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid_cv_lr_neutro = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "crossval_cv_lr_neutro = CrossValidator(estimator=pipeline_cv_lr_neutro,\n",
    "                          estimatorParamMaps=paramGrid_cv_lr_neutro,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eng_cv_lr_neutro = crossval_cv_lr_neutro.fit(train_english_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa_cv_lr_neutro = crossval_cv_lr_neutro.fit(train_spanish_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer parameters datos ingl√©s:\n",
      "   CountVectorizer_9187c9b13a2b__binary False\n",
      "   CountVectorizer_9187c9b13a2b__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_9187c9b13a2b__minDF 1.0\n",
      "   CountVectorizer_9187c9b13a2b__minTF 1.0\n",
      "   CountVectorizer_9187c9b13a2b__outputCol countfeatures\n",
      "   CountVectorizer_9187c9b13a2b__vocabSize 262144\n",
      "   CountVectorizer_9187c9b13a2b__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Vectorizer parameters datos espa√±ol:\n",
      "   CountVectorizer_9187c9b13a2b__binary False\n",
      "   CountVectorizer_9187c9b13a2b__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_9187c9b13a2b__minDF 1.0\n",
      "   CountVectorizer_9187c9b13a2b__minTF 1.0\n",
      "   CountVectorizer_9187c9b13a2b__outputCol countfeatures\n",
      "   CountVectorizer_9187c9b13a2b__vocabSize 262144\n",
      "   CountVectorizer_9187c9b13a2b__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Model parameters datos ingl√©s:\n",
      "   LogisticRegression_35077ec1faf4__aggregationDepth 2\n",
      "   LogisticRegression_35077ec1faf4__elasticNetParam 0.2\n",
      "   LogisticRegression_35077ec1faf4__family auto\n",
      "   LogisticRegression_35077ec1faf4__featuresCol features\n",
      "   LogisticRegression_35077ec1faf4__fitIntercept True\n",
      "   LogisticRegression_35077ec1faf4__labelCol label\n",
      "   LogisticRegression_35077ec1faf4__maxIter 5\n",
      "   LogisticRegression_35077ec1faf4__predictionCol prediction\n",
      "   LogisticRegression_35077ec1faf4__probabilityCol probability\n",
      "   LogisticRegression_35077ec1faf4__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_35077ec1faf4__regParam 0.01\n",
      "   LogisticRegression_35077ec1faf4__standardization True\n",
      "   LogisticRegression_35077ec1faf4__threshold 0.5\n",
      "   LogisticRegression_35077ec1faf4__tol 1e-06\n",
      "\n",
      "\n",
      "Model parameters datos espa√±ol:\n",
      "   LogisticRegression_35077ec1faf4__aggregationDepth 2\n",
      "   LogisticRegression_35077ec1faf4__elasticNetParam 0.1\n",
      "   LogisticRegression_35077ec1faf4__family auto\n",
      "   LogisticRegression_35077ec1faf4__featuresCol features\n",
      "   LogisticRegression_35077ec1faf4__fitIntercept True\n",
      "   LogisticRegression_35077ec1faf4__labelCol label\n",
      "   LogisticRegression_35077ec1faf4__maxIter 20\n",
      "   LogisticRegression_35077ec1faf4__predictionCol prediction\n",
      "   LogisticRegression_35077ec1faf4__probabilityCol probability\n",
      "   LogisticRegression_35077ec1faf4__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_35077ec1faf4__regParam 0.1\n",
      "   LogisticRegression_35077ec1faf4__standardization True\n",
      "   LogisticRegression_35077ec1faf4__threshold 0.5\n",
      "   LogisticRegression_35077ec1faf4__tol 1e-06\n"
     ]
    }
   ],
   "source": [
    "# mostrar los mejores par√°metros\n",
    "mejores_parametros(model_eng_cv_lr_neutro, model_spa_cv_lr_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_cv_lr_neutro = model_eng_cv_lr_neutro.transform(test_english_neutro)\n",
    "pred_spa_cv_lr_neutro = model_spa_cv_lr_neutro.transform(test_spanish_neutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.590517\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.625248\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_cv_lr_neutro, pred_spa_cv_lr_neutro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba con TrainValidationSplit en lugar de CrossValidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos el modelo con los mejores resultados, y el uso de TrainValidationSplit en lugar de CrossValidation\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline_cv_lr = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid_cv_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline_cv_lr,\n",
    "                           estimatorParamMaps=paramGrid_cv_lr,\n",
    "                           evaluator=MulticlassClassificationEvaluator(),\n",
    "                           trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tvs_eng = tvs.fit(train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tvs_spa = tvs.fit(train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_tvs = model_tvs_eng.transform(test_english)\n",
    "pred_spa_tvs = model_tvs_spa.transform(test_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.761035\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.801400\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng_tvs, pred_spa_tvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probar el modelo para predecir los tweets sin etiquetar recogidos de MongoDB.\n",
    "\n",
    "Usando el modelo anterior, donde se han obtenido m√°s o menos los mejores resultados que con el mejor anterior (sobre el 76% con datos en ingl√©s, y un 80% con datos en espa√±ol, vamos a predecir el sentimiento de los datos almacenados en MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cogemos un sample del DF con datos en ingl√©s ya que tiene muchos datos y da problemas de c√≥mputo y tiempos\n",
    "df_mongo_english_sample = df_mongo_english.sample(False, 0.4, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la funci√≥n que engloba el preprocesado de los datos, con la tokenizaci√≥n y la limpieza de tweets\n",
    "df_tokens_english_clean_mongo, df_tokens_spanish_clean_mongo = preprocesado_dataframe(df_mongo_english_sample,\\\n",
    "                                                                          df_mongo_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @liamyoung: Strange that Tony Blair has sud...</td>\n",
       "      <td>[strange , tony , blair , suddenly , become , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hard work puts you where good luck can find you.</td>\n",
       "      <td>[hard , work , puts , good , luck , find ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @xCiphxr: When creative kids try playing co...</td>\n",
       "      <td>[creative , kids , try , playing , comp ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @bonang_m: I‚Äôm working on one as we speak. ...</td>\n",
       "      <td>[im , working , one , speak , thank , incredib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @akashbanerjee: After #PulwamaAttack, terro...</td>\n",
       "      <td>[pulwamaattack , terrorists , scored , bigger ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @godisgodisback: Beyonc√©: WORLD STOP!\\n\\nWo...</td>\n",
       "      <td>[beyonc , world , stop , world ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No. It just needs money and connections. Your ...</td>\n",
       "      <td>[needs , money , connections , average , norma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @hopeoverfear01: It‚Äôs time to run our own c...</td>\n",
       "      <td>[time , run , country , retweet , agree ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @gugugaga__: this works. i‚Äôm not even gonna...</td>\n",
       "      <td>[works , im , even , gon na , front , retweete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @pushetblur: Trying to prove a point. Lets ...</td>\n",
       "      <td>[trying , prove , point , lets , settle , retw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT @liamyoung: Strange that Tony Blair has sud...   \n",
       "1   Hard work puts you where good luck can find you.   \n",
       "2  RT @xCiphxr: When creative kids try playing co...   \n",
       "3  RT @bonang_m: I‚Äôm working on one as we speak. ...   \n",
       "4  RT @akashbanerjee: After #PulwamaAttack, terro...   \n",
       "5  RT @godisgodisback: Beyonc√©: WORLD STOP!\\n\\nWo...   \n",
       "6  No. It just needs money and connections. Your ...   \n",
       "7  RT @hopeoverfear01: It‚Äôs time to run our own c...   \n",
       "8  RT @gugugaga__: this works. i‚Äôm not even gonna...   \n",
       "9  RT @pushetblur: Trying to prove a point. Lets ...   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0  [strange , tony , blair , suddenly , become , ...  \n",
       "1         [hard , work , puts , good , luck , find ]  \n",
       "2          [creative , kids , try , playing , comp ]  \n",
       "3  [im , working , one , speak , thank , incredib...  \n",
       "4  [pulwamaattack , terrorists , scored , bigger ...  \n",
       "5                   [beyonc , world , stop , world ]  \n",
       "6  [needs , money , connections , average , norma...  \n",
       "7          [time , run , country , retweet , agree ]  \n",
       "8  [works , im , even , gon na , front , retweete...  \n",
       "9  [trying , prove , point , lets , settle , retw...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_english_clean_mongo.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@EJFC26 Y Messi</td>\n",
       "      <td>[messi ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @MBelenAlegre: Hermoso y sensual viernes ‚ù£Ô∏è</td>\n",
       "      <td>[hermoso , sensual , viernes ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Foro_TV: Suman 47 muertos y 640 heridos po...</td>\n",
       "      <td>[suman , muertos , heridos , explosion , china ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @revistaetcetera: .@lopezobrador_ dice que ...</td>\n",
       "      <td>[dice , tan , grande , gloria , benito , juare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me retracto de mi respuesta , √©l gobierno se s...</td>\n",
       "      <td>[retracto , respuesta , gobierno , acorralado ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anda a saber si te esta eligiendo o sos lo √∫ni...</td>\n",
       "      <td>[anda , saber , si , eligiendo , sos , unico ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @JCTrujilloCPCCS: Hay que propiciar una Con...</td>\n",
       "      <td>[propiciar , consulta , popular , pueblo , ecu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@RicciuP Terraplanistas</td>\n",
       "      <td>[terraplanistas ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @oriolguellipuig: Jeje jeje https://t.co/NU...</td>\n",
       "      <td>[jeje , jeje ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas? no quiero ser part...</td>\n",
       "      <td>[veganarcas , quiero , ser , parte , team , ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                    @EJFC26 Y Messi   \n",
       "1     RT @MBelenAlegre: Hermoso y sensual viernes ‚ù£Ô∏è   \n",
       "2  RT @Foro_TV: Suman 47 muertos y 640 heridos po...   \n",
       "3  RT @revistaetcetera: .@lopezobrador_ dice que ...   \n",
       "4  Me retracto de mi respuesta , √©l gobierno se s...   \n",
       "5  Anda a saber si te esta eligiendo o sos lo √∫ni...   \n",
       "6  RT @JCTrujilloCPCCS: Hay que propiciar una Con...   \n",
       "7                            @RicciuP Terraplanistas   \n",
       "8  RT @oriolguellipuig: Jeje jeje https://t.co/NU...   \n",
       "9  quienes son las veganarcas? no quiero ser part...   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0                                           [messi ]  \n",
       "1                     [hermoso , sensual , viernes ]  \n",
       "2   [suman , muertos , heridos , explosion , china ]  \n",
       "3  [dice , tan , grande , gloria , benito , juare...  \n",
       "4    [retracto , respuesta , gobierno , acorralado ]  \n",
       "5  [anda , saber , si , eligiendo , sos , unico ,...  \n",
       "6  [propiciar , consulta , popular , pueblo , ecu...  \n",
       "7                                  [terraplanistas ]  \n",
       "8                                     [jeje , jeje ]  \n",
       "9  [veganarcas , quiero , ser , parte , team , ex...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_spanish_clean_mongo.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng_clean = model_tvs_eng.transform(df_tokens_english_clean_mongo)\n",
    "pred_spa_clean = model_tvs_spa.transform(df_tokens_spanish_clean_mongo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nos quedamos con las columnas del texto, tokens y la predicci√≥n calculada\n",
    "pred_eng_clean = pred_eng_clean.drop(\"countfeatures\",\"features\",\"rawPrediction\",\"probability\")\n",
    "pred_spa_clean = pred_spa_clean.drop(\"countfeatures\",\"features\",\"rawPrediction\",\"probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_clean</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @liamyoung: Strange that Tony Blair has sud...</td>\n",
       "      <td>[strange , tony , blair , suddenly , become , ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hard work puts you where good luck can find you.</td>\n",
       "      <td>[hard , work , puts , good , luck , find ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @xCiphxr: When creative kids try playing co...</td>\n",
       "      <td>[creative , kids , try , playing , comp ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @bonang_m: I‚Äôm working on one as we speak. ...</td>\n",
       "      <td>[im , working , one , speak , thank , incredib...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @akashbanerjee: After #PulwamaAttack, terro...</td>\n",
       "      <td>[pulwamaattack , terrorists , scored , bigger ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @godisgodisback: Beyonc√©: WORLD STOP!\\n\\nWo...</td>\n",
       "      <td>[beyonc , world , stop , world ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No. It just needs money and connections. Your ...</td>\n",
       "      <td>[needs , money , connections , average , norma...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @hopeoverfear01: It‚Äôs time to run our own c...</td>\n",
       "      <td>[time , run , country , retweet , agree ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Do me a favor right? Follow this absolute top ...</td>\n",
       "      <td>[favor , right , follow , absolute , top , fuc...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @pushetblur: Trying to prove a point. Lets ...</td>\n",
       "      <td>[trying , prove , point , lets , settle , retw...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@TeilZeitMensch Love you.</td>\n",
       "      <td>[love ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>allergies are already acting up</td>\n",
       "      <td>[allergies , already , acting ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Well there was a fly in the milk and a over do...</td>\n",
       "      <td>[well , fly , milk , done , poached , egg , mu...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@factsionary Men tend to grunt more...</td>\n",
       "      <td>[men , tend , grunt ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RT @hasanminhaj: Everyone should have @ShashiT...</td>\n",
       "      <td>[everyone , order , taco , bell ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RT @UNSW: It's #WorldFrogDay! Here are the 7 k...</td>\n",
       "      <td>[worldfrogday , known , frog , amplexus , mati...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RT @printpuncakalam: CC ARAB  24 Hours for UiT...</td>\n",
       "      <td>[cc , arab , hours , uitm , student , cc , ara...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@TheKriten what‚Äôs the shop also do u know whic...</td>\n",
       "      <td>[whats , shop , also , u , know , artist ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>We are now up to two PS4‚Äôs, PS VR, a PS2, a Dr...</td>\n",
       "      <td>[two , ps4s , ps , vr , ps , dreamcast , sega ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RT @realDonaldTrump: It was announced today by...</td>\n",
       "      <td>[announced , today , u.s , treasury , addition...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   RT @liamyoung: Strange that Tony Blair has sud...   \n",
       "1    Hard work puts you where good luck can find you.   \n",
       "2   RT @xCiphxr: When creative kids try playing co...   \n",
       "3   RT @bonang_m: I‚Äôm working on one as we speak. ...   \n",
       "4   RT @akashbanerjee: After #PulwamaAttack, terro...   \n",
       "5   RT @godisgodisback: Beyonc√©: WORLD STOP!\\n\\nWo...   \n",
       "6   No. It just needs money and connections. Your ...   \n",
       "7   RT @hopeoverfear01: It‚Äôs time to run our own c...   \n",
       "8   Do me a favor right? Follow this absolute top ...   \n",
       "9   RT @pushetblur: Trying to prove a point. Lets ...   \n",
       "10                          @TeilZeitMensch Love you.   \n",
       "11                    allergies are already acting up   \n",
       "12  Well there was a fly in the milk and a over do...   \n",
       "13             @factsionary Men tend to grunt more...   \n",
       "14  RT @hasanminhaj: Everyone should have @ShashiT...   \n",
       "15  RT @UNSW: It's #WorldFrogDay! Here are the 7 k...   \n",
       "16  RT @printpuncakalam: CC ARAB  24 Hours for UiT...   \n",
       "17  @TheKriten what‚Äôs the shop also do u know whic...   \n",
       "18  We are now up to two PS4‚Äôs, PS VR, a PS2, a Dr...   \n",
       "19  RT @realDonaldTrump: It was announced today by...   \n",
       "\n",
       "                                         tokens_clean  prediction  \n",
       "0   [strange , tony , blair , suddenly , become , ...         0.0  \n",
       "1          [hard , work , puts , good , luck , find ]         0.0  \n",
       "2           [creative , kids , try , playing , comp ]         2.0  \n",
       "3   [im , working , one , speak , thank , incredib...         2.0  \n",
       "4   [pulwamaattack , terrorists , scored , bigger ...         2.0  \n",
       "5                    [beyonc , world , stop , world ]         2.0  \n",
       "6   [needs , money , connections , average , norma...         0.0  \n",
       "7           [time , run , country , retweet , agree ]         2.0  \n",
       "8   [favor , right , follow , absolute , top , fuc...         2.0  \n",
       "9   [trying , prove , point , lets , settle , retw...         2.0  \n",
       "10                                            [love ]         2.0  \n",
       "11                    [allergies , already , acting ]         0.0  \n",
       "12  [well , fly , milk , done , poached , egg , mu...         2.0  \n",
       "13                              [men , tend , grunt ]         2.0  \n",
       "14                  [everyone , order , taco , bell ]         2.0  \n",
       "15  [worldfrogday , known , frog , amplexus , mati...         2.0  \n",
       "16  [cc , arab , hours , uitm , student , cc , ara...         0.0  \n",
       "17         [whats , shop , also , u , know , artist ]         2.0  \n",
       "18  [two , ps4s , ps , vr , ps , dreamcast , sega ...         2.0  \n",
       "19  [announced , today , u.s , treasury , addition...         2.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "pred_eng_clean.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_clean</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@EJFC26 Y Messi</td>\n",
       "      <td>[messi ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @MBelenAlegre: Hermoso y sensual viernes ‚ù£Ô∏è</td>\n",
       "      <td>[hermoso , sensual , viernes ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Foro_TV: Suman 47 muertos y 640 heridos po...</td>\n",
       "      <td>[suman , muertos , heridos , explosion , china ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @revistaetcetera: .@lopezobrador_ dice que ...</td>\n",
       "      <td>[dice , tan , grande , gloria , benito , juare...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me retracto de mi respuesta , √©l gobierno se s...</td>\n",
       "      <td>[retracto , respuesta , gobierno , acorralado ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anda a saber si te esta eligiendo o sos lo √∫ni...</td>\n",
       "      <td>[anda , saber , si , eligiendo , sos , unico ,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @JCTrujilloCPCCS: Hay que propiciar una Con...</td>\n",
       "      <td>[propiciar , consulta , popular , pueblo , ecu...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@RicciuP Terraplanistas</td>\n",
       "      <td>[terraplanistas ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @oriolguellipuig: Jeje jeje https://t.co/NU...</td>\n",
       "      <td>[jeje , jeje ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas? no quiero ser part...</td>\n",
       "      <td>[veganarcas , quiero , ser , parte , team , ex...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT @NatalyArteta16: M√°s d√≠as con este clima‚ù§</td>\n",
       "      <td>[mas , dias , clima ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RT @julxval: mood del fandom despu√©s de haber ...</td>\n",
       "      <td>[mood , fandom , despues , haber , recaudado ,...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RT @aguantaok: Yo te quiero pa' mi no te quier...</td>\n",
       "      <td>[quiero , pa , quiero , pa , mas , nadie , sol...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @zoecoria9: Me llego a poner eso y parezco ...</td>\n",
       "      <td>[llego , poner , parezco , arrollado , pollo ]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RT @fagundezcertad1: Un gran saludo a mis cama...</td>\n",
       "      <td>[gran , saludo , camaradas , patriotas ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@gcfwithmochi Yo tambi√©n vi a una chica army e...</td>\n",
       "      <td>[tambien , vi , chica , army , u , quise , hab...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#Cultura\\nEl Festival de #C√≥mic Europeo de #√öb...</td>\n",
       "      <td>[cultura , festival , comic , europeo , ubeda ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@COOPERMENDI Solo entro en twitter por ti</td>\n",
       "      <td>[solo , entro , twitter ]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RT @gil_cas: Que tan feo debes ser, para prefe...</td>\n",
       "      <td>[tan , feo , debes , ser , preferir , emoji , ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RT @Juliaenlaonda: #PortarArmasJELO @noeliacla...</td>\n",
       "      <td>[portararmasjelo , unidos , unico , pais , mun...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0                                     @EJFC26 Y Messi   \n",
       "1      RT @MBelenAlegre: Hermoso y sensual viernes ‚ù£Ô∏è   \n",
       "2   RT @Foro_TV: Suman 47 muertos y 640 heridos po...   \n",
       "3   RT @revistaetcetera: .@lopezobrador_ dice que ...   \n",
       "4   Me retracto de mi respuesta , √©l gobierno se s...   \n",
       "5   Anda a saber si te esta eligiendo o sos lo √∫ni...   \n",
       "6   RT @JCTrujilloCPCCS: Hay que propiciar una Con...   \n",
       "7                             @RicciuP Terraplanistas   \n",
       "8   RT @oriolguellipuig: Jeje jeje https://t.co/NU...   \n",
       "9   quienes son las veganarcas? no quiero ser part...   \n",
       "10       RT @NatalyArteta16: M√°s d√≠as con este clima‚ù§   \n",
       "11  RT @julxval: mood del fandom despu√©s de haber ...   \n",
       "12  RT @aguantaok: Yo te quiero pa' mi no te quier...   \n",
       "13  RT @zoecoria9: Me llego a poner eso y parezco ...   \n",
       "14  RT @fagundezcertad1: Un gran saludo a mis cama...   \n",
       "15  @gcfwithmochi Yo tambi√©n vi a una chica army e...   \n",
       "16  #Cultura\\nEl Festival de #C√≥mic Europeo de #√öb...   \n",
       "17          @COOPERMENDI Solo entro en twitter por ti   \n",
       "18  RT @gil_cas: Que tan feo debes ser, para prefe...   \n",
       "19  RT @Juliaenlaonda: #PortarArmasJELO @noeliacla...   \n",
       "\n",
       "                                         tokens_clean  prediction  \n",
       "0                                            [messi ]         2.0  \n",
       "1                      [hermoso , sensual , viernes ]         2.0  \n",
       "2    [suman , muertos , heridos , explosion , china ]         0.0  \n",
       "3   [dice , tan , grande , gloria , benito , juare...         0.0  \n",
       "4     [retracto , respuesta , gobierno , acorralado ]         0.0  \n",
       "5   [anda , saber , si , eligiendo , sos , unico ,...         0.0  \n",
       "6   [propiciar , consulta , popular , pueblo , ecu...         2.0  \n",
       "7                                   [terraplanistas ]         0.0  \n",
       "8                                      [jeje , jeje ]         2.0  \n",
       "9   [veganarcas , quiero , ser , parte , team , ex...         2.0  \n",
       "10                              [mas , dias , clima ]         2.0  \n",
       "11  [mood , fandom , despues , haber , recaudado ,...         2.0  \n",
       "12  [quiero , pa , quiero , pa , mas , nadie , sol...         2.0  \n",
       "13     [llego , poner , parezco , arrollado , pollo ]         0.0  \n",
       "14           [gran , saludo , camaradas , patriotas ]         2.0  \n",
       "15  [tambien , vi , chica , army , u , quise , hab...         1.0  \n",
       "16  [cultura , festival , comic , europeo , ubeda ...         2.0  \n",
       "17                          [solo , entro , twitter ]         2.0  \n",
       "18  [tan , feo , debes , ser , preferir , emoji , ...         0.0  \n",
       "19  [portararmasjelo , unidos , unico , pais , mun...         2.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "pred_spa_clean.limit(20).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
