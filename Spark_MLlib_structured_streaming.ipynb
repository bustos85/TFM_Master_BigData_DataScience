{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Structured streaming con Spark ML.\n",
    "\n",
    "Se recogen los datos en streaming desde Kafka, y se les aplicará el análisis de sentimiento para etiquetar el sentimiento de cada tweet recogido, utilizando Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports y configuraciones necesarias\n",
    "# Spark Streaming\n",
    "from pyspark.streaming import StreamingContext  \n",
    "# Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "from pyspark.mllib.util import *\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# cargamos las stopwords para cada idioma\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# !pip install elasticsearch --si fuera necesario instalarlo\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"twitter\")\\\n",
    "          .master(\"spark://MacBook-Pro-de-Jose.local:7077\")\\\n",
    "          .config(\"spark.io.compression.codec\", \"snappy\")\\\n",
    "          .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\\\n",
    "          .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos las funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de carga de datos desde MongoDB de los datos almacenados desde Apache Nifi, tanto en inglés como español\n",
    "def carga_datos_mongo():\n",
    "    # conectamos con las tablas de Mongo desde donde cargamos los datos\n",
    "    df_mongo_english = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_english\").load()\n",
    "    df_mongo_spanish = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_spanish\").load()\n",
    "    \n",
    "    # nos quedamos solo con el atributo texto que es la información que nos interesa de cada dataset\n",
    "    df_text_english = df_mongo_english[['text']]\n",
    "    df_text_spanish = df_mongo_spanish[['text']]\n",
    "    \n",
    "    return df_text_english, df_text_spanish\n",
    "\n",
    "\n",
    "# Función de carga de datos desde los csv generados con anterioridad con registros ya con el sentimiento anotado\n",
    "def carga_datos_csv(csv):\n",
    "    schema_csv = StructType([ \n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"sentiment\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    # cargamos los csv\n",
    "    if(csv=='english_full'): fichero = './data/df_result_english.csv'\n",
    "    elif(csv=='spanish_full'): fichero = './data/df_result_spanish.csv'\n",
    "    elif(csv=='english_neutro'): fichero = './data/df_result_english_neutral.csv'\n",
    "    elif(csv=='spanish_neutro'): fichero = './data/df_result_spanish_neutral.csv'\n",
    "    elif(csv=='english_noNeutro'): fichero = './data/df_result_english_noNeutral.csv'\n",
    "    elif(csv=='spanish_noNeutro'): fichero = './data/df_result_spanish_noNeutral.csv'\n",
    "    \n",
    "    df_csv = sqlContext.\\\n",
    "        read.format(\"com.databricks.spark.csv\").\\\n",
    "        option(\"header\", \"true\").\\\n",
    "        option(\"inferschema\", \"true\").\\\n",
    "        option(\"mode\", \"DROPMALFORMED\").\\\n",
    "        schema(schema_csv).\\\n",
    "        load(fichero).\\\n",
    "        cache()\n",
    "    \n",
    "    return df_csv\n",
    "\n",
    "\n",
    "# Funciones de visualización de DFs para ver las columnas, el número de registros o dimensiones, y el recuento\n",
    "# de valores del atributo sentimiento en caso de tener la columna.\n",
    "def visualizar_datos_csv(df):\n",
    "    print(\"Columnas del dataframe: \", df.columns)\n",
    "    print(\"Numero de registros = %d\" % df.count())\n",
    "    print(\"\\n\")\n",
    "    print(df.limit(10).toPandas())\n",
    "    print(\"\\n\")\n",
    "    recuento_sentiment = df.select('sentiment').groupBy(\"sentiment\").count().show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "def visualizar_datos_mongo(df):\n",
    "    df_pandas = df.toPandas()\n",
    "    print(\"num_rows: %d\\tColumnas: %d\\n\" % (df_pandas.shape[0], df_pandas.shape[1]) )\n",
    "    print(\"Columnas:\\n\", list(df_pandas.columns))\n",
    "    print(df_pandas.head(10))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# función para eliminar palabras que no queramos analizar\n",
    "def eliminar_stopwords(texto, palabras_eliminar):\n",
    "    tok = nltk.tokenize\n",
    "    palabras = tok.word_tokenize(texto)\n",
    "        \n",
    "    palabras_salida = []\n",
    "        \n",
    "    for palabra in palabras:\n",
    "        if palabra not in palabras_eliminar:\n",
    "            palabras_salida.append(palabra)\n",
    "        \n",
    "    salida = \"\"\n",
    "    for i in range(len(palabras_salida)):\n",
    "        if palabras_salida[i] in string.punctuation:\n",
    "            salida = salida.strip()+palabras_salida[i] + \" \"\n",
    "        else:\n",
    "            salida += palabras_salida[i] + \" \"\n",
    "\n",
    "    return salida\n",
    "\n",
    "\n",
    "# Funciones de limpieza de los tweets\n",
    "def limpieza_tweets_spanish(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    # eliminar full_text que es como comienzan los textos que son extendidos\n",
    "    tweet = [re.sub(r'full_text', '', s) for s in tweet] \n",
    "    \n",
    "    # convertir la repetición de una letra más de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "    # eliminar acentos\n",
    "    tweet = [''.join((c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')) for s in tweet]\n",
    "        \n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\º\\'\\’\\‘\\|()%/\\\"{}@;:+\\[\\]\\–\\”\\…\\“\\】\\【=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, spanish_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "# a los datos en inglés le aplicamos sus stopwords correspondientes y no les quitamos los acentos\n",
    "def limpieza_tweets_english(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    # eliminar full_text que es como comienzan los textos que son extendidos\n",
    "    tweet = [re.sub(r'full_text', '', s) for s in tweet] \n",
    "    \n",
    "    # convertir la repetición de una letra más de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "\n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\º\\'\\’\\‘\\|()%/\\\"{}@;:+\\[\\]\\–\\”\\…\\“\\】\\【=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, english_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "# Función de preprocesado de los datos\n",
    "def preprocesado_dataframe(df1, df2, tipo):\n",
    "    # primero usamos tokenizer y vamos a partir los tweets por palabras\n",
    "    if(tipo==0): tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"token\")\n",
    "    elif(tipo==1): tokenizer = Tokenizer(inputCol = \"texto\", outputCol = \"token\")\n",
    "\n",
    "    df_tokens_english = tokenizer.transform(df1)\n",
    "    df_tokens_spanish = tokenizer.transform(df2)  \n",
    "    \n",
    "    # usamos las funciones de limpieza y preprocesado con ambos DFs ya con los textos divididos en tokens\n",
    "    limpiezaUDF_english = F.udf(limpieza_tweets_english, ArrayType(StringType()))\n",
    "    limpiezaUDF_spanish = F.udf(limpieza_tweets_spanish, ArrayType(StringType()))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.withColumn(\"tokens_clean\", limpiezaUDF_english(df_tokens_english[\"token\"]))\n",
    "    df_tokens_spanish = df_tokens_spanish.withColumn(\"tokens_clean\", limpiezaUDF_spanish(df_tokens_spanish[\"token\"]))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.drop(\"token\")\n",
    "    df_tokens_spanish = df_tokens_spanish.drop(\"token\")\n",
    "\n",
    "    df_tokens_english_clean = df_tokens_english.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    df_tokens_spanish_clean = df_tokens_spanish.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    \n",
    "    return df_tokens_english_clean, df_tokens_spanish_clean\n",
    "\n",
    "\n",
    "# Función evaluación de modelo\n",
    "def evaluar_modelo(metric, predCol, labelCol, pred1, pred2):\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric, predictionCol=predCol, labelCol=labelCol)\n",
    "\n",
    "    eval_eng_clean = evaluator.evaluate(pred1)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Medidas de rendimiento datos en inglés\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"F1-score = %f\" % eval_eng_clean)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    eval_spa_clean = evaluator.evaluate(pred2)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Medidas de rendimiento datos en español\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"F1-score = %f\" % eval_spa_clean)\n",
    "    print(\"\\n\") \n",
    "\n",
    "\n",
    "# Función para ver los mejores parámetros de un modelo al usar cross-validation\n",
    "def mejores_parametros(model1, model2):\n",
    "    bestPipeline_eng = model1.bestModel\n",
    "    bestPipeline_spa = model2.bestModel\n",
    "\n",
    "    bestVectorizer_eng = bestPipeline_eng.stages[0]\n",
    "    bestVectorizer_spa = bestPipeline_spa.stages[0]\n",
    "\n",
    "    bestParamsVect_eng = bestVectorizer_eng.extractParamMap()\n",
    "    print (\"Vectorizer parameters datos inglés:\")\n",
    "    for k in bestParamsVect_eng.keys():\n",
    "        print (\"  \", k, bestParamsVect_eng[k])\n",
    "    \n",
    "    bestParamsVect_spa = bestVectorizer_spa.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print (\"Vectorizer parameters datos español:\")\n",
    "    for k in bestParamsVect_spa.keys():\n",
    "        print (\"  \", k, bestParamsVect_spa[k])\n",
    "\n",
    "    bestModel_eng = bestPipeline_eng.stages[2]\n",
    "    bestModel_spa = bestPipeline_spa.stages[2]\n",
    "\n",
    "    bestParamsModel_eng = bestModel_eng.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print(\"Model parameters datos inglés:\")\n",
    "    for k in bestParamsModel_eng.keys():\n",
    "        print(\"  \", k, bestParamsModel_eng[k])\n",
    "\n",
    "    bestParamsModel_spa = bestModel_spa.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print(\"Model parameters datos español:\")\n",
    "    for k in bestParamsModel_spa.keys():\n",
    "        print(\"  \", k, bestParamsModel_spa[k]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de los datos guardados en formato .csv para entrenar el modelo a usar con los tweets que vienen en streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los distintos csv generados con anterioridad y con el sentimiento ya etiquetado\n",
    "df_csv_english_full = carga_datos_csv('english_full')\n",
    "df_csv_spanish_full = carga_datos_csv('spanish_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 175818\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @VirginAmerica amazing to me that we can't get...          0\n",
      "1  @VirginAmerica View of downtown Los Angeles, t...          2\n",
      "2  @VirginAmerica plz help me win my bid upgrade ...          1\n",
      "3  @VirginAmerica I'm #elevategold for a good rea...          2\n",
      "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1\n",
      "5  @VirginAmerica I’m having trouble adding this ...          0\n",
      "6  @VirginAmerica you have the absolute best team...          2\n",
      "7  @VirginAmerica has flight number 276 from SFO ...          1\n",
      "8  @VirginAmerica Another delayed flight? #liking...          0\n",
      "9  @VirginAmerica Can you find us a flt out of LA...          1\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 1372|\n",
      "|        2|87372|\n",
      "|        0|87074|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cogemos un sample del DF con datos en inglés ya que tiene muchos datos y da problemas de cómputo y tiempos\n",
    "df_csv_english_sample = df_csv_english_full.sample(False, 0.1, 45)\n",
    "\n",
    "visualizar_datos_csv(df_csv_english_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 46787\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @PauladeLasHeras No te libraras de ayudar me/n...          1\n",
      "1                          @marodriguezb Gracias MAR          2\n",
      "2  Off pensando en el regalito Sinde, la que se v...          0\n",
      "3  Conozco a alguien q es adicto al drama! Ja ja ...          2\n",
      "4  Toca @crackoviadeTV3 . Grabación dl especial N...          2\n",
      "5  Buen día todos! Lo primero mandar un abrazo gr...          2\n",
      "6  Desde el escaño. Todo listo para empezar #endi...          2\n",
      "7  Bdías. EM no se ira de puente. Si vosotros os ...          2\n",
      "8  Un sistema económico q recorta dinero para pre...          2\n",
      "9                  #programascambiados caca d ajuste          0\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 2927|\n",
      "|        2|25392|\n",
      "|        0|18468|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_spanish_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generamos el modelo que se probó anteriormente y dio los mejores resultados, y lo entrenamos con los datos recogidos de los csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la función que engloba el preprocesado de los datos, con la tokenización y la limpieza de tweets\n",
    "df_tokens_english_clean, df_tokens_spanish_clean = preprocesado_dataframe(df_csv_english_sample,df_csv_spanish_full,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica amazing to me that we can't get...</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazing , cant , get , cold , air , vents , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica View of downtown Los Angeles, t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[view , downtown , los , angeles , hollywood ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[plz , help , win , bid , upgrade , flight , l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica I'm #elevategold for a good rea...</td>\n",
       "      <td>2</td>\n",
       "      <td>[im , elevategold , good , reason , rock ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood Juli...</td>\n",
       "      <td>1</td>\n",
       "      <td>[julie , andrews , first , lady , gaga , wowd ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  @VirginAmerica amazing to me that we can't get...          0   \n",
       "1  @VirginAmerica View of downtown Los Angeles, t...          2   \n",
       "2  @VirginAmerica plz help me win my bid upgrade ...          1   \n",
       "3  @VirginAmerica I'm #elevategold for a good rea...          2   \n",
       "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0  [amazing , cant , get , cold , air , vents , v...  \n",
       "1  [view , downtown , los , angeles , hollywood ,...  \n",
       "2  [plz , help , win , bid , upgrade , flight , l...  \n",
       "3         [im , elevategold , good , reason , rock ]  \n",
       "4  [julie , andrews , first , lady , gaga , wowd ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_english_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>1</td>\n",
       "      <td>[libraras , ayudar , menos , besos , gracias ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2</td>\n",
       "      <td>[gracias , mar ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>0</td>\n",
       "      <td>[off , pensando , regalito , sinde , va , sgae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[conozco , alguien , q , adicto , drama , ja ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "      <td>2</td>\n",
       "      <td>[toca , grabacion , dl , especial , navideno m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  @PauladeLasHeras No te libraras de ayudar me/n...          1   \n",
       "1                          @marodriguezb Gracias MAR          2   \n",
       "2  Off pensando en el regalito Sinde, la que se v...          0   \n",
       "3  Conozco a alguien q es adicto al drama! Ja ja ...          2   \n",
       "4  Toca @crackoviadeTV3 . Grabación dl especial N...          2   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0     [libraras , ayudar , menos , besos , gracias ]  \n",
       "1                                   [gracias , mar ]  \n",
       "2  [off , pensando , regalito , sinde , va , sgae...  \n",
       "3  [conozco , alguien , q , adicto , drama , ja ,...  \n",
       "4  [toca , grabacion , dl , especial , navideno m...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_spanish_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a partir del dataframe anterior, vamos a eliminar la columna texto en los datos que han pasado por la limpieza y\n",
    "# preprocesado.\n",
    "df_tokens_english_clean = df_tokens_english_clean.drop(\"text\")\n",
    "df_tokens_spanish_clean = df_tokens_spanish_clean.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos el nombre de la columna sentiment por label a los dataframes\n",
    "df_english_clean = df_tokens_english_clean.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean = df_tokens_spanish_clean.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generación de conjunto de training y test de los dataframes\n",
    "train_english_clean, test_english_clean = df_english_clean.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_spanish_clean, test_spanish_clean = df_spanish_clean.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos el pipeline del modelo con sus etapas y parámetros, además de utilizar TrainValidationSplit\n",
    "# sería la configuración del modelo con el que se obtuvieron los mejores resultados en el notebook de pruebas de ML\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=MulticlassClassificationEvaluator(),\n",
    "                           trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng = tvs.fit(train_english_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa = tvs.fit(train_spanish_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng = model_eng.transform(test_english_clean)\n",
    "pred_spa = model_spa.transform(test_spanish_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer parameters datos inglés:\n",
      "   CountVectorizer_684b6f2e96fc__binary False\n",
      "   CountVectorizer_684b6f2e96fc__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_684b6f2e96fc__minDF 1.0\n",
      "   CountVectorizer_684b6f2e96fc__minTF 1.0\n",
      "   CountVectorizer_684b6f2e96fc__outputCol countfeatures\n",
      "   CountVectorizer_684b6f2e96fc__vocabSize 262144\n",
      "   CountVectorizer_684b6f2e96fc__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Vectorizer parameters datos español:\n",
      "   CountVectorizer_684b6f2e96fc__binary False\n",
      "   CountVectorizer_684b6f2e96fc__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_684b6f2e96fc__minDF 1.0\n",
      "   CountVectorizer_684b6f2e96fc__minTF 1.0\n",
      "   CountVectorizer_684b6f2e96fc__outputCol countfeatures\n",
      "   CountVectorizer_684b6f2e96fc__vocabSize 262144\n",
      "   CountVectorizer_684b6f2e96fc__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Model parameters datos inglés:\n",
      "   LogisticRegression_0e605d2eb517__aggregationDepth 2\n",
      "   LogisticRegression_0e605d2eb517__elasticNetParam 0.1\n",
      "   LogisticRegression_0e605d2eb517__family auto\n",
      "   LogisticRegression_0e605d2eb517__featuresCol features\n",
      "   LogisticRegression_0e605d2eb517__fitIntercept True\n",
      "   LogisticRegression_0e605d2eb517__labelCol label\n",
      "   LogisticRegression_0e605d2eb517__maxIter 20\n",
      "   LogisticRegression_0e605d2eb517__predictionCol prediction\n",
      "   LogisticRegression_0e605d2eb517__probabilityCol probability\n",
      "   LogisticRegression_0e605d2eb517__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_0e605d2eb517__regParam 0.01\n",
      "   LogisticRegression_0e605d2eb517__standardization True\n",
      "   LogisticRegression_0e605d2eb517__threshold 0.5\n",
      "   LogisticRegression_0e605d2eb517__tol 1e-06\n",
      "\n",
      "\n",
      "Model parameters datos español:\n",
      "   LogisticRegression_0e605d2eb517__aggregationDepth 2\n",
      "   LogisticRegression_0e605d2eb517__elasticNetParam 0.1\n",
      "   LogisticRegression_0e605d2eb517__family auto\n",
      "   LogisticRegression_0e605d2eb517__featuresCol features\n",
      "   LogisticRegression_0e605d2eb517__fitIntercept False\n",
      "   LogisticRegression_0e605d2eb517__labelCol label\n",
      "   LogisticRegression_0e605d2eb517__maxIter 20\n",
      "   LogisticRegression_0e605d2eb517__predictionCol prediction\n",
      "   LogisticRegression_0e605d2eb517__probabilityCol probability\n",
      "   LogisticRegression_0e605d2eb517__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_0e605d2eb517__regParam 0.01\n",
      "   LogisticRegression_0e605d2eb517__standardization True\n",
      "   LogisticRegression_0e605d2eb517__threshold 0.5\n",
      "   LogisticRegression_0e605d2eb517__tol 1e-06\n"
     ]
    }
   ],
   "source": [
    "# mostrar los mejores parámetros\n",
    "mejores_parametros(model_eng, model_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en inglés\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.761035\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en español\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.801400\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng, pred_spa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conectamos con el streaming para recoger los datos de Kafka y tenerlos en un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el schema de los datos que queremos guardar de los datos de entrada al sistema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"truncated\", BooleanType(), True),\n",
    "    StructField(\"extended_tweet\", StringType(), True),\n",
    "    StructField(\"favorite_count\", IntegerType(), True),\n",
    "    StructField(\"quote_count\", IntegerType(), True),\n",
    "    StructField(\"reply_count\", IntegerType(), True),\n",
    "    StructField(\"retweet_count\", IntegerType(), True),\n",
    "    StructField(\"sentiment\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtienen desde Kafka los datos del topic en español mediante schema\n",
    "# cambiamos a latest startingOffsets, y True en failOnDataLoss. \n",
    "df_spanish = spark\\\n",
    ".readStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    ".option(\"startingOffsets\", \"latest\")\\\n",
    ".option(\"subscribe\", \"TopicSpanish\")\\\n",
    ".option(\"failOnDataLoss\", \"true\") \\\n",
    ".load()\n",
    "\n",
    "df_spanish = df_spanish.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_spanish = df_spanish.select(F.from_json(F.col(\"value\"), schema).alias(\"data\")).select(\"data.*\")           \n",
    "                                                                                          \n",
    "df_spanish.createOrReplaceTempView(\"spanish_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, truncated: boolean, extended_tweet: string, favorite_count: int, quote_count: int, reply_count: int, retweet_count: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtienen desde Kafka los datos del topic en inglés mediante schema\n",
    "df_english = spark\\\n",
    ".readStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9093\")\\\n",
    ".option(\"startingOffsets\", \"latest\")\\\n",
    ".option(\"subscribe\", \"TopicEnglish\")\\\n",
    ".option(\"failOnDataLoss\", \"true\") \\\n",
    ".load()\n",
    "\n",
    "df_english = df_english.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_english = df_english.select(F.from_json(F.col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "df_english.createOrReplaceTempView(\"english_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, truncated: boolean, extended_tweet: string, favorite_count: int, quote_count: int, reply_count: int, retweet_count: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Is the stream ready?\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "# chequeamos los datos en streaming\n",
    "print(\" \")\n",
    "print(\"Is the stream ready?\")\n",
    "print(df_spanish.isStreaming, df_english.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumamos las 4 columnas del recuento de replies, favorites, quotes y retweet para tener una sola columna\n",
    "df_spanish = df_spanish.withColumn('interacciones', df_spanish.favorite_count\\\n",
    "                + df_spanish.quote_count + df_spanish.reply_count + df_spanish.retweet_count)\n",
    "df_english = df_english.withColumn('interacciones', df_english.favorite_count\\\n",
    "                + df_english.quote_count + df_english.reply_count + df_english.retweet_count)\n",
    "\n",
    "# eliminamos las columnas sumadas para el total anterior\n",
    "columns_to_drop = ['favorite_count', 'quote_count', 'reply_count', 'retweet_count']\n",
    "df_spanish = df_spanish.drop(*columns_to_drop)\n",
    "df_english = df_english.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos ahora a ver el valor de truncated, si es True, el texto del tweet está truncado en la columna text y por tanto\n",
    "# debemos coger el texto de la columna extended_tweet. Para ello usamos una función udf.\n",
    "def udf_cambiar_texto(col1,col2,col3):\n",
    "    if(col1==True):\n",
    "        col2=col3\n",
    "    return col2\n",
    "\n",
    "cambiar_texto_udf=F.udf(udf_cambiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicamos la función udf a los dataframes. Generamos una columna 'texto' que tendrá el texto final, ya sea el \n",
    "# inicial de la columna 'text' o si está truncado será el que venga en la columna 'extended_tweet'\n",
    "df_spanish = df_spanish.select('*').withColumn(\"texto\", cambiar_texto_udf(\"truncated\",\"text\",\"extended_tweet\"))\n",
    "df_english = df_english.select('*').withColumn(\"texto\", cambiar_texto_udf(\"truncated\",\"text\",\"extended_tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar columnas y dejar los dataframes como queremos finalmente ordenando las columnas si es necesario\n",
    "columns = ['id', 'texto', 'interacciones', 'sentiment']\n",
    "df_spanish = df_spanish[columns] \n",
    "df_english = df_english[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, texto: string, interacciones: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, texto: string, interacciones: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_spanish)\n",
    "display(df_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Getting offsets from KafkaV2[Subscribe[TopicSpanish]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[TopicEnglish]]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "# definimos querys en memoria para ver los datos que se han recogido\n",
    "query_spanish = df_spanish.writeStream.outputMode(\"append\").queryName(\"spanish\").format(\"memory\")\\\n",
    ".option(\"truncate\", \"False\").start()\n",
    "\n",
    "query_english = df_english.writeStream.outputMode(\"append\").queryName(\"english\").format(\"memory\")\\\n",
    ".option(\"truncate\", \"False\").start()\n",
    "\n",
    "print(query_spanish.status)\n",
    "print(query_english.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_spanish = spark.table(\"spanish\")\n",
    "result_english = spark.table(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros en español:  24\n",
      "Número de registros en inglés:  76\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de registros en español: \", result_spanish.count())\n",
    "print(\"Número de registros en inglés: \", result_english.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167074214811242497</td>\n",
       "      <td>RT @FiltracionesB: Ahora si!! #benvindoneymar !</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167074214827966464</td>\n",
       "      <td>RT amirnorman #MaiaReficco #KallysMashupShow #...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167074214815383555</td>\n",
       "      <td>RT @DaniCubidesF: Afortunados ustedes que no h...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167074214832168960</td>\n",
       "      <td>RT @FutbolFrancais: Según Sky Italia, el ficha...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167074214819577857</td>\n",
       "      <td>@Ushld_loveme Valid 😭😭</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167074214844743682</td>\n",
       "      <td>@CamiiAguilarrr Jajaja, no se 🤷‍♀️🤷‍♀️</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1167074214806982656</td>\n",
       "      <td>El valiente. https://t.co/0LIosZ4yoZ</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1167074214815289344</td>\n",
       "      <td>@CHOMPS Y estoooooo? 😂</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1167074214836416512</td>\n",
       "      <td>{\"full_text\":\"@Malote39 @Marconav1981 @Jupol_Z...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1167074214819454976</td>\n",
       "      <td>RT @xxvane99xx: 100 RT y subo más 💦 https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1167074219022311424</td>\n",
       "      <td>RT @MPPSMiranda: #29Ago #VenezuelaInvencible P...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1167074219013943296</td>\n",
       "      <td>RT @JuanDiegoSoyYo: Nos llamaban exagerados po...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1167074219039105024</td>\n",
       "      <td>RT @patchaibiza__: Neymar el año que viene cua...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1167074219001204736</td>\n",
       "      <td>Ayer olvide mis llaves y me dejo una gran ense...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1167074223208243200</td>\n",
       "      <td>RT @Albertoenserie: A ver si lo pilláis: que s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1167074223220826112</td>\n",
       "      <td>Buenos días! en Televisa San Ángel https://t.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1167074223220592640</td>\n",
       "      <td>@melishcs En la primera chance que no llueva p...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1167074227427500032</td>\n",
       "      <td>RT @Cawnas: Que guapo estás neta</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1167074227415130113</td>\n",
       "      <td>@Cockie04 @Chenoathletic @abuelacharo Amiga ya...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1167074227410952192</td>\n",
       "      <td>@NRegpr @LilibyPrada Mejóralas porque no se en...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              texto  \\\n",
       "0   1167074214811242497    RT @FiltracionesB: Ahora si!! #benvindoneymar !   \n",
       "1   1167074214827966464  RT amirnorman #MaiaReficco #KallysMashupShow #...   \n",
       "2   1167074214815383555  RT @DaniCubidesF: Afortunados ustedes que no h...   \n",
       "3   1167074214832168960  RT @FutbolFrancais: Según Sky Italia, el ficha...   \n",
       "4   1167074214819577857                             @Ushld_loveme Valid 😭😭   \n",
       "5   1167074214844743682             @CamiiAguilarrr Jajaja, no se 🤷‍♀️🤷‍♀️   \n",
       "6   1167074214806982656               El valiente. https://t.co/0LIosZ4yoZ   \n",
       "7   1167074214815289344                             @CHOMPS Y estoooooo? 😂   \n",
       "8   1167074214836416512  {\"full_text\":\"@Malote39 @Marconav1981 @Jupol_Z...   \n",
       "9   1167074214819454976  RT @xxvane99xx: 100 RT y subo más 💦 https://t....   \n",
       "10  1167074219022311424  RT @MPPSMiranda: #29Ago #VenezuelaInvencible P...   \n",
       "11  1167074219013943296  RT @JuanDiegoSoyYo: Nos llamaban exagerados po...   \n",
       "12  1167074219039105024  RT @patchaibiza__: Neymar el año que viene cua...   \n",
       "13  1167074219001204736  Ayer olvide mis llaves y me dejo una gran ense...   \n",
       "14  1167074223208243200  RT @Albertoenserie: A ver si lo pilláis: que s...   \n",
       "15  1167074223220826112  Buenos días! en Televisa San Ángel https://t.c...   \n",
       "16  1167074223220592640  @melishcs En la primera chance que no llueva p...   \n",
       "17  1167074227427500032                   RT @Cawnas: Que guapo estás neta   \n",
       "18  1167074227415130113  @Cockie04 @Chenoathletic @abuelacharo Amiga ya...   \n",
       "19  1167074227410952192  @NRegpr @LilibyPrada Mejóralas porque no se en...   \n",
       "\n",
       "    interacciones sentiment  \n",
       "0               0      None  \n",
       "1               0      None  \n",
       "2               0      None  \n",
       "3               0      None  \n",
       "4               0      None  \n",
       "5               0      None  \n",
       "6               0      None  \n",
       "7               0      None  \n",
       "8               0      None  \n",
       "9               0      None  \n",
       "10              0      None  \n",
       "11              0      None  \n",
       "12              0      None  \n",
       "13              0      None  \n",
       "14              0      None  \n",
       "15              0      None  \n",
       "16              0      None  \n",
       "17              0      None  \n",
       "18              0      None  \n",
       "19              0      None  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos los datos en español\n",
    "result_spanish.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167074210642092032</td>\n",
       "      <td>RT @FBlankenshipWSB: This!! https://t.co/yVFRN...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167074210646085632</td>\n",
       "      <td>SARKAR real verdict enna ?\\nBLOCKBUSTER OR SUP...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167074210637844480</td>\n",
       "      <td>{\"full_text\":\"@america_katz Nero (the left) fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167074210650263552</td>\n",
       "      <td>{\"full_text\":\"or recycling sentences on the In...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167074210637762560</td>\n",
       "      <td>{\"full_text\":\"Chelsea, Tottenham, Liverpool an...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167074210642104320</td>\n",
       "      <td>@shakguerrero awantiaaaaaaaaaaaaaaaaa https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1167074214807031808</td>\n",
       "      <td>RT @SocDogFacts: when you think your dog is ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1167074214823809024</td>\n",
       "      <td>you gotta tell me the reason why we can't fall...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1167074214840602624</td>\n",
       "      <td>@KallMe_Kris @BreeziNUrMouth If he didn’t do i...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1167074214815227904</td>\n",
       "      <td>RT @russellsidhu2: Turning cigarette buds into...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1167074214806880262</td>\n",
       "      <td>RT @superjanella: btw.... HAHA GOTCHU ALL 👅😈 \\...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1167074214823845888</td>\n",
       "      <td>RT @davidlee: 'I will not endorse that': The C...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1167074214828007427</td>\n",
       "      <td>RT @1004_HJS: mmm watcha say 🔫\\n\\n#한 #한지성 #HAN...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1167074214811013120</td>\n",
       "      <td>RT @catturd2: Reminder  ...\\n\\nMSNBC tried to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1167074214815379457</td>\n",
       "      <td>RT @maddow: And she's 14 years old. https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1167074214840557568</td>\n",
       "      <td>RT @CruelTSummer: Taylor Swift's latest tweets...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1167074214832214016</td>\n",
       "      <td>RT @webshootrs: THIS IS THE FUNNIEST THING I’V...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1167074214823780355</td>\n",
       "      <td>UpToDate registration / drop in event for NGH ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1167074214844739585</td>\n",
       "      <td>RT @Maxallwood: We now live in a world where p...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1167074214827814912</td>\n",
       "      <td>I sure hope you are right!</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              texto  \\\n",
       "0   1167074210642092032  RT @FBlankenshipWSB: This!! https://t.co/yVFRN...   \n",
       "1   1167074210646085632  SARKAR real verdict enna ?\\nBLOCKBUSTER OR SUP...   \n",
       "2   1167074210637844480  {\"full_text\":\"@america_katz Nero (the left) fi...   \n",
       "3   1167074210650263552  {\"full_text\":\"or recycling sentences on the In...   \n",
       "4   1167074210637762560  {\"full_text\":\"Chelsea, Tottenham, Liverpool an...   \n",
       "5   1167074210642104320  @shakguerrero awantiaaaaaaaaaaaaaaaaa https://...   \n",
       "6   1167074214807031808  RT @SocDogFacts: when you think your dog is ba...   \n",
       "7   1167074214823809024  you gotta tell me the reason why we can't fall...   \n",
       "8   1167074214840602624  @KallMe_Kris @BreeziNUrMouth If he didn’t do i...   \n",
       "9   1167074214815227904  RT @russellsidhu2: Turning cigarette buds into...   \n",
       "10  1167074214806880262  RT @superjanella: btw.... HAHA GOTCHU ALL 👅😈 \\...   \n",
       "11  1167074214823845888  RT @davidlee: 'I will not endorse that': The C...   \n",
       "12  1167074214828007427  RT @1004_HJS: mmm watcha say 🔫\\n\\n#한 #한지성 #HAN...   \n",
       "13  1167074214811013120  RT @catturd2: Reminder  ...\\n\\nMSNBC tried to ...   \n",
       "14  1167074214815379457  RT @maddow: And she's 14 years old. https://t....   \n",
       "15  1167074214840557568  RT @CruelTSummer: Taylor Swift's latest tweets...   \n",
       "16  1167074214832214016  RT @webshootrs: THIS IS THE FUNNIEST THING I’V...   \n",
       "17  1167074214823780355  UpToDate registration / drop in event for NGH ...   \n",
       "18  1167074214844739585  RT @Maxallwood: We now live in a world where p...   \n",
       "19  1167074214827814912                         I sure hope you are right!   \n",
       "\n",
       "    interacciones sentiment  \n",
       "0               0      None  \n",
       "1               0      None  \n",
       "2               0      None  \n",
       "3               0      None  \n",
       "4               0      None  \n",
       "5               0      None  \n",
       "6               0      None  \n",
       "7               0      None  \n",
       "8               0      None  \n",
       "9               0      None  \n",
       "10              0      None  \n",
       "11              0      None  \n",
       "12              0      None  \n",
       "13              0      None  \n",
       "14              0      None  \n",
       "15              0      None  \n",
       "16              0      None  \n",
       "17              0      None  \n",
       "18              0      None  \n",
       "19              0      None  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos los datos en inglés\n",
    "result_english.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end streaming\n",
    "query_spanish.stop()\n",
    "query_english.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora aplicamos a los DFs recogidos en streaming el modelo entrenado para etiquetar el sentimiento de cada tweet.\n",
    "\n",
    "Vamos también a probar a limpiar estos datos, y aplicamos el modelo para predecir el sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la función que engloba el preprocesado de los datos, con la tokenización y la limpieza de tweets\n",
    "df_tokens_english, df_tokens_spanish = preprocesado_dataframe(result_english,result_spanish,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english_clean = df_tokens_english.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean = df_tokens_spanish.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el modelo para predecir el sentimiento de los tweets en los distintos DFs por idioma\n",
    "pred_eng = model_eng.transform(df_english_clean)\n",
    "pred_spa = model_spa.transform(df_spanish_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_eng = pred_eng.drop(\"label\",\"countfeatures\",\"features\",\"rawPrediction\",\"tokens_clean\")\n",
    "pred_spa = pred_spa.drop(\"label\",\"countfeatures\",\"features\",\"rawPrediction\",\"tokens_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función udf para calcular fecha y hora\n",
    "def add_timestamp():\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return timestamp\n",
    "\n",
    "add_timestamp_udf = F.udf(add_timestamp, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# añadimos la columna con el timestamp a los datos\n",
    "pred_eng = pred_eng.withColumn(\"timestamp\", add_timestamp_udf())\n",
    "pred_spa = pred_spa.withColumn(\"timestamp\", add_timestamp_udf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la columna de probabilidad se genera de tipo DenseVector, vamos a crear una función para pasarla a Array y poder\n",
    "# tratarla mejor.\n",
    "def sparse_to_array(v):\n",
    "    v = DenseVector(v)\n",
    "    new_array = list([float(x) for x in v])\n",
    "    return new_array\n",
    "\n",
    "sparse_to_array_udf = F.udf(sparse_to_array, ArrayType(FloatType()))\n",
    "\n",
    "pred_eng = pred_eng.withColumn('probability_array', sparse_to_array_udf('probability'))\n",
    "pred_spa = pred_spa.withColumn('probability_array', sparse_to_array_udf('probability'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos una función udf para según el valor del sentimiento calculado por el modelo, nos quedamos con el valor\n",
    "# del array de probabilidad que corresponde a esa etiqueta de sentimiento, que será el mayor.\n",
    "def udf_probability(col1,col2):\n",
    "    if(col2==0.0):\n",
    "        col1=col1[0]\n",
    "    elif(col2==1.0):\n",
    "        col1=col1[1]\n",
    "    elif(col2==2.0):\n",
    "        col1=col1[2]\n",
    "        \n",
    "    return col1\n",
    "\n",
    "probability_udf=F.udf(udf_probability)\n",
    "\n",
    "pred_eng = pred_eng.withColumn('probabilidad', probability_udf('probability_array','prediction'))\n",
    "pred_spa = pred_spa.withColumn('probabilidad', probability_udf('probability_array','prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordenamos las columnas en el orden que queremos\n",
    "columns = ['id', 'texto', 'interacciones', 'probabilidad', 'prediction', 'timestamp']\n",
    "pred_eng = pred_eng[columns] \n",
    "pred_spa = pred_spa[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos en los dataframes con las siguientes columnas:\n",
    "- ID: id único del tweet\n",
    "- Texto: el texto del tweet\n",
    "- Interacciones: sería la suma de respuestas, citas, retweets y favoritos del tweet.\n",
    "- Probability: probabilidad con la que se obtiene el valor del sentimiento de cada tweet.\n",
    "- Prediction: etiqueta de sentimiento elegida por el modelo para el tweet.\n",
    "- Timestamp: añadimos fecha y hora del procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>probabilidad</th>\n",
       "      <th>prediction</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167074210646085632</td>\n",
       "      <td>SARKAR real verdict enna ?\\nBLOCKBUSTER OR SUP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6209715008735657</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167074210637844480</td>\n",
       "      <td>{\"full_text\":\"@america_katz Nero (the left) fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7965912818908691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167074210650263552</td>\n",
       "      <td>{\"full_text\":\"or recycling sentences on the In...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9550488591194153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167074210637762560</td>\n",
       "      <td>{\"full_text\":\"Chelsea, Tottenham, Liverpool an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5845345854759216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167074210642104320</td>\n",
       "      <td>@shakguerrero awantiaaaaaaaaaaaaaaaaa https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.575066089630127</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167074214807031808</td>\n",
       "      <td>RT @SocDogFacts: when you think your dog is ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7224486470222473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1167074214823809024</td>\n",
       "      <td>you gotta tell me the reason why we can't fall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5488168001174927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1167074214840602624</td>\n",
       "      <td>@KallMe_Kris @BreeziNUrMouth If he didn’t do i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8246603012084961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1167074214815227904</td>\n",
       "      <td>RT @russellsidhu2: Turning cigarette buds into...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7484982013702393</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1167074214806880262</td>\n",
       "      <td>RT @superjanella: btw.... HAHA GOTCHU ALL 👅😈 \\...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9120923280715942</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1167074214823845888</td>\n",
       "      <td>RT @davidlee: 'I will not endorse that': The C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5650935769081116</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1167074214828007427</td>\n",
       "      <td>RT @1004_HJS: mmm watcha say 🔫\\n\\n#한 #한지성 #HAN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7535411715507507</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1167074214811013120</td>\n",
       "      <td>RT @catturd2: Reminder  ...\\n\\nMSNBC tried to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.98491370677948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1167074214815379457</td>\n",
       "      <td>RT @maddow: And she's 14 years old. https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5395572781562805</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1167074214840557568</td>\n",
       "      <td>RT @CruelTSummer: Taylor Swift's latest tweets...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6909745335578918</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1167074214832214016</td>\n",
       "      <td>RT @webshootrs: THIS IS THE FUNNIEST THING I’V...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7732287645339966</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1167074214823780355</td>\n",
       "      <td>UpToDate registration / drop in event for NGH ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5410575866699219</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1167074214844739585</td>\n",
       "      <td>RT @Maxallwood: We now live in a world where p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7670692205429077</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1167074214827814912</td>\n",
       "      <td>I sure hope you are right!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6913769245147705</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1167074214819418112</td>\n",
       "      <td>RT @SaketGokhale: No @madhukishwar - u, frankl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5995330214500427</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              texto  \\\n",
       "0   1167074210646085632  SARKAR real verdict enna ?\\nBLOCKBUSTER OR SUP...   \n",
       "1   1167074210637844480  {\"full_text\":\"@america_katz Nero (the left) fi...   \n",
       "2   1167074210650263552  {\"full_text\":\"or recycling sentences on the In...   \n",
       "3   1167074210637762560  {\"full_text\":\"Chelsea, Tottenham, Liverpool an...   \n",
       "4   1167074210642104320  @shakguerrero awantiaaaaaaaaaaaaaaaaa https://...   \n",
       "5   1167074214807031808  RT @SocDogFacts: when you think your dog is ba...   \n",
       "6   1167074214823809024  you gotta tell me the reason why we can't fall...   \n",
       "7   1167074214840602624  @KallMe_Kris @BreeziNUrMouth If he didn’t do i...   \n",
       "8   1167074214815227904  RT @russellsidhu2: Turning cigarette buds into...   \n",
       "9   1167074214806880262  RT @superjanella: btw.... HAHA GOTCHU ALL 👅😈 \\...   \n",
       "10  1167074214823845888  RT @davidlee: 'I will not endorse that': The C...   \n",
       "11  1167074214828007427  RT @1004_HJS: mmm watcha say 🔫\\n\\n#한 #한지성 #HAN...   \n",
       "12  1167074214811013120  RT @catturd2: Reminder  ...\\n\\nMSNBC tried to ...   \n",
       "13  1167074214815379457  RT @maddow: And she's 14 years old. https://t....   \n",
       "14  1167074214840557568  RT @CruelTSummer: Taylor Swift's latest tweets...   \n",
       "15  1167074214832214016  RT @webshootrs: THIS IS THE FUNNIEST THING I’V...   \n",
       "16  1167074214823780355  UpToDate registration / drop in event for NGH ...   \n",
       "17  1167074214844739585  RT @Maxallwood: We now live in a world where p...   \n",
       "18  1167074214827814912                         I sure hope you are right!   \n",
       "19  1167074214819418112  RT @SaketGokhale: No @madhukishwar - u, frankl...   \n",
       "\n",
       "    interacciones        probabilidad  prediction            timestamp  \n",
       "0               0  0.6209715008735657         2.0  2019-08-29 16:16:00  \n",
       "1               0  0.7965912818908691         2.0  2019-08-29 16:16:00  \n",
       "2               0  0.9550488591194153         0.0  2019-08-29 16:16:00  \n",
       "3               0  0.5845345854759216         0.0  2019-08-29 16:16:00  \n",
       "4               0   0.575066089630127         2.0  2019-08-29 16:16:00  \n",
       "5               0  0.7224486470222473         0.0  2019-08-29 16:16:00  \n",
       "6               0  0.5488168001174927         0.0  2019-08-29 16:16:00  \n",
       "7               0  0.8246603012084961         0.0  2019-08-29 16:16:00  \n",
       "8               0  0.7484982013702393         2.0  2019-08-29 16:16:02  \n",
       "9               0  0.9120923280715942         2.0  2019-08-29 16:16:02  \n",
       "10              0  0.5650935769081116         2.0  2019-08-29 16:16:02  \n",
       "11              0  0.7535411715507507         2.0  2019-08-29 16:16:02  \n",
       "12              0    0.98491370677948         0.0  2019-08-29 16:16:02  \n",
       "13              0  0.5395572781562805         2.0  2019-08-29 16:16:02  \n",
       "14              0  0.6909745335578918         2.0  2019-08-29 16:16:02  \n",
       "15              0  0.7732287645339966         2.0  2019-08-29 16:16:02  \n",
       "16              0  0.5410575866699219         2.0  2019-08-29 16:16:02  \n",
       "17              0  0.7670692205429077         2.0  2019-08-29 16:16:02  \n",
       "18              0  0.6913769245147705         2.0  2019-08-29 16:16:01  \n",
       "19              0  0.5995330214500427         2.0  2019-08-29 16:16:01  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "pred_eng.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>probabilidad</th>\n",
       "      <th>prediction</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167074214811242497</td>\n",
       "      <td>RT @FiltracionesB: Ahora si!! #benvindoneymar !</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38416776061058044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167074214827966464</td>\n",
       "      <td>RT amirnorman #MaiaReficco #KallysMashupShow #...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3333333432674408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167074214815383555</td>\n",
       "      <td>RT @DaniCubidesF: Afortunados ustedes que no h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6482715010643005</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167074214832168960</td>\n",
       "      <td>RT @FutbolFrancais: Según Sky Italia, el ficha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9297542572021484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167074214819577857</td>\n",
       "      <td>@Ushld_loveme Valid 😭😭</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3333333432674408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167074214844743682</td>\n",
       "      <td>@CamiiAguilarrr Jajaja, no se 🤷‍♀️🤷‍♀️</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7352712154388428</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1167074214806982656</td>\n",
       "      <td>El valiente. https://t.co/0LIosZ4yoZ</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5833292007446289</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1167074214815289344</td>\n",
       "      <td>@CHOMPS Y estoooooo? 😂</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3333333432674408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1167074214836416512</td>\n",
       "      <td>{\"full_text\":\"@Malote39 @Marconav1981 @Jupol_Z...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7096914052963257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1167074214819454976</td>\n",
       "      <td>RT @xxvane99xx: 100 RT y subo más 💦 https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5462927222251892</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1167074219022311424</td>\n",
       "      <td>RT @MPPSMiranda: #29Ago #VenezuelaInvencible P...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6402695178985596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1167074219013943296</td>\n",
       "      <td>RT @JuanDiegoSoyYo: Nos llamaban exagerados po...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7625149488449097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1167074219039105024</td>\n",
       "      <td>RT @patchaibiza__: Neymar el año que viene cua...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.47002482414245605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1167074219001204736</td>\n",
       "      <td>Ayer olvide mis llaves y me dejo una gran ense...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9302811026573181</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1167074223208243200</td>\n",
       "      <td>RT @Albertoenserie: A ver si lo pilláis: que s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41164451837539673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1167074223220826112</td>\n",
       "      <td>Buenos días! en Televisa San Ángel https://t.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8996736407279968</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1167074223220592640</td>\n",
       "      <td>@melishcs En la primera chance que no llueva p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9721677899360657</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1167074227427500032</td>\n",
       "      <td>RT @Cawnas: Que guapo estás neta</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5734358429908752</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1167074227415130113</td>\n",
       "      <td>@Cockie04 @Chenoathletic @abuelacharo Amiga ya...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.935014009475708</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1167074227410952192</td>\n",
       "      <td>@NRegpr @LilibyPrada Mejóralas porque no se en...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3802420496940613</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              texto  \\\n",
       "0   1167074214811242497    RT @FiltracionesB: Ahora si!! #benvindoneymar !   \n",
       "1   1167074214827966464  RT amirnorman #MaiaReficco #KallysMashupShow #...   \n",
       "2   1167074214815383555  RT @DaniCubidesF: Afortunados ustedes que no h...   \n",
       "3   1167074214832168960  RT @FutbolFrancais: Según Sky Italia, el ficha...   \n",
       "4   1167074214819577857                             @Ushld_loveme Valid 😭😭   \n",
       "5   1167074214844743682             @CamiiAguilarrr Jajaja, no se 🤷‍♀️🤷‍♀️   \n",
       "6   1167074214806982656               El valiente. https://t.co/0LIosZ4yoZ   \n",
       "7   1167074214815289344                             @CHOMPS Y estoooooo? 😂   \n",
       "8   1167074214836416512  {\"full_text\":\"@Malote39 @Marconav1981 @Jupol_Z...   \n",
       "9   1167074214819454976  RT @xxvane99xx: 100 RT y subo más 💦 https://t....   \n",
       "10  1167074219022311424  RT @MPPSMiranda: #29Ago #VenezuelaInvencible P...   \n",
       "11  1167074219013943296  RT @JuanDiegoSoyYo: Nos llamaban exagerados po...   \n",
       "12  1167074219039105024  RT @patchaibiza__: Neymar el año que viene cua...   \n",
       "13  1167074219001204736  Ayer olvide mis llaves y me dejo una gran ense...   \n",
       "14  1167074223208243200  RT @Albertoenserie: A ver si lo pilláis: que s...   \n",
       "15  1167074223220826112  Buenos días! en Televisa San Ángel https://t.c...   \n",
       "16  1167074223220592640  @melishcs En la primera chance que no llueva p...   \n",
       "17  1167074227427500032                   RT @Cawnas: Que guapo estás neta   \n",
       "18  1167074227415130113  @Cockie04 @Chenoathletic @abuelacharo Amiga ya...   \n",
       "19  1167074227410952192  @NRegpr @LilibyPrada Mejóralas porque no se en...   \n",
       "\n",
       "    interacciones         probabilidad  prediction            timestamp  \n",
       "0               0  0.38416776061058044         0.0  2019-08-29 16:16:08  \n",
       "1               0   0.3333333432674408         0.0  2019-08-29 16:16:08  \n",
       "2               0   0.6482715010643005         2.0  2019-08-29 16:16:08  \n",
       "3               0   0.9297542572021484         0.0  2019-08-29 16:16:09  \n",
       "4               0   0.3333333432674408         0.0  2019-08-29 16:16:09  \n",
       "5               0   0.7352712154388428         2.0  2019-08-29 16:16:09  \n",
       "6               0   0.5833292007446289         2.0  2019-08-29 16:16:09  \n",
       "7               0   0.3333333432674408         0.0  2019-08-29 16:16:09  \n",
       "8               0   0.7096914052963257         0.0  2019-08-29 16:16:09  \n",
       "9               0   0.5462927222251892         2.0  2019-08-29 16:16:08  \n",
       "10              0   0.6402695178985596         0.0  2019-08-29 16:16:08  \n",
       "11              0   0.7625149488449097         0.0  2019-08-29 16:16:08  \n",
       "12              0  0.47002482414245605         0.0  2019-08-29 16:16:08  \n",
       "13              0   0.9302811026573181         2.0  2019-08-29 16:16:08  \n",
       "14              0  0.41164451837539673         0.0  2019-08-29 16:16:08  \n",
       "15              0   0.8996736407279968         2.0  2019-08-29 16:16:09  \n",
       "16              0   0.9721677899360657         2.0  2019-08-29 16:16:09  \n",
       "17              0   0.5734358429908752         2.0  2019-08-29 16:16:09  \n",
       "18              0    0.935014009475708         2.0  2019-08-29 16:16:09  \n",
       "19              0   0.3802420496940613         2.0  2019-08-29 16:16:09  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "pred_spa.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almacenar datos en CSV, MongoDB y ElasticSearch.\n",
    "\n",
    "Probamos a almacenar los datos etiquetados en diferentes formatos: archivos csv, MongoDB, y en ElasticSearch para su posterior visualización con Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190829-161613\n"
     ]
    }
   ],
   "source": [
    "# sacamos una cadena con fecha y hora para añadirla al nombre de los csv que se generan a continuación\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasamos los dataframes con el sentimiento calculado a pandas\n",
    "df_eng_pred = pred_eng.toPandas()\n",
    "df_spa_pred = pred_spa.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se guardan los dataframes con el sentimiento calculado en csv\n",
    "df_eng_pred.to_csv('./data/predict_streaming_english_'+timestr+'.csv', index=False)\n",
    "df_spa_pred.to_csv('./data/predict_streaming_spanish_'+timestr+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1a26de0548>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardar datos en MongoDB\n",
    "conexion = 'mongodb://localhost:27017'\n",
    "client = MongoClient(conexion)\n",
    "\n",
    "# accedemos a la base de datos\n",
    "db = client.tfm_twitter\n",
    "# insertamos los dataframes en la tabla correspondiente\n",
    "db.tweets_streaming_english.insert_many(df_eng_pred.to_dict(\"records\"))\n",
    "db.tweets_streaming_spanish.insert_many(df_spa_pred.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tweets_sentiment_spa_20190829-161613\"}{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tweets_sentiment_eng_20190829-161613\"}True True\n"
     ]
    }
   ],
   "source": [
    "# guardar datos en ElasticSearch\n",
    "es = Elasticsearch('http://localhost:9200/')\n",
    "\n",
    "# añadimos fecha y hora a los índices\n",
    "indice_spa = \"tweets_sentiment_spa_\"+timestr\n",
    "indice_eng = \"tweets_sentiment_eng_\"+timestr\n",
    "\n",
    "# primero comprobamos si ya existen los índices y se borran\n",
    "if es.indices.exists(indice_spa):\n",
    "    !curl -X DELETE localhost:9200/{indice_spa}\n",
    "if es.indices.exists(indice_eng):\n",
    "    !curl -X DELETE localhost:9200/{indice_eng}\n",
    "\n",
    "# generación de índices\n",
    "!curl -X PUT localhost:9200/{indice_spa}\n",
    "!curl -X PUT localhost:9200/{indice_eng}\n",
    "\n",
    "TYPE = \"record\"\n",
    "\n",
    "def rec_to_actions(df, lang):\n",
    "    for record in df.to_dict(orient=\"records\"):\n",
    "        if(lang==0): yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (indice_spa, TYPE))\n",
    "        elif(lang==1): yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (indice_eng, TYPE))\n",
    "        yield (json.dumps(record, default=int))\n",
    "\n",
    "if not es.indices.exists(indice_spa):\n",
    "    raise RuntimeError('index does not exists, use `curl -X PUT \"localhost:9200/%s\"` and try again'%indice_spa)\n",
    "if not es.indices.exists(indice_eng):\n",
    "    raise RuntimeError('index does not exists, use `curl -X PUT \"localhost:9200/%s\"` and try again'%indice_eng)\n",
    "\n",
    "r_spa = es.bulk(rec_to_actions(df_spa_pred, 0))\n",
    "r_eng = es.bulk(rec_to_actions(df_eng_pred, 1)) \n",
    "\n",
    "print(not r_spa[\"errors\"], not r_eng[\"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "# se guardan los modelos utilizados para la predicción\n",
    "model_eng_best = model_eng.bestModel\n",
    "model_spa_best = model_spa.bestModel\n",
    "model_eng_best.write().overwrite().save('./data/models/model_eng_'+timestr)\n",
    "model_spa_best.write().overwrite().save('./data/models/model_spa_'+timestr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
