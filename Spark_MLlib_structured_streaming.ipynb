{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Structured streaming con Spark ML.\n",
    "\n",
    "Se recogen los datos en streaming desde Kafka, y se les aplicar√° el an√°lisis de sentimiento para etiquetar el sentimiento de cada tweet recogido, utilizando Spark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports y configuraciones necesarias\n",
    "# Spark Streaming\n",
    "from pyspark.streaming import StreamingContext  \n",
    "# Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "from pyspark.mllib.util import *\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# cargamos las stopwords para cada idioma\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# !pip install elasticsearch --si fuera necesario instalarlo\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"twitter\")\\\n",
    "          .master(\"spark://MacBook-Pro-de-Jose.local:7077\")\\\n",
    "          .config(\"spark.io.compression.codec\", \"snappy\")\\\n",
    "          .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\\\n",
    "          .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####¬†Creamos las funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de carga de datos desde MongoDB de los datos almacenados desde Apache Nifi, tanto en ingl√©s como espa√±ol\n",
    "def carga_datos_mongo():\n",
    "    # conectamos con las tablas de Mongo desde donde cargamos los datos\n",
    "    df_mongo_english = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_english\").load()\n",
    "    df_mongo_spanish = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_spanish\").load()\n",
    "    \n",
    "    # nos quedamos solo con el atributo texto que es la informaci√≥n que nos interesa de cada dataset\n",
    "    df_text_english = df_mongo_english[['text']]\n",
    "    df_text_spanish = df_mongo_spanish[['text']]\n",
    "    \n",
    "    return df_text_english, df_text_spanish\n",
    "\n",
    "\n",
    "# Funci√≥n de carga de datos desde los csv generados con anterioridad con registros ya con el sentimiento anotado\n",
    "def carga_datos_csv(csv):\n",
    "    schema_csv = StructType([ \n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"sentiment\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    # cargamos los csv\n",
    "    if(csv=='english_full'): fichero = './data/df_result_english.csv'\n",
    "    elif(csv=='spanish_full'): fichero = './data/df_result_spanish.csv'\n",
    "    elif(csv=='english_neutro'): fichero = './data/df_result_english_neutral.csv'\n",
    "    elif(csv=='spanish_neutro'): fichero = './data/df_result_spanish_neutral.csv'\n",
    "    elif(csv=='english_noNeutro'): fichero = './data/df_result_english_noNeutral.csv'\n",
    "    elif(csv=='spanish_noNeutro'): fichero = './data/df_result_spanish_noNeutral.csv'\n",
    "    \n",
    "    df_csv = sqlContext.\\\n",
    "        read.format(\"com.databricks.spark.csv\").\\\n",
    "        option(\"header\", \"true\").\\\n",
    "        option(\"inferschema\", \"true\").\\\n",
    "        option(\"mode\", \"DROPMALFORMED\").\\\n",
    "        schema(schema_csv).\\\n",
    "        load(fichero).\\\n",
    "        cache()\n",
    "    \n",
    "    return df_csv\n",
    "\n",
    "\n",
    "# Funciones de visualizaci√≥n de DFs para ver las columnas, el n√∫mero de registros o dimensiones, y el recuento\n",
    "# de valores del atributo sentimiento en caso de tener la columna.\n",
    "def visualizar_datos_csv(df):\n",
    "    print(\"Columnas del dataframe: \", df.columns)\n",
    "    print(\"Numero de registros = %d\" % df.count())\n",
    "    print(\"\\n\")\n",
    "    print(df.limit(10).toPandas())\n",
    "    print(\"\\n\")\n",
    "    recuento_sentiment = df.select('sentiment').groupBy(\"sentiment\").count().show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "def visualizar_datos_mongo(df):\n",
    "    df_pandas = df.toPandas()\n",
    "    print(\"num_rows: %d\\tColumnas: %d\\n\" % (df_pandas.shape[0], df_pandas.shape[1]) )\n",
    "    print(\"Columnas:\\n\", list(df_pandas.columns))\n",
    "    print(df_pandas.head(10))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# funci√≥n para eliminar palabras que no queramos analizar\n",
    "def eliminar_stopwords(texto, palabras_eliminar):\n",
    "    tok = nltk.tokenize\n",
    "    palabras = tok.word_tokenize(texto)\n",
    "        \n",
    "    palabras_salida = []\n",
    "        \n",
    "    for palabra in palabras:\n",
    "        if palabra not in palabras_eliminar:\n",
    "            palabras_salida.append(palabra)\n",
    "        \n",
    "    salida = \"\"\n",
    "    for i in range(len(palabras_salida)):\n",
    "        if palabras_salida[i] in string.punctuation:\n",
    "            salida = salida.strip()+palabras_salida[i] + \" \"\n",
    "        else:\n",
    "            salida += palabras_salida[i] + \" \"\n",
    "\n",
    "    return salida\n",
    "\n",
    "\n",
    "# Funciones de limpieza de los tweets\n",
    "def limpieza_tweets_spanish(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    # eliminar full_text que es como comienzan los textos que son extendidos\n",
    "    tweet = [re.sub(r'full_text', '', s) for s in tweet] \n",
    "    \n",
    "    # convertir la repetici√≥n de una letra m√°s de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "    # eliminar acentos\n",
    "    tweet = [''.join((c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')) for s in tweet]\n",
    "        \n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\¬∫\\'\\‚Äô\\‚Äò\\|()%/\\\"{}@;:+\\[\\]\\‚Äì\\‚Äù\\‚Ä¶\\‚Äú\\„Äë\\„Äê=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, spanish_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "# a los datos en ingl√©s le aplicamos sus stopwords correspondientes y no les quitamos los acentos\n",
    "def limpieza_tweets_english(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    # eliminar full_text que es como comienzan los textos que son extendidos\n",
    "    tweet = [re.sub(r'full_text', '', s) for s in tweet] \n",
    "    \n",
    "    # convertir la repetici√≥n de una letra m√°s de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "\n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\¬∫\\'\\‚Äô\\‚Äò\\|()%/\\\"{}@;:+\\[\\]\\‚Äì\\‚Äù\\‚Ä¶\\‚Äú\\„Äë\\„Äê=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, english_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "# Funci√≥n de preprocesado de los datos\n",
    "def preprocesado_dataframe(df1, df2, tipo):\n",
    "    # primero usamos tokenizer y vamos a partir los tweets por palabras\n",
    "    if(tipo==0): tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"token\")\n",
    "    elif(tipo==1): tokenizer = Tokenizer(inputCol = \"texto\", outputCol = \"token\")\n",
    "\n",
    "    df_tokens_english = tokenizer.transform(df1)\n",
    "    df_tokens_spanish = tokenizer.transform(df2)  \n",
    "    \n",
    "    # usamos las funciones de limpieza y preprocesado con ambos DFs ya con los textos divididos en tokens\n",
    "    limpiezaUDF_english = F.udf(limpieza_tweets_english, ArrayType(StringType()))\n",
    "    limpiezaUDF_spanish = F.udf(limpieza_tweets_spanish, ArrayType(StringType()))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.withColumn(\"tokens_clean\", limpiezaUDF_english(df_tokens_english[\"token\"]))\n",
    "    df_tokens_spanish = df_tokens_spanish.withColumn(\"tokens_clean\", limpiezaUDF_spanish(df_tokens_spanish[\"token\"]))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.drop(\"token\")\n",
    "    df_tokens_spanish = df_tokens_spanish.drop(\"token\")\n",
    "\n",
    "    df_tokens_english_clean = df_tokens_english.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    df_tokens_spanish_clean = df_tokens_spanish.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    \n",
    "    return df_tokens_english_clean, df_tokens_spanish_clean\n",
    "\n",
    "\n",
    "# Funci√≥n evaluaci√≥n de modelo\n",
    "def evaluar_modelo(metric, predCol, labelCol, pred1, pred2):\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=metric, predictionCol=predCol, labelCol=labelCol)\n",
    "\n",
    "    eval_eng_clean = evaluator.evaluate(pred1)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Medidas de rendimiento datos en ingl√©s\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"F1-score = %f\" % eval_eng_clean)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    eval_spa_clean = evaluator.evaluate(pred2)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Medidas de rendimiento datos en espa√±ol\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"F1-score = %f\" % eval_spa_clean)\n",
    "    print(\"\\n\") \n",
    "\n",
    "\n",
    "# Funci√≥n para ver los mejores par√°metros de un modelo al usar cross-validation\n",
    "def mejores_parametros(model1, model2):\n",
    "    bestPipeline_eng = model1.bestModel\n",
    "    bestPipeline_spa = model2.bestModel\n",
    "\n",
    "    bestVectorizer_eng = bestPipeline_eng.stages[0]\n",
    "    bestVectorizer_spa = bestPipeline_spa.stages[0]\n",
    "\n",
    "    bestParamsVect_eng = bestVectorizer_eng.extractParamMap()\n",
    "    print (\"Vectorizer parameters datos ingl√©s:\")\n",
    "    for k in bestParamsVect_eng.keys():\n",
    "        print (\"  \", k, bestParamsVect_eng[k])\n",
    "    \n",
    "    bestParamsVect_spa = bestVectorizer_spa.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print (\"Vectorizer parameters datos espa√±ol:\")\n",
    "    for k in bestParamsVect_spa.keys():\n",
    "        print (\"  \", k, bestParamsVect_spa[k])\n",
    "\n",
    "    bestModel_eng = bestPipeline_eng.stages[2]\n",
    "    bestModel_spa = bestPipeline_spa.stages[2]\n",
    "\n",
    "    bestParamsModel_eng = bestModel_eng.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print(\"Model parameters datos ingl√©s:\")\n",
    "    for k in bestParamsModel_eng.keys():\n",
    "        print(\"  \", k, bestParamsModel_eng[k])\n",
    "\n",
    "    bestParamsModel_spa = bestModel_spa.extractParamMap()\n",
    "    print(\"\\n\") \n",
    "    print(\"Model parameters datos espa√±ol:\")\n",
    "    for k in bestParamsModel_spa.keys():\n",
    "        print(\"  \", k, bestParamsModel_spa[k]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de los datos guardados en formato .csv para entrenar el modelo a usar con los tweets que vienen en streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los distintos csv generados con anterioridad y con el sentimiento ya etiquetado\n",
    "df_csv_english_full = carga_datos_csv('english_full')\n",
    "df_csv_spanish_full = carga_datos_csv('spanish_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 175818\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @VirginAmerica amazing to me that we can't get...          0\n",
      "1  @VirginAmerica View of downtown Los Angeles, t...          2\n",
      "2  @VirginAmerica plz help me win my bid upgrade ...          1\n",
      "3  @VirginAmerica I'm #elevategold for a good rea...          2\n",
      "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1\n",
      "5  @VirginAmerica I‚Äôm having trouble adding this ...          0\n",
      "6  @VirginAmerica you have the absolute best team...          2\n",
      "7  @VirginAmerica has flight number 276 from SFO ...          1\n",
      "8  @VirginAmerica Another delayed flight? #liking...          0\n",
      "9  @VirginAmerica Can you find us a flt out of LA...          1\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 1372|\n",
      "|        2|87372|\n",
      "|        0|87074|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cogemos un sample del DF con datos en ingl√©s ya que tiene muchos datos y da problemas de c√≥mputo y tiempos\n",
    "df_csv_english_sample = df_csv_english_full.sample(False, 0.1, 45)\n",
    "\n",
    "visualizar_datos_csv(df_csv_english_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 46787\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @PauladeLasHeras No te libraras de ayudar me/n...          1\n",
      "1                          @marodriguezb Gracias MAR          2\n",
      "2  Off pensando en el regalito Sinde, la que se v...          0\n",
      "3  Conozco a alguien q es adicto al drama! Ja ja ...          2\n",
      "4  Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...          2\n",
      "5  Buen d√≠a todos! Lo primero mandar un abrazo gr...          2\n",
      "6  Desde el esca√±o. Todo listo para empezar #endi...          2\n",
      "7  Bd√≠as. EM no se ira de puente. Si vosotros os ...          2\n",
      "8  Un sistema econ√≥mico q recorta dinero para pre...          2\n",
      "9                  #programascambiados caca d ajuste          0\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 2927|\n",
      "|        2|25392|\n",
      "|        0|18468|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_spanish_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generamos el modelo que se prob√≥ anteriormente y dio los mejores resultados, y lo entrenamos con los datos recogidos de los csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la funci√≥n que engloba el preprocesado de los datos, con la tokenizaci√≥n y la limpieza de tweets\n",
    "df_tokens_english_clean, df_tokens_spanish_clean = preprocesado_dataframe(df_csv_english_sample,df_csv_spanish_full,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica amazing to me that we can't get...</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazing , cant , get , cold , air , vents , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica View of downtown Los Angeles, t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[view , downtown , los , angeles , hollywood ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[plz , help , win , bid , upgrade , flight , l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica I'm #elevategold for a good rea...</td>\n",
       "      <td>2</td>\n",
       "      <td>[im , elevategold , good , reason , rock ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood Juli...</td>\n",
       "      <td>1</td>\n",
       "      <td>[julie , andrews , first , lady , gaga , wowd ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  @VirginAmerica amazing to me that we can't get...          0   \n",
       "1  @VirginAmerica View of downtown Los Angeles, t...          2   \n",
       "2  @VirginAmerica plz help me win my bid upgrade ...          1   \n",
       "3  @VirginAmerica I'm #elevategold for a good rea...          2   \n",
       "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0  [amazing , cant , get , cold , air , vents , v...  \n",
       "1  [view , downtown , los , angeles , hollywood ,...  \n",
       "2  [plz , help , win , bid , upgrade , flight , l...  \n",
       "3         [im , elevategold , good , reason , rock ]  \n",
       "4  [julie , andrews , first , lady , gaga , wowd ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_english_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>1</td>\n",
       "      <td>[libraras , ayudar , menos , besos , gracias ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2</td>\n",
       "      <td>[gracias , mar ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>0</td>\n",
       "      <td>[off , pensando , regalito , sinde , va , sgae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[conozco , alguien , q , adicto , drama , ja ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...</td>\n",
       "      <td>2</td>\n",
       "      <td>[toca , grabacion , dl , especial , navideno m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  @PauladeLasHeras No te libraras de ayudar me/n...          1   \n",
       "1                          @marodriguezb Gracias MAR          2   \n",
       "2  Off pensando en el regalito Sinde, la que se v...          0   \n",
       "3  Conozco a alguien q es adicto al drama! Ja ja ...          2   \n",
       "4  Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...          2   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0     [libraras , ayudar , menos , besos , gracias ]  \n",
       "1                                   [gracias , mar ]  \n",
       "2  [off , pensando , regalito , sinde , va , sgae...  \n",
       "3  [conozco , alguien , q , adicto , drama , ja ,...  \n",
       "4  [toca , grabacion , dl , especial , navideno m...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_spanish_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a partir del dataframe anterior, vamos a eliminar la columna texto en los datos que han pasado por la limpieza y\n",
    "# preprocesado.\n",
    "df_tokens_english_clean = df_tokens_english_clean.drop(\"text\")\n",
    "df_tokens_spanish_clean = df_tokens_spanish_clean.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos el nombre de la columna sentiment por label a los dataframes\n",
    "df_english_clean = df_tokens_english_clean.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean = df_tokens_spanish_clean.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generaci√≥n de conjunto de training y test de los dataframes\n",
    "train_english_clean, test_english_clean = df_english_clean.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_spanish_clean, test_spanish_clean = df_spanish_clean.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos el pipeline del modelo con sus etapas y par√°metros, adem√°s de utilizar TrainValidationSplit\n",
    "# ser√≠a la configuraci√≥n del modelo con el que se obtuvieron los mejores resultados en el notebook de pruebas de ML\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=MulticlassClassificationEvaluator(),\n",
    "                           trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng = tvs.fit(train_english_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa = tvs.fit(train_spanish_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos los modelos para predecir usando los datos de test\n",
    "pred_eng = model_eng.transform(test_english_clean)\n",
    "pred_spa = model_spa.transform(test_spanish_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer parameters datos ingl√©s:\n",
      "   CountVectorizer_684b6f2e96fc__binary False\n",
      "   CountVectorizer_684b6f2e96fc__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_684b6f2e96fc__minDF 1.0\n",
      "   CountVectorizer_684b6f2e96fc__minTF 1.0\n",
      "   CountVectorizer_684b6f2e96fc__outputCol countfeatures\n",
      "   CountVectorizer_684b6f2e96fc__vocabSize 262144\n",
      "   CountVectorizer_684b6f2e96fc__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Vectorizer parameters datos espa√±ol:\n",
      "   CountVectorizer_684b6f2e96fc__binary False\n",
      "   CountVectorizer_684b6f2e96fc__maxDF 9.223372036854776e+18\n",
      "   CountVectorizer_684b6f2e96fc__minDF 1.0\n",
      "   CountVectorizer_684b6f2e96fc__minTF 1.0\n",
      "   CountVectorizer_684b6f2e96fc__outputCol countfeatures\n",
      "   CountVectorizer_684b6f2e96fc__vocabSize 262144\n",
      "   CountVectorizer_684b6f2e96fc__inputCol tokens_clean\n",
      "\n",
      "\n",
      "Model parameters datos ingl√©s:\n",
      "   LogisticRegression_0e605d2eb517__aggregationDepth 2\n",
      "   LogisticRegression_0e605d2eb517__elasticNetParam 0.1\n",
      "   LogisticRegression_0e605d2eb517__family auto\n",
      "   LogisticRegression_0e605d2eb517__featuresCol features\n",
      "   LogisticRegression_0e605d2eb517__fitIntercept True\n",
      "   LogisticRegression_0e605d2eb517__labelCol label\n",
      "   LogisticRegression_0e605d2eb517__maxIter 20\n",
      "   LogisticRegression_0e605d2eb517__predictionCol prediction\n",
      "   LogisticRegression_0e605d2eb517__probabilityCol probability\n",
      "   LogisticRegression_0e605d2eb517__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_0e605d2eb517__regParam 0.01\n",
      "   LogisticRegression_0e605d2eb517__standardization True\n",
      "   LogisticRegression_0e605d2eb517__threshold 0.5\n",
      "   LogisticRegression_0e605d2eb517__tol 1e-06\n",
      "\n",
      "\n",
      "Model parameters datos espa√±ol:\n",
      "   LogisticRegression_0e605d2eb517__aggregationDepth 2\n",
      "   LogisticRegression_0e605d2eb517__elasticNetParam 0.1\n",
      "   LogisticRegression_0e605d2eb517__family auto\n",
      "   LogisticRegression_0e605d2eb517__featuresCol features\n",
      "   LogisticRegression_0e605d2eb517__fitIntercept False\n",
      "   LogisticRegression_0e605d2eb517__labelCol label\n",
      "   LogisticRegression_0e605d2eb517__maxIter 20\n",
      "   LogisticRegression_0e605d2eb517__predictionCol prediction\n",
      "   LogisticRegression_0e605d2eb517__probabilityCol probability\n",
      "   LogisticRegression_0e605d2eb517__rawPredictionCol rawPrediction\n",
      "   LogisticRegression_0e605d2eb517__regParam 0.01\n",
      "   LogisticRegression_0e605d2eb517__standardization True\n",
      "   LogisticRegression_0e605d2eb517__threshold 0.5\n",
      "   LogisticRegression_0e605d2eb517__tol 1e-06\n"
     ]
    }
   ],
   "source": [
    "# mostrar los mejores par√°metros\n",
    "mejores_parametros(model_eng, model_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en ingl√©s\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.761035\n",
      "\n",
      "\n",
      "-------------------------------------------------------\n",
      "Medidas de rendimiento datos en espa√±ol\n",
      "-------------------------------------------------------\n",
      "F1-score = 0.801400\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos el modelo\n",
    "evaluar_modelo('f1', 'prediction', 'label', pred_eng, pred_spa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conectamos con el streaming para recoger los datos de Kafka y tenerlos en un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el schema de los datos que queremos guardar de los datos de entrada al sistema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"truncated\", BooleanType(), True),\n",
    "    StructField(\"extended_tweet\", StringType(), True),\n",
    "    StructField(\"favorite_count\", IntegerType(), True),\n",
    "    StructField(\"quote_count\", IntegerType(), True),\n",
    "    StructField(\"reply_count\", IntegerType(), True),\n",
    "    StructField(\"retweet_count\", IntegerType(), True),\n",
    "    StructField(\"sentiment\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtienen desde Kafka los datos del topic en espa√±ol mediante schema\n",
    "# cambiamos a latest startingOffsets, y True en failOnDataLoss. \n",
    "df_spanish = spark\\\n",
    ".readStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    ".option(\"startingOffsets\", \"latest\")\\\n",
    ".option(\"subscribe\", \"TopicSpanish\")\\\n",
    ".option(\"failOnDataLoss\", \"true\") \\\n",
    ".load()\n",
    "\n",
    "df_spanish = df_spanish.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_spanish = df_spanish.select(F.from_json(F.col(\"value\"), schema).alias(\"data\")).select(\"data.*\")           \n",
    "                                                                                          \n",
    "df_spanish.createOrReplaceTempView(\"spanish_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, truncated: boolean, extended_tweet: string, favorite_count: int, quote_count: int, reply_count: int, retweet_count: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtienen desde Kafka los datos del topic en ingl√©s mediante schema\n",
    "df_english = spark\\\n",
    ".readStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9093\")\\\n",
    ".option(\"startingOffsets\", \"latest\")\\\n",
    ".option(\"subscribe\", \"TopicEnglish\")\\\n",
    ".option(\"failOnDataLoss\", \"true\") \\\n",
    ".load()\n",
    "\n",
    "df_english = df_english.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_english = df_english.select(F.from_json(F.col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "df_english.createOrReplaceTempView(\"english_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, truncated: boolean, extended_tweet: string, favorite_count: int, quote_count: int, reply_count: int, retweet_count: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Is the stream ready?\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "# chequeamos los datos en streaming\n",
    "print(\" \")\n",
    "print(\"Is the stream ready?\")\n",
    "print(df_spanish.isStreaming, df_english.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumamos las 4 columnas del recuento de replies, favorites, quotes y retweet para tener una sola columna\n",
    "df_spanish = df_spanish.withColumn('interacciones', df_spanish.favorite_count\\\n",
    "                + df_spanish.quote_count + df_spanish.reply_count + df_spanish.retweet_count)\n",
    "df_english = df_english.withColumn('interacciones', df_english.favorite_count\\\n",
    "                + df_english.quote_count + df_english.reply_count + df_english.retweet_count)\n",
    "\n",
    "# eliminamos las columnas sumadas para el total anterior\n",
    "columns_to_drop = ['favorite_count', 'quote_count', 'reply_count', 'retweet_count']\n",
    "df_spanish = df_spanish.drop(*columns_to_drop)\n",
    "df_english = df_english.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos ahora a ver el valor de truncated, si es True, el texto del tweet est√° truncado en la columna text y por tanto\n",
    "# debemos coger el texto de la columna extended_tweet. Para ello usamos una funci√≥n udf.\n",
    "def udf_cambiar_texto(col1,col2,col3):\n",
    "    if(col1==True):\n",
    "        col2=col3\n",
    "    return col2\n",
    "\n",
    "cambiar_texto_udf=F.udf(udf_cambiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicamos la funci√≥n udf a los dataframes. Generamos una columna 'texto' que tendr√° el texto final, ya sea el \n",
    "# inicial de la columna 'text' o si est√° truncado ser√° el que venga en la columna 'extended_tweet'\n",
    "df_spanish = df_spanish.select('*').withColumn(\"texto\", cambiar_texto_udf(\"truncated\",\"text\",\"extended_tweet\"))\n",
    "df_english = df_english.select('*').withColumn(\"texto\", cambiar_texto_udf(\"truncated\",\"text\",\"extended_tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar columnas y dejar los dataframes como queremos finalmente ordenando las columnas si es necesario\n",
    "columns = ['id', 'texto', 'interacciones', 'sentiment']\n",
    "df_spanish = df_spanish[columns] \n",
    "df_english = df_english[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, texto: string, interacciones: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, texto: string, interacciones: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_spanish)\n",
    "display(df_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Getting offsets from KafkaV2[Subscribe[TopicSpanish]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[TopicEnglish]]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "# definimos querys en memoria para ver los datos que se han recogido\n",
    "query_spanish = df_spanish.writeStream.outputMode(\"append\").queryName(\"spanish\").format(\"memory\")\\\n",
    ".option(\"truncate\", \"False\").start()\n",
    "\n",
    "query_english = df_english.writeStream.outputMode(\"append\").queryName(\"english\").format(\"memory\")\\\n",
    ".option(\"truncate\", \"False\").start()\n",
    "\n",
    "print(query_spanish.status)\n",
    "print(query_english.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_spanish = spark.table(\"spanish\")\n",
    "result_english = spark.table(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de registros en espa√±ol:  24\n",
      "N√∫mero de registros en ingl√©s:  76\n"
     ]
    }
   ],
   "source": [
    "print(\"N√∫mero de registros en espa√±ol: \", result_spanish.count())\n",
    "print(\"N√∫mero de registros en ingl√©s: \", result_english.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167074214811242497</td>\n",
       "      <td>RT @FiltracionesB: Ahora si!! #benvindoneymar !</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167074214827966464</td>\n",
       "      <td>RT amirnorman #MaiaReficco #KallysMashupShow #...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167074214815383555</td>\n",
       "      <td>RT @DaniCubidesF: Afortunados ustedes que no h...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167074214832168960</td>\n",
       "      <td>RT @FutbolFrancais: Seg√∫n Sky Italia, el ficha...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167074214819577857</td>\n",
       "      <td>@Ushld_loveme Valid üò≠üò≠</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167074214844743682</td>\n",
       "      <td>@CamiiAguilarrr Jajaja, no se ü§∑‚Äç‚ôÄÔ∏èü§∑‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1167074214806982656</td>\n",
       "      <td>El valiente. https://t.co/0LIosZ4yoZ</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1167074214815289344</td>\n",
       "      <td>@CHOMPS Y estoooooo? üòÇ</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1167074214836416512</td>\n",
       "      <td>{\"full_text\":\"@Malote39 @Marconav1981 @Jupol_Z...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1167074214819454976</td>\n",
       "      <td>RT @xxvane99xx: 100 RT y subo m√°s üí¶ https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1167074219022311424</td>\n",
       "      <td>RT @MPPSMiranda: #29Ago #VenezuelaInvencible P...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1167074219013943296</td>\n",
       "      <td>RT @JuanDiegoSoyYo: Nos llamaban exagerados po...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1167074219039105024</td>\n",
       "      <td>RT @patchaibiza__: Neymar el a√±o que viene cua...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1167074219001204736</td>\n",
       "      <td>Ayer olvide mis llaves y me dejo una gran ense...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1167074223208243200</td>\n",
       "      <td>RT @Albertoenserie: A ver si lo pill√°is: que s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1167074223220826112</td>\n",
       "      <td>Buenos d√≠as! en Televisa San √Ångel https://t.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1167074223220592640</td>\n",
       "      <td>@melishcs En la primera chance que no llueva p...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1167074227427500032</td>\n",
       "      <td>RT @Cawnas: Que guapo est√°s neta</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1167074227415130113</td>\n",
       "      <td>@Cockie04 @Chenoathletic @abuelacharo Amiga ya...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1167074227410952192</td>\n",
       "      <td>@NRegpr @LilibyPrada Mej√≥ralas porque no se en...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              texto  \\\n",
       "0   1167074214811242497    RT @FiltracionesB: Ahora si!! #benvindoneymar !   \n",
       "1   1167074214827966464  RT amirnorman #MaiaReficco #KallysMashupShow #...   \n",
       "2   1167074214815383555  RT @DaniCubidesF: Afortunados ustedes que no h...   \n",
       "3   1167074214832168960  RT @FutbolFrancais: Seg√∫n Sky Italia, el ficha...   \n",
       "4   1167074214819577857                             @Ushld_loveme Valid üò≠üò≠   \n",
       "5   1167074214844743682             @CamiiAguilarrr Jajaja, no se ü§∑‚Äç‚ôÄÔ∏èü§∑‚Äç‚ôÄÔ∏è   \n",
       "6   1167074214806982656               El valiente. https://t.co/0LIosZ4yoZ   \n",
       "7   1167074214815289344                             @CHOMPS Y estoooooo? üòÇ   \n",
       "8   1167074214836416512  {\"full_text\":\"@Malote39 @Marconav1981 @Jupol_Z...   \n",
       "9   1167074214819454976  RT @xxvane99xx: 100 RT y subo m√°s üí¶ https://t....   \n",
       "10  1167074219022311424  RT @MPPSMiranda: #29Ago #VenezuelaInvencible P...   \n",
       "11  1167074219013943296  RT @JuanDiegoSoyYo: Nos llamaban exagerados po...   \n",
       "12  1167074219039105024  RT @patchaibiza__: Neymar el a√±o que viene cua...   \n",
       "13  1167074219001204736  Ayer olvide mis llaves y me dejo una gran ense...   \n",
       "14  1167074223208243200  RT @Albertoenserie: A ver si lo pill√°is: que s...   \n",
       "15  1167074223220826112  Buenos d√≠as! en Televisa San √Ångel https://t.c...   \n",
       "16  1167074223220592640  @melishcs En la primera chance que no llueva p...   \n",
       "17  1167074227427500032                   RT @Cawnas: Que guapo est√°s neta   \n",
       "18  1167074227415130113  @Cockie04 @Chenoathletic @abuelacharo Amiga ya...   \n",
       "19  1167074227410952192  @NRegpr @LilibyPrada Mej√≥ralas porque no se en...   \n",
       "\n",
       "    interacciones sentiment  \n",
       "0               0      None  \n",
       "1               0      None  \n",
       "2               0      None  \n",
       "3               0      None  \n",
       "4               0      None  \n",
       "5               0      None  \n",
       "6               0      None  \n",
       "7               0      None  \n",
       "8               0      None  \n",
       "9               0      None  \n",
       "10              0      None  \n",
       "11              0      None  \n",
       "12              0      None  \n",
       "13              0      None  \n",
       "14              0      None  \n",
       "15              0      None  \n",
       "16              0      None  \n",
       "17              0      None  \n",
       "18              0      None  \n",
       "19              0      None  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos los datos en espa√±ol\n",
    "result_spanish.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167074210642092032</td>\n",
       "      <td>RT @FBlankenshipWSB: This!! https://t.co/yVFRN...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167074210646085632</td>\n",
       "      <td>SARKAR real verdict enna ?\\nBLOCKBUSTER OR SUP...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167074210637844480</td>\n",
       "      <td>{\"full_text\":\"@america_katz Nero (the left) fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167074210650263552</td>\n",
       "      <td>{\"full_text\":\"or recycling sentences on the In...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167074210637762560</td>\n",
       "      <td>{\"full_text\":\"Chelsea, Tottenham, Liverpool an...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167074210642104320</td>\n",
       "      <td>@shakguerrero awantiaaaaaaaaaaaaaaaaa https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1167074214807031808</td>\n",
       "      <td>RT @SocDogFacts: when you think your dog is ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1167074214823809024</td>\n",
       "      <td>you gotta tell me the reason why we can't fall...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1167074214840602624</td>\n",
       "      <td>@KallMe_Kris @BreeziNUrMouth If he didn‚Äôt do i...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1167074214815227904</td>\n",
       "      <td>RT @russellsidhu2: Turning cigarette buds into...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1167074214806880262</td>\n",
       "      <td>RT @superjanella: btw.... HAHA GOTCHU ALL üëÖüòà \\...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1167074214823845888</td>\n",
       "      <td>RT @davidlee: 'I will not endorse that': The C...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1167074214828007427</td>\n",
       "      <td>RT @1004_HJS: mmm watcha say üî´\\n\\n#Ìïú #ÌïúÏßÄÏÑ± #HAN...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1167074214811013120</td>\n",
       "      <td>RT @catturd2: Reminder  ...\\n\\nMSNBC tried to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1167074214815379457</td>\n",
       "      <td>RT @maddow: And she's 14 years old. https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1167074214840557568</td>\n",
       "      <td>RT @CruelTSummer: Taylor Swift's latest tweets...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1167074214832214016</td>\n",
       "      <td>RT @webshootrs: THIS IS THE FUNNIEST THING I‚ÄôV...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1167074214823780355</td>\n",
       "      <td>UpToDate registration / drop in event for NGH ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1167074214844739585</td>\n",
       "      <td>RT @Maxallwood: We now live in a world where p...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1167074214827814912</td>\n",
       "      <td>I sure hope you are right!</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              texto  \\\n",
       "0   1167074210642092032  RT @FBlankenshipWSB: This!! https://t.co/yVFRN...   \n",
       "1   1167074210646085632  SARKAR real verdict enna ?\\nBLOCKBUSTER OR SUP...   \n",
       "2   1167074210637844480  {\"full_text\":\"@america_katz Nero (the left) fi...   \n",
       "3   1167074210650263552  {\"full_text\":\"or recycling sentences on the In...   \n",
       "4   1167074210637762560  {\"full_text\":\"Chelsea, Tottenham, Liverpool an...   \n",
       "5   1167074210642104320  @shakguerrero awantiaaaaaaaaaaaaaaaaa https://...   \n",
       "6   1167074214807031808  RT @SocDogFacts: when you think your dog is ba...   \n",
       "7   1167074214823809024  you gotta tell me the reason why we can't fall...   \n",
       "8   1167074214840602624  @KallMe_Kris @BreeziNUrMouth If he didn‚Äôt do i...   \n",
       "9   1167074214815227904  RT @russellsidhu2: Turning cigarette buds into...   \n",
       "10  1167074214806880262  RT @superjanella: btw.... HAHA GOTCHU ALL üëÖüòà \\...   \n",
       "11  1167074214823845888  RT @davidlee: 'I will not endorse that': The C...   \n",
       "12  1167074214828007427  RT @1004_HJS: mmm watcha say üî´\\n\\n#Ìïú #ÌïúÏßÄÏÑ± #HAN...   \n",
       "13  1167074214811013120  RT @catturd2: Reminder  ...\\n\\nMSNBC tried to ...   \n",
       "14  1167074214815379457  RT @maddow: And she's 14 years old. https://t....   \n",
       "15  1167074214840557568  RT @CruelTSummer: Taylor Swift's latest tweets...   \n",
       "16  1167074214832214016  RT @webshootrs: THIS IS THE FUNNIEST THING I‚ÄôV...   \n",
       "17  1167074214823780355  UpToDate registration / drop in event for NGH ...   \n",
       "18  1167074214844739585  RT @Maxallwood: We now live in a world where p...   \n",
       "19  1167074214827814912                         I sure hope you are right!   \n",
       "\n",
       "    interacciones sentiment  \n",
       "0               0      None  \n",
       "1               0      None  \n",
       "2               0      None  \n",
       "3               0      None  \n",
       "4               0      None  \n",
       "5               0      None  \n",
       "6               0      None  \n",
       "7               0      None  \n",
       "8               0      None  \n",
       "9               0      None  \n",
       "10              0      None  \n",
       "11              0      None  \n",
       "12              0      None  \n",
       "13              0      None  \n",
       "14              0      None  \n",
       "15              0      None  \n",
       "16              0      None  \n",
       "17              0      None  \n",
       "18              0      None  \n",
       "19              0      None  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos los datos en ingl√©s\n",
    "result_english.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end streaming\n",
    "query_spanish.stop()\n",
    "query_english.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora aplicamos a los DFs recogidos en streaming el modelo entrenado para etiquetar el sentimiento de cada tweet.\n",
    "\n",
    "Vamos tambi√©n a probar a limpiar estos datos, y aplicamos el modelo para predecir el sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la funci√≥n que engloba el preprocesado de los datos, con la tokenizaci√≥n y la limpieza de tweets\n",
    "df_tokens_english, df_tokens_spanish = preprocesado_dataframe(result_english,result_spanish,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english_clean = df_tokens_english.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean = df_tokens_spanish.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el modelo para predecir el sentimiento de los tweets en los distintos DFs por idioma\n",
    "pred_eng = model_eng.transform(df_english_clean)\n",
    "pred_spa = model_spa.transform(df_spanish_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_eng = pred_eng.drop(\"label\",\"countfeatures\",\"features\",\"rawPrediction\",\"tokens_clean\")\n",
    "pred_spa = pred_spa.drop(\"label\",\"countfeatures\",\"features\",\"rawPrediction\",\"tokens_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funci√≥n udf para calcular fecha y hora\n",
    "def add_timestamp():\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return timestamp\n",
    "\n",
    "add_timestamp_udf = F.udf(add_timestamp, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a√±adimos la columna con el timestamp a los datos\n",
    "pred_eng = pred_eng.withColumn(\"timestamp\", add_timestamp_udf())\n",
    "pred_spa = pred_spa.withColumn(\"timestamp\", add_timestamp_udf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la columna de probabilidad se genera de tipo DenseVector, vamos a crear una funci√≥n para pasarla a Array y poder\n",
    "# tratarla mejor.\n",
    "def sparse_to_array(v):\n",
    "    v = DenseVector(v)\n",
    "    new_array = list([float(x) for x in v])\n",
    "    return new_array\n",
    "\n",
    "sparse_to_array_udf = F.udf(sparse_to_array, ArrayType(FloatType()))\n",
    "\n",
    "pred_eng = pred_eng.withColumn('probability_array', sparse_to_array_udf('probability'))\n",
    "pred_spa = pred_spa.withColumn('probability_array', sparse_to_array_udf('probability'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos una funci√≥n udf para seg√∫n el valor del sentimiento calculado por el modelo, nos quedamos con el valor\n",
    "# del array de probabilidad que corresponde a esa etiqueta de sentimiento, que ser√° el mayor.\n",
    "def udf_probability(col1,col2):\n",
    "    if(col2==0.0):\n",
    "        col1=col1[0]\n",
    "    elif(col2==1.0):\n",
    "        col1=col1[1]\n",
    "    elif(col2==2.0):\n",
    "        col1=col1[2]\n",
    "        \n",
    "    return col1\n",
    "\n",
    "probability_udf=F.udf(udf_probability)\n",
    "\n",
    "pred_eng = pred_eng.withColumn('probabilidad', probability_udf('probability_array','prediction'))\n",
    "pred_spa = pred_spa.withColumn('probabilidad', probability_udf('probability_array','prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordenamos las columnas en el orden que queremos\n",
    "columns = ['id', 'texto', 'interacciones', 'probabilidad', 'prediction', 'timestamp']\n",
    "pred_eng = pred_eng[columns] \n",
    "pred_spa = pred_spa[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos en los dataframes con las siguientes columnas:\n",
    "- ID: id √∫nico del tweet\n",
    "- Texto: el texto del tweet\n",
    "- Interacciones: ser√≠a la suma de respuestas, citas, retweets y favoritos del tweet.\n",
    "- Probability: probabilidad con la que se obtiene el valor del sentimiento de cada tweet.\n",
    "- Prediction: etiqueta de sentimiento elegida por el modelo para el tweet.\n",
    "- Timestamp: a√±adimos fecha y hora del procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>probabilidad</th>\n",
       "      <th>prediction</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167074210646085632</td>\n",
       "      <td>SARKAR real verdict enna ?\\nBLOCKBUSTER OR SUP...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6209715008735657</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167074210637844480</td>\n",
       "      <td>{\"full_text\":\"@america_katz Nero (the left) fi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7965912818908691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167074210650263552</td>\n",
       "      <td>{\"full_text\":\"or recycling sentences on the In...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9550488591194153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167074210637762560</td>\n",
       "      <td>{\"full_text\":\"Chelsea, Tottenham, Liverpool an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5845345854759216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167074210642104320</td>\n",
       "      <td>@shakguerrero awantiaaaaaaaaaaaaaaaaa https://...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.575066089630127</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167074214807031808</td>\n",
       "      <td>RT @SocDogFacts: when you think your dog is ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7224486470222473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1167074214823809024</td>\n",
       "      <td>you gotta tell me the reason why we can't fall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5488168001174927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1167074214840602624</td>\n",
       "      <td>@KallMe_Kris @BreeziNUrMouth If he didn‚Äôt do i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8246603012084961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1167074214815227904</td>\n",
       "      <td>RT @russellsidhu2: Turning cigarette buds into...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7484982013702393</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1167074214806880262</td>\n",
       "      <td>RT @superjanella: btw.... HAHA GOTCHU ALL üëÖüòà \\...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9120923280715942</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1167074214823845888</td>\n",
       "      <td>RT @davidlee: 'I will not endorse that': The C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5650935769081116</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1167074214828007427</td>\n",
       "      <td>RT @1004_HJS: mmm watcha say üî´\\n\\n#Ìïú #ÌïúÏßÄÏÑ± #HAN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7535411715507507</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1167074214811013120</td>\n",
       "      <td>RT @catturd2: Reminder  ...\\n\\nMSNBC tried to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.98491370677948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1167074214815379457</td>\n",
       "      <td>RT @maddow: And she's 14 years old. https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5395572781562805</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1167074214840557568</td>\n",
       "      <td>RT @CruelTSummer: Taylor Swift's latest tweets...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6909745335578918</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1167074214832214016</td>\n",
       "      <td>RT @webshootrs: THIS IS THE FUNNIEST THING I‚ÄôV...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7732287645339966</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1167074214823780355</td>\n",
       "      <td>UpToDate registration / drop in event for NGH ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5410575866699219</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1167074214844739585</td>\n",
       "      <td>RT @Maxallwood: We now live in a world where p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7670692205429077</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1167074214827814912</td>\n",
       "      <td>I sure hope you are right!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6913769245147705</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1167074214819418112</td>\n",
       "      <td>RT @SaketGokhale: No @madhukishwar - u, frankl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5995330214500427</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              texto  \\\n",
       "0   1167074210646085632  SARKAR real verdict enna ?\\nBLOCKBUSTER OR SUP...   \n",
       "1   1167074210637844480  {\"full_text\":\"@america_katz Nero (the left) fi...   \n",
       "2   1167074210650263552  {\"full_text\":\"or recycling sentences on the In...   \n",
       "3   1167074210637762560  {\"full_text\":\"Chelsea, Tottenham, Liverpool an...   \n",
       "4   1167074210642104320  @shakguerrero awantiaaaaaaaaaaaaaaaaa https://...   \n",
       "5   1167074214807031808  RT @SocDogFacts: when you think your dog is ba...   \n",
       "6   1167074214823809024  you gotta tell me the reason why we can't fall...   \n",
       "7   1167074214840602624  @KallMe_Kris @BreeziNUrMouth If he didn‚Äôt do i...   \n",
       "8   1167074214815227904  RT @russellsidhu2: Turning cigarette buds into...   \n",
       "9   1167074214806880262  RT @superjanella: btw.... HAHA GOTCHU ALL üëÖüòà \\...   \n",
       "10  1167074214823845888  RT @davidlee: 'I will not endorse that': The C...   \n",
       "11  1167074214828007427  RT @1004_HJS: mmm watcha say üî´\\n\\n#Ìïú #ÌïúÏßÄÏÑ± #HAN...   \n",
       "12  1167074214811013120  RT @catturd2: Reminder  ...\\n\\nMSNBC tried to ...   \n",
       "13  1167074214815379457  RT @maddow: And she's 14 years old. https://t....   \n",
       "14  1167074214840557568  RT @CruelTSummer: Taylor Swift's latest tweets...   \n",
       "15  1167074214832214016  RT @webshootrs: THIS IS THE FUNNIEST THING I‚ÄôV...   \n",
       "16  1167074214823780355  UpToDate registration / drop in event for NGH ...   \n",
       "17  1167074214844739585  RT @Maxallwood: We now live in a world where p...   \n",
       "18  1167074214827814912                         I sure hope you are right!   \n",
       "19  1167074214819418112  RT @SaketGokhale: No @madhukishwar - u, frankl...   \n",
       "\n",
       "    interacciones        probabilidad  prediction            timestamp  \n",
       "0               0  0.6209715008735657         2.0  2019-08-29 16:16:00  \n",
       "1               0  0.7965912818908691         2.0  2019-08-29 16:16:00  \n",
       "2               0  0.9550488591194153         0.0  2019-08-29 16:16:00  \n",
       "3               0  0.5845345854759216         0.0  2019-08-29 16:16:00  \n",
       "4               0   0.575066089630127         2.0  2019-08-29 16:16:00  \n",
       "5               0  0.7224486470222473         0.0  2019-08-29 16:16:00  \n",
       "6               0  0.5488168001174927         0.0  2019-08-29 16:16:00  \n",
       "7               0  0.8246603012084961         0.0  2019-08-29 16:16:00  \n",
       "8               0  0.7484982013702393         2.0  2019-08-29 16:16:02  \n",
       "9               0  0.9120923280715942         2.0  2019-08-29 16:16:02  \n",
       "10              0  0.5650935769081116         2.0  2019-08-29 16:16:02  \n",
       "11              0  0.7535411715507507         2.0  2019-08-29 16:16:02  \n",
       "12              0    0.98491370677948         0.0  2019-08-29 16:16:02  \n",
       "13              0  0.5395572781562805         2.0  2019-08-29 16:16:02  \n",
       "14              0  0.6909745335578918         2.0  2019-08-29 16:16:02  \n",
       "15              0  0.7732287645339966         2.0  2019-08-29 16:16:02  \n",
       "16              0  0.5410575866699219         2.0  2019-08-29 16:16:02  \n",
       "17              0  0.7670692205429077         2.0  2019-08-29 16:16:02  \n",
       "18              0  0.6913769245147705         2.0  2019-08-29 16:16:01  \n",
       "19              0  0.5995330214500427         2.0  2019-08-29 16:16:01  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "pred_eng.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>probabilidad</th>\n",
       "      <th>prediction</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167074214811242497</td>\n",
       "      <td>RT @FiltracionesB: Ahora si!! #benvindoneymar !</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38416776061058044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167074214827966464</td>\n",
       "      <td>RT amirnorman #MaiaReficco #KallysMashupShow #...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3333333432674408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167074214815383555</td>\n",
       "      <td>RT @DaniCubidesF: Afortunados ustedes que no h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6482715010643005</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167074214832168960</td>\n",
       "      <td>RT @FutbolFrancais: Seg√∫n Sky Italia, el ficha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9297542572021484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167074214819577857</td>\n",
       "      <td>@Ushld_loveme Valid üò≠üò≠</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3333333432674408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167074214844743682</td>\n",
       "      <td>@CamiiAguilarrr Jajaja, no se ü§∑‚Äç‚ôÄÔ∏èü§∑‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7352712154388428</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1167074214806982656</td>\n",
       "      <td>El valiente. https://t.co/0LIosZ4yoZ</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5833292007446289</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1167074214815289344</td>\n",
       "      <td>@CHOMPS Y estoooooo? üòÇ</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3333333432674408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1167074214836416512</td>\n",
       "      <td>{\"full_text\":\"@Malote39 @Marconav1981 @Jupol_Z...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7096914052963257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1167074214819454976</td>\n",
       "      <td>RT @xxvane99xx: 100 RT y subo m√°s üí¶ https://t....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5462927222251892</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1167074219022311424</td>\n",
       "      <td>RT @MPPSMiranda: #29Ago #VenezuelaInvencible P...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6402695178985596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1167074219013943296</td>\n",
       "      <td>RT @JuanDiegoSoyYo: Nos llamaban exagerados po...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7625149488449097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1167074219039105024</td>\n",
       "      <td>RT @patchaibiza__: Neymar el a√±o que viene cua...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.47002482414245605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1167074219001204736</td>\n",
       "      <td>Ayer olvide mis llaves y me dejo una gran ense...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9302811026573181</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1167074223208243200</td>\n",
       "      <td>RT @Albertoenserie: A ver si lo pill√°is: que s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41164451837539673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1167074223220826112</td>\n",
       "      <td>Buenos d√≠as! en Televisa San √Ångel https://t.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8996736407279968</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1167074223220592640</td>\n",
       "      <td>@melishcs En la primera chance que no llueva p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9721677899360657</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1167074227427500032</td>\n",
       "      <td>RT @Cawnas: Que guapo est√°s neta</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5734358429908752</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1167074227415130113</td>\n",
       "      <td>@Cockie04 @Chenoathletic @abuelacharo Amiga ya...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.935014009475708</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1167074227410952192</td>\n",
       "      <td>@NRegpr @LilibyPrada Mej√≥ralas porque no se en...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3802420496940613</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:16:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                              texto  \\\n",
       "0   1167074214811242497    RT @FiltracionesB: Ahora si!! #benvindoneymar !   \n",
       "1   1167074214827966464  RT amirnorman #MaiaReficco #KallysMashupShow #...   \n",
       "2   1167074214815383555  RT @DaniCubidesF: Afortunados ustedes que no h...   \n",
       "3   1167074214832168960  RT @FutbolFrancais: Seg√∫n Sky Italia, el ficha...   \n",
       "4   1167074214819577857                             @Ushld_loveme Valid üò≠üò≠   \n",
       "5   1167074214844743682             @CamiiAguilarrr Jajaja, no se ü§∑‚Äç‚ôÄÔ∏èü§∑‚Äç‚ôÄÔ∏è   \n",
       "6   1167074214806982656               El valiente. https://t.co/0LIosZ4yoZ   \n",
       "7   1167074214815289344                             @CHOMPS Y estoooooo? üòÇ   \n",
       "8   1167074214836416512  {\"full_text\":\"@Malote39 @Marconav1981 @Jupol_Z...   \n",
       "9   1167074214819454976  RT @xxvane99xx: 100 RT y subo m√°s üí¶ https://t....   \n",
       "10  1167074219022311424  RT @MPPSMiranda: #29Ago #VenezuelaInvencible P...   \n",
       "11  1167074219013943296  RT @JuanDiegoSoyYo: Nos llamaban exagerados po...   \n",
       "12  1167074219039105024  RT @patchaibiza__: Neymar el a√±o que viene cua...   \n",
       "13  1167074219001204736  Ayer olvide mis llaves y me dejo una gran ense...   \n",
       "14  1167074223208243200  RT @Albertoenserie: A ver si lo pill√°is: que s...   \n",
       "15  1167074223220826112  Buenos d√≠as! en Televisa San √Ångel https://t.c...   \n",
       "16  1167074223220592640  @melishcs En la primera chance que no llueva p...   \n",
       "17  1167074227427500032                   RT @Cawnas: Que guapo est√°s neta   \n",
       "18  1167074227415130113  @Cockie04 @Chenoathletic @abuelacharo Amiga ya...   \n",
       "19  1167074227410952192  @NRegpr @LilibyPrada Mej√≥ralas porque no se en...   \n",
       "\n",
       "    interacciones         probabilidad  prediction            timestamp  \n",
       "0               0  0.38416776061058044         0.0  2019-08-29 16:16:08  \n",
       "1               0   0.3333333432674408         0.0  2019-08-29 16:16:08  \n",
       "2               0   0.6482715010643005         2.0  2019-08-29 16:16:08  \n",
       "3               0   0.9297542572021484         0.0  2019-08-29 16:16:09  \n",
       "4               0   0.3333333432674408         0.0  2019-08-29 16:16:09  \n",
       "5               0   0.7352712154388428         2.0  2019-08-29 16:16:09  \n",
       "6               0   0.5833292007446289         2.0  2019-08-29 16:16:09  \n",
       "7               0   0.3333333432674408         0.0  2019-08-29 16:16:09  \n",
       "8               0   0.7096914052963257         0.0  2019-08-29 16:16:09  \n",
       "9               0   0.5462927222251892         2.0  2019-08-29 16:16:08  \n",
       "10              0   0.6402695178985596         0.0  2019-08-29 16:16:08  \n",
       "11              0   0.7625149488449097         0.0  2019-08-29 16:16:08  \n",
       "12              0  0.47002482414245605         0.0  2019-08-29 16:16:08  \n",
       "13              0   0.9302811026573181         2.0  2019-08-29 16:16:08  \n",
       "14              0  0.41164451837539673         0.0  2019-08-29 16:16:08  \n",
       "15              0   0.8996736407279968         2.0  2019-08-29 16:16:09  \n",
       "16              0   0.9721677899360657         2.0  2019-08-29 16:16:09  \n",
       "17              0   0.5734358429908752         2.0  2019-08-29 16:16:09  \n",
       "18              0    0.935014009475708         2.0  2019-08-29 16:16:09  \n",
       "19              0   0.3802420496940613         2.0  2019-08-29 16:16:09  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predictions:\")\n",
    "pred_spa.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almacenar datos en CSV, MongoDB y ElasticSearch.\n",
    "\n",
    "Probamos a almacenar los datos etiquetados en diferentes formatos: archivos csv, MongoDB, y en ElasticSearch para su posterior visualizaci√≥n con Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190829-161613\n"
     ]
    }
   ],
   "source": [
    "# sacamos una cadena con fecha y hora para a√±adirla al nombre de los csv que se generan a continuaci√≥n\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasamos los dataframes con el sentimiento calculado a pandas\n",
    "df_eng_pred = pred_eng.toPandas()\n",
    "df_spa_pred = pred_spa.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se guardan los dataframes con el sentimiento calculado en csv\n",
    "df_eng_pred.to_csv('./data/predict_streaming_english_'+timestr+'.csv', index=False)\n",
    "df_spa_pred.to_csv('./data/predict_streaming_spanish_'+timestr+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1a26de0548>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardar datos en MongoDB\n",
    "conexion = 'mongodb://localhost:27017'\n",
    "client = MongoClient(conexion)\n",
    "\n",
    "# accedemos a la base de datos\n",
    "db = client.tfm_twitter\n",
    "# insertamos los dataframes en la tabla correspondiente\n",
    "db.tweets_streaming_english.insert_many(df_eng_pred.to_dict(\"records\"))\n",
    "db.tweets_streaming_spanish.insert_many(df_spa_pred.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tweets_sentiment_spa_20190829-161613\"}{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tweets_sentiment_eng_20190829-161613\"}True True\n"
     ]
    }
   ],
   "source": [
    "# guardar datos en ElasticSearch\n",
    "es = Elasticsearch('http://localhost:9200/')\n",
    "\n",
    "# a√±adimos fecha y hora a los √≠ndices\n",
    "indice_spa = \"tweets_sentiment_spa_\"+timestr\n",
    "indice_eng = \"tweets_sentiment_eng_\"+timestr\n",
    "\n",
    "# primero comprobamos si ya existen los √≠ndices y se borran\n",
    "if es.indices.exists(indice_spa):\n",
    "    !curl -X DELETE localhost:9200/{indice_spa}\n",
    "if es.indices.exists(indice_eng):\n",
    "    !curl -X DELETE localhost:9200/{indice_eng}\n",
    "\n",
    "# generaci√≥n de √≠ndices\n",
    "!curl -X PUT localhost:9200/{indice_spa}\n",
    "!curl -X PUT localhost:9200/{indice_eng}\n",
    "\n",
    "TYPE = \"record\"\n",
    "\n",
    "def rec_to_actions(df, lang):\n",
    "    for record in df.to_dict(orient=\"records\"):\n",
    "        if(lang==0): yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (indice_spa, TYPE))\n",
    "        elif(lang==1): yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (indice_eng, TYPE))\n",
    "        yield (json.dumps(record, default=int))\n",
    "\n",
    "if not es.indices.exists(indice_spa):\n",
    "    raise RuntimeError('index does not exists, use `curl -X PUT \"localhost:9200/%s\"` and try again'%indice_spa)\n",
    "if not es.indices.exists(indice_eng):\n",
    "    raise RuntimeError('index does not exists, use `curl -X PUT \"localhost:9200/%s\"` and try again'%indice_eng)\n",
    "\n",
    "r_spa = es.bulk(rec_to_actions(df_spa_pred, 0))\n",
    "r_eng = es.bulk(rec_to_actions(df_eng_pred, 1)) \n",
    "\n",
    "print(not r_spa[\"errors\"], not r_eng[\"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "# se guardan los modelos utilizados para la predicci√≥n\n",
    "model_eng_best = model_eng.bestModel\n",
    "model_spa_best = model_spa.bestModel\n",
    "model_eng_best.write().overwrite().save('./data/models/model_eng_'+timestr)\n",
    "model_spa_best.write().overwrite().save('./data/models/model_spa_'+timestr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
