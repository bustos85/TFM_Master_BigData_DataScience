{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de ScikitLearn y prueba del modelo con distintos algoritmos de clasificaci√≥n para hacer el an√°lisis de sentimiento de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports y configuraciones necesarias\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "conf = (SparkSession\\\n",
    "          .builder\\\n",
    "          .appName(\"twitter\")\\\n",
    "          .master(\"spark://MacBook-Pro-de-Jose.local:7077\")\\\n",
    "          .config(\"spark.io.compression.codec\", \"snappy\")\\\n",
    "          .getOrCreate())  \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creaci√≥n de funciones necesarias, intentando modular y limpiar el notebook de c√≥digo repetitivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de carga de datos desde MongoDB de los datos almacenados desde Apache Nifi, tanto en ingl√©s como espa√±ol\n",
    "def carga_datos_mongo():\n",
    "    # conectamos con las tablas de Mongo desde donde cargamos los datos\n",
    "    df_mongo_english = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_english\").load()\n",
    "    df_mongo_spanish = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/tfm_twitter.tweets_spanish\").load()\n",
    "    \n",
    "    # nos quedamos solo con el atributo texto que es la informaci√≥n que nos interesa de cada dataset\n",
    "    df_text_english = df_mongo_english[['text']]\n",
    "    df_text_spanish = df_mongo_spanish[['text']]\n",
    "    \n",
    "    # pasamos a DF pandas\n",
    "    df_pandas_english = df_text_english.toPandas()\n",
    "    df_pandas_spanish = df_text_spanish.toPandas()\n",
    "    \n",
    "    return df_pandas_english, df_pandas_spanish\n",
    "\n",
    "\n",
    "# Funci√≥n de carga de datos desde los csv generados con anterioridad con registros ya con el sentimiento anotado\n",
    "def carga_datos_csv(csv):\n",
    "    # cargamos el csv\n",
    "    if(csv=='english_full'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_english.csv', sep=',')\n",
    "    elif(csv=='spanish_full'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_spanish.csv', sep=',')\n",
    "    elif(csv=='english_neutro'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_english_neutral.csv', sep=',')\n",
    "    elif(csv=='spanish_neutro'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_spanish_neutral.csv', sep=',')\n",
    "    elif(csv=='english_noNeutro'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_english_noNeutral.csv', sep=',')\n",
    "    elif(csv=='spanish_noNeutro'):\n",
    "        df_anotados = pd.read_csv('./data/df_result_spanish_noNeutral.csv', sep=',')\n",
    "        \n",
    "    # eliminamos posibles valores de sentimiento vacio, pas√°ndo el valor a -1 y eliminando el registro despu√©s\n",
    "    df_anotados = df_anotados.fillna(-1)\n",
    "    df_anotados = df_anotados[df_anotados['sentiment']!=-1]\n",
    "    \n",
    "    # el atributo sentimiento nos aseguramos que sea tipo int en lugar de float\n",
    "    col_float = df_anotados.columns[df_anotados.dtypes == float]\n",
    "    df_anotados[col_float] = df_anotados[col_float].astype(int)\n",
    "\n",
    "    # como los datos en ingl√©s del df full son muy grandes, para evitar problemas computacionales nos quedamos \n",
    "    # con una parte similar al tama√±o de los datos que tenemos en espa√±ol\n",
    "    if(csv=='english_full'):\n",
    "        df_anotados = df_anotados.sample(frac=0.04, replace=False, random_state=1)\n",
    "    \n",
    "    return df_anotados\n",
    "\n",
    "\n",
    "# Funci√≥n de visualizaci√≥n de DFs: n√∫mero de registros y columnas, lista de columnas, recuento de los distintos\n",
    "# valores de la columna sentimiento.\n",
    "def visualizar_datos_df(df,tipo):\n",
    "    print(\"num_rows: %d\\tColumnas: %d\\n\" % (df.shape[0], df.shape[1]) )\n",
    "    print(\"Columnas:\\n\", list(df.columns))\n",
    "    print(\"\\n\")\n",
    "    if(tipo==0):\n",
    "        print(\"Recuento valores columna sentimiento:\\n\", pd.value_counts(df['sentiment']))\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Funci√≥n de limpieza de los tweets\n",
    "def limpiar_tweet(tweet):\n",
    "    # quitamos RT, @nombre_usuario, links y urls, hashtags, menciones, caracteres extra√±os o emoticonos\n",
    "    tweet = re.sub('  +', ' ', tweet)\n",
    "    # eliminar acentos\n",
    "    tweet = ''.join((c for c in unicodedata.normalize('NFD', tweet) if unicodedata.category(c) != 'Mn'))  \n",
    "    # convertir la repetici√≥n de una letra m√°s de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "    # eliminar \"RT\", \"@usuario\", o los enlaces que es informaci√≥n que no ser√≠a √∫til analizar\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub('','',tweet).lower() \n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet)\n",
    "    tweet = re.sub(r'[0-9]', '', tweet) \n",
    "\n",
    "    return tweet \n",
    "\n",
    "\n",
    "# Funci√≥n que le pasamos un Dataframe y nos lo devuelve con los datos preprocesados con la funci√≥n anterior\n",
    "def limpiar_df(df):\n",
    "    df_clean = df.copy()\n",
    "    df_clean['text'] = df_clean['text'].apply(limpiar_tweet)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Funci√≥n que genera los conjuntos de train y test a partir de un dataframe. Reparto 90% train - 10% test\n",
    "def split_df(df):\n",
    "    X_eng = df['text']\n",
    "    y_eng = df['sentiment']\n",
    "\n",
    "    X, X_test, y, y_test = train_test_split(X_eng, y_eng, test_size=0.1, random_state=42)\n",
    "\n",
    "    return X, X_test, y, y_test\n",
    "\n",
    "\n",
    "# Funci√≥n que genera el pipeline y par√°metros del modelo, y lo entrena.\n",
    "# Se centra la prueba en ver el resultado con distintos clasificadores para ver que tipo de algoritmo da los \n",
    "# mejores resultados. En cuanto al vectorizador se prueba tanto con CountVectorizer como con TDIDFVectorizer.\n",
    "# No se definen muchos otros par√°metros por cuesti√≥n de tiempo y del elevado coste computacional en el caso de\n",
    "# jugar con muchos par√°metros y varias posibilidades para cada uno.\n",
    "def generar_modelo(clasificador, stopwords, X_train, y_train):\n",
    "    # uso de tokenizer que es una buena pr√°ctica en el tratamiento de textos\n",
    "    tokenizer = TweetTokenizer().tokenize\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', None),\n",
    "        ('classifier', clasificador)]\n",
    "    )\n",
    "    \n",
    "    # tambi√©n se prueba a usar o no stopwords, que ser√≠an palabras muy comunes en el lenguaje, que no tienen valor\n",
    "    # real a la hora de analizar el sentimiento de un texto y pueden no tenerse en cuenta.\n",
    "    params = {\n",
    "        'vectorizer': [CountVectorizer(binary=True,tokenizer=tokenizer),\\\n",
    "                       CountVectorizer(binary=False,tokenizer=tokenizer),\\\n",
    "                       TfidfVectorizer(use_idf=False, tokenizer=tokenizer),\\\n",
    "                       TfidfVectorizer(use_idf=True, tokenizer=tokenizer)],\n",
    "        'vectorizer__stop_words': [None, stopwords],\n",
    "    }\n",
    "\n",
    "    # uso tambi√©n de StratifiedKFold que es otra buena pr√°ctica\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    modelo = GridSearchCV(pipeline, params, n_jobs=-1, cv=skf, refit='f1_weighted')\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    return modelo\n",
    "\n",
    "\n",
    "# Funci√≥n que evalua el modelo que le pasamos y saca las m√©tricas que se quieren observar\n",
    "def evaluar_modelos(modelo1, modelo2, x_eng_test, x_spa_test, y_eng_test, y_spa_test):\n",
    "    # pintamos los mejores par√°metros de cada modelo\n",
    "    print(\"Mejores par√°metros del modelo para datos en ingl√©s: \\n\", modelo1.best_params_)\n",
    "    print(\"\\n\")\n",
    "    print(\"Mejores par√°metros del modelo para datos en espa√±ol: \\n\", modelo2.best_params_)\n",
    "\n",
    "    # se usan los modelos entrenados para predecir los datos de test\n",
    "    y_pred_eng = modelo1.predict(x_eng_test)\n",
    "    y_pred_spa = modelo2.predict(x_spa_test)\n",
    "\n",
    "    # pintamos las m√©tricas de cada modelo\n",
    "    print(\"\\n\")\n",
    "    print(\"M√©tricas del modelo con datos en ingl√©s:\\n\")\n",
    "    print(classification_report(y_eng_test, y_pred_eng, target_names=None))\n",
    "    print(\"\\n\")\n",
    "    print(\"M√©tricas del modelo con datos en espa√±ol:\\n\")\n",
    "    print(classification_report(y_spa_test, y_pred_spa, target_names=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos almacenados en MongoDB, de datos sin sentimiento calculado, por si probamos a utilizarlos con el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 322364\tColumnas: 1\n",
      "\n",
      "Columnas:\n",
      " ['text']\n",
      "\n",
      "\n",
      "num_rows: 79036\tColumnas: 1\n",
      "\n",
      "Columnas:\n",
      " ['text']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mongo_english, df_mongo_spanish = carga_datos_mongo()\n",
    "\n",
    "visualizar_datos_df(df_mongo_english,1)\n",
    "visualizar_datos_df(df_mongo_spanish,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@EJFC26 Y Messi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @btsmoonchild64: These group photos deserve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@rehankkhanNDS Overacting *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pay day play dayü§ëü§ëü§ëü§ëü§ë</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@pandaeyed1 Thank you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @liamyoung: Strange that Tony Blair has sud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hard work puts you where good luck can find you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @xCiphxr: When creative kids try playing co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @bonang_m: I‚Äôm working on one as we speak. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @akashbanerjee: After #PulwamaAttack, terro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                    @EJFC26 Y Messi\n",
       "1  RT @btsmoonchild64: These group photos deserve...\n",
       "2                        @rehankkhanNDS Overacting *\n",
       "3                              Pay day play dayü§ëü§ëü§ëü§ëü§ë\n",
       "4                             @pandaeyed1 Thank you!\n",
       "5  RT @liamyoung: Strange that Tony Blair has sud...\n",
       "6   Hard work puts you where good luck can find you.\n",
       "7  RT @xCiphxr: When creative kids try playing co...\n",
       "8  RT @bonang_m: I‚Äôm working on one as we speak. ...\n",
       "9  RT @akashbanerjee: After #PulwamaAttack, terro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mongo_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@EJFC26 Y Messi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @MBelenAlegre: Hermoso y sensual viernes ‚ù£Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Foro_TV: Suman 47 muertos y 640 heridos po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @revistaetcetera: .@lopezobrador_ dice que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me retracto de mi respuesta , √©l gobierno se s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anda a saber si te esta eligiendo o sos lo √∫ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @JCTrujilloCPCCS: Hay que propiciar una Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@RicciuP Terraplanistas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @oriolguellipuig: Jeje jeje https://t.co/NU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas? no quiero ser part...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                    @EJFC26 Y Messi\n",
       "1     RT @MBelenAlegre: Hermoso y sensual viernes ‚ù£Ô∏è\n",
       "2  RT @Foro_TV: Suman 47 muertos y 640 heridos po...\n",
       "3  RT @revistaetcetera: .@lopezobrador_ dice que ...\n",
       "4  Me retracto de mi respuesta , √©l gobierno se s...\n",
       "5  Anda a saber si te esta eligiendo o sos lo √∫ni...\n",
       "6  RT @JCTrujilloCPCCS: Hay que propiciar una Con...\n",
       "7                            @RicciuP Terraplanistas\n",
       "8  RT @oriolguellipuig: Jeje jeje https://t.co/NU...\n",
       "9  quienes son las veganarcas? no quiero ser part..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mongo_spanish.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos desde csv generados, con el sentimiento de cada texto anotado, tanto con datos en ingl√©s como en espa√±ol, para utilizarlos en el entrenamiento y testeo de los modelos y ver el mejor clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 70373\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    35092\n",
      "0    34714\n",
      "1      567\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 48658\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    26204\n",
      "0    19344\n",
      "1     3110\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv_english = carga_datos_csv('english_full')\n",
    "df_csv_spanish = carga_datos_csv('spanish_full')\n",
    "\n",
    "visualizar_datos_df(df_csv_english,0)\n",
    "visualizar_datos_df(df_csv_spanish,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128038</th>\n",
       "      <td>@BradshawPhotogr  seems i always end up at tx ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491756</th>\n",
       "      <td>@unitechy yeah, don't worry, you will!!! there...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470925</th>\n",
       "      <td>Excited that my email works reliably now with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491264</th>\n",
       "      <td>@MF213 yea that's the sad part</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836490</th>\n",
       "      <td>@modeladrienne you let me know! I think you to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371404</th>\n",
       "      <td>2155 - Well - Susan didn't make it ...   Runne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73350</th>\n",
       "      <td>..Before the cool runs out..Ima be trying my B...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166160</th>\n",
       "      <td>@dbdc LMAO not today sir sorry sir I did go ye...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070017</th>\n",
       "      <td>LOVES that lubbock is wet..its about time..no ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229521</th>\n",
       "      <td>I want a golden retriever puppy!! soooo cute!!...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment\n",
       "128038   @BradshawPhotogr  seems i always end up at tx ...          2\n",
       "491756   @unitechy yeah, don't worry, you will!!! there...          0\n",
       "470925   Excited that my email works reliably now with ...          0\n",
       "491264                     @MF213 yea that's the sad part           0\n",
       "836490   @modeladrienne you let me know! I think you to...          0\n",
       "371404   2155 - Well - Susan didn't make it ...   Runne...          0\n",
       "73350    ..Before the cool runs out..Ima be trying my B...          2\n",
       "1166160  @dbdc LMAO not today sir sorry sir I did go ye...          2\n",
       "1070017  LOVES that lubbock is wet..its about time..no ...          2\n",
       "229521   I want a golden retriever puppy!! soooo cute!!...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Buen d√≠a todos! Lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Desde el esca√±o. Todo listo para empezar #endi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bd√≠as. EM no se ira de puente. Si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Un sistema econ√≥mico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  @PauladeLasHeras No te libraras de ayudar me/n...          1\n",
       "1                          @marodriguezb Gracias MAR          2\n",
       "2  Off pensando en el regalito Sinde, la que se v...          0\n",
       "3  Conozco a alguien q es adicto al drama! Ja ja ...          2\n",
       "4  Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...          2\n",
       "5  Buen d√≠a todos! Lo primero mandar un abrazo gr...          2\n",
       "6  Desde el esca√±o. Todo listo para empezar #endi...          2\n",
       "7  Bd√≠as. EM no se ira de puente. Si vosotros os ...          2\n",
       "8  Un sistema econ√≥mico q recorta dinero para pre...          2\n",
       "9                  #programascambiados caca d ajuste          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv_spanish.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizamos una limpieza de los textos para eliminar caracteres que no tengan valor para el an√°lisis a realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 70373\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    35092\n",
      "0    34714\n",
      "1      567\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 48658\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    26204\n",
      "0    19344\n",
      "1     3110\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean_english = limpiar_df(df_csv_english)\n",
    "df_clean_spanish = limpiar_df(df_csv_spanish)\n",
    "\n",
    "visualizar_datos_df(df_clean_english,0)\n",
    "visualizar_datos_df(df_clean_spanish,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128038</th>\n",
       "      <td>seems i always end up at tx schl  email lori...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491756</th>\n",
       "      <td>yeah  don t worry  you will   there s still ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470925</th>\n",
       "      <td>excited that my email works reliably now with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491264</th>\n",
       "      <td>yea that s the sad part</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836490</th>\n",
       "      <td>you let me know  i think you too busy for li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371404</th>\n",
       "      <td>well   susan didn t make it    runner up to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73350</th>\n",
       "      <td>before the cool runs out  ima be trying my b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166160</th>\n",
       "      <td>lmao not today sir sorry sir i did go yester...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070017</th>\n",
       "      <td>loves that lubbock is wet  its about time  no ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229521</th>\n",
       "      <td>i want a golden retriever puppy   soo cute   d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment\n",
       "128038     seems i always end up at tx schl  email lori...          2\n",
       "491756     yeah  don t worry  you will   there s still ...          0\n",
       "470925   excited that my email works reliably now with ...          0\n",
       "491264                            yea that s the sad part           0\n",
       "836490     you let me know  i think you too busy for li...          0\n",
       "371404      well   susan didn t make it    runner up to...          0\n",
       "73350      before the cool runs out  ima be trying my b...          2\n",
       "1166160    lmao not today sir sorry sir i did go yester...          2\n",
       "1070017  loves that lubbock is wet  its about time  no ...          2\n",
       "229521   i want a golden retriever puppy   soo cute   d...          0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no te libraras de ayudar me nos  besos y gra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gracias mar</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>off pensando en el regalito sinde  la que se v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conozco a alguien q es adicto al drama  ja ja ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toca     grabacion dl especial navideno  mari ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>buen dia todos  lo primero mandar un abrazo gr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>desde el escano  todo listo para empezar endia...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bdias  em no se ira de puente  si vosotros os ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>un sistema economico q recorta dinero para pre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>programascambiados caca d ajuste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0    no te libraras de ayudar me nos  besos y gra...          1\n",
       "1                                        gracias mar          2\n",
       "2  off pensando en el regalito sinde  la que se v...          0\n",
       "3  conozco a alguien q es adicto al drama  ja ja ...          2\n",
       "4  toca     grabacion dl especial navideno  mari ...          2\n",
       "5  buen dia todos  lo primero mandar un abrazo gr...          2\n",
       "6  desde el escano  todo listo para empezar endia...          2\n",
       "7  bdias  em no se ira de puente  si vosotros os ...          2\n",
       "8  un sistema economico q recorta dinero para pre...          2\n",
       "9                   programascambiados caca d ajuste          0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_spanish.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separamos los datos tanto en ingl√©s como en espa√±ol en conjuntos de entrenamiento y de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eng_train, X_eng_test, y_eng_train, y_eng_test = split_df(df_clean_english)\n",
    "X_spa_train, X_spa_test, y_spa_train, y_spa_test = split_df(df_clean_spanish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba modelo final.\n",
    "A la hora de aplicar machine learning para abordar problemas como √©ste del an√°lisis de sentimiento, hay dos tipos de aprendizaje: supervisado y no supervisado.\n",
    "En este trabajo optamos por aplicar aprendizaje supervisado, que son m√©todos basados en algoritmos de aprendizaje autom√°tico que necesitan para entrenarse un conjunto de datos ya etiquetados.\n",
    "\n",
    "Vamos a probar con el modelo final y sus par√°metros que probamos anteriormente, con 4 algoritmos de clasificaci√≥n distintos, usando los siguientes: Naive Bayes, M√°quinas de vector soporte, K vecinos m√°s cercanos, y √°rboles de decisi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos las stopwords para cada idioma\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####¬†Algoritmo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los modelos para datos en ingl√©s y en espa√±ol\n",
    "clasificador = MultinomialNB()\n",
    "\n",
    "model_eng_naive = generar_modelo(clasificador, english_stopwords, X_eng_train, y_eng_train)\n",
    "model_spa_naive = generar_modelo(clasificador, spanish_stopwords, X_spa_train, y_spa_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores par√°metros del modelo para datos en ingl√©s: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a1ebf82b0>>,\n",
      "        use_idf=False, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores par√°metros del modelo para datos en espa√±ol: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a1ebf8358>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en ingl√©s:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78      3497\n",
      "           1       0.00      0.00      0.00        62\n",
      "           2       0.78      0.73      0.76      3479\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      7038\n",
      "   macro avg       0.51      0.51      0.51      7038\n",
      "weighted avg       0.76      0.76      0.76      7038\n",
      "\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en espa√±ol:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.80      1881\n",
      "           1       0.57      0.03      0.05       310\n",
      "           2       0.87      0.86      0.86      2675\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      4866\n",
      "   macro avg       0.73      0.59      0.57      4866\n",
      "weighted avg       0.80      0.81      0.79      4866\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josemanuel/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos\n",
    "evaluar_modelos(model_eng_naive, model_spa_naive, X_eng_test, X_spa_test, y_eng_test, y_spa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo K-vecinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los modelos para datos en ingl√©s y en espa√±ol\n",
    "clasificador = KNeighborsClassifier()\n",
    "\n",
    "model_eng_vecinos = generar_modelo(clasificador, english_stopwords, X_eng_train, y_eng_train)\n",
    "model_spa_vecinos = generar_modelo(clasificador, spanish_stopwords, X_spa_train, y_spa_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores par√°metros del modelo para datos en ingl√©s: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a2471b160>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores par√°metros del modelo para datos en espa√±ol: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a2471b208>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en ingl√©s:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67      3497\n",
      "           1       0.60      0.05      0.09        62\n",
      "           2       0.66      0.69      0.67      3479\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      7038\n",
      "   macro avg       0.65      0.46      0.48      7038\n",
      "weighted avg       0.67      0.67      0.67      7038\n",
      "\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en espa√±ol:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.40      0.50      1881\n",
      "           1       0.23      0.03      0.06       310\n",
      "           2       0.64      0.89      0.75      2675\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      4866\n",
      "   macro avg       0.51      0.44      0.43      4866\n",
      "weighted avg       0.63      0.65      0.61      4866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos\n",
    "evaluar_modelos(model_eng_vecinos, model_spa_vecinos, X_eng_test, X_spa_test, y_eng_test, y_spa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo M√°quinas vector soporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los modelos para datos en ingl√©s y en espa√±ol\n",
    "clasificador = LinearSVC()\n",
    "\n",
    "model_eng_svms = generar_modelo(clasificador, english_stopwords, X_eng_train, y_eng_train)\n",
    "model_spa_svms = generar_modelo(clasificador, spanish_stopwords, X_spa_train, y_spa_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores par√°metros del modelo para datos en ingl√©s: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a28d09710>>,\n",
      "        use_idf=False, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores par√°metros del modelo para datos en espa√±ol: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a23335358>>,\n",
      "        use_idf=False, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en ingl√©s:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      3497\n",
      "           1       0.75      0.10      0.17        62\n",
      "           2       0.76      0.78      0.77      3479\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      7038\n",
      "   macro avg       0.76      0.55      0.57      7038\n",
      "weighted avg       0.77      0.77      0.77      7038\n",
      "\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en espa√±ol:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      1881\n",
      "           1       0.55      0.10      0.16       310\n",
      "           2       0.86      0.91      0.89      2675\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      4866\n",
      "   macro avg       0.74      0.62      0.63      4866\n",
      "weighted avg       0.82      0.84      0.82      4866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos\n",
    "evaluar_modelos(model_eng_svms, model_spa_svms, X_eng_test, X_spa_test, y_eng_test, y_spa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo √°rboles de decisi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos los modelos para datos en ingl√©s y en espa√±ol\n",
    "clasificador = DecisionTreeClassifier()\n",
    "\n",
    "model_eng_tree = generar_modelo(clasificador, english_stopwords, X_eng_train, y_eng_train)\n",
    "model_spa_tree = generar_modelo(clasificador, spanish_stopwords, X_spa_train, y_spa_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores par√°metros del modelo para datos en ingl√©s: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a2531f860>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores par√°metros del modelo para datos en espa√±ol: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'm√°s', 'pero', 'sus', 'le', 'ya', 'o', 'este', 's√≠', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'tambi√©n', 'me', 'hasta', 'hay', 'donde'...tuvi√©semos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened'],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a291da630>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'm√°s', 'pero', 'sus', 'le', 'ya', 'o', 'este', 's√≠', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'tambi√©n', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'm√≠', 'antes', 'algunos', 'qu√©', 'unos', 'yo', 'otro', 'otras', 'otra', '√©l', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 't√∫', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'm√≠o', 'm√≠a', 'm√≠os', 'm√≠as', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'est√°s', 'est√°', 'estamos', 'est√°is', 'est√°n', 'est√©', 'est√©s', 'estemos', 'est√©is', 'est√©n', 'estar√©', 'estar√°s', 'estar√°', 'estaremos', 'estar√©is', 'estar√°n', 'estar√≠a', 'estar√≠as', 'estar√≠amos', 'estar√≠ais', 'estar√≠an', 'estaba', 'estabas', 'est√°bamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuvi√©ramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuvi√©semos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'hab√©is', 'han', 'haya', 'hayas', 'hayamos', 'hay√°is', 'hayan', 'habr√©', 'habr√°s', 'habr√°', 'habremos', 'habr√©is', 'habr√°n', 'habr√≠a', 'habr√≠as', 'habr√≠amos', 'habr√≠ais', 'habr√≠an', 'hab√≠a', 'hab√≠as', 'hab√≠amos', 'hab√≠ais', 'hab√≠an', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubi√©ramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubi√©semos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'se√°is', 'sean', 'ser√©', 'ser√°s', 'ser√°', 'seremos', 'ser√©is', 'ser√°n', 'ser√≠a', 'ser√≠as', 'ser√≠amos', 'ser√≠ais', 'ser√≠an', 'era', 'eras', '√©ramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fu√©ramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fu√©semos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'ten√©is', 'tienen', 'tenga', 'tengas', 'tengamos', 'teng√°is', 'tengan', 'tendr√©', 'tendr√°s', 'tendr√°', 'tendremos', 'tendr√©is', 'tendr√°n', 'tendr√≠a', 'tendr√≠as', 'tendr√≠amos', 'tendr√≠ais', 'tendr√≠an', 'ten√≠a', 'ten√≠as', 'ten√≠amos', 'ten√≠ais', 'ten√≠an', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuvi√©ramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuvi√©semos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']}\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en ingl√©s:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70      3497\n",
      "           1       0.32      0.16      0.22        62\n",
      "           2       0.69      0.69      0.69      3479\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      7038\n",
      "   macro avg       0.57      0.52      0.54      7038\n",
      "weighted avg       0.69      0.69      0.69      7038\n",
      "\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en espa√±ol:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70      1881\n",
      "           1       0.24      0.13      0.17       310\n",
      "           2       0.79      0.83      0.81      2675\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      4866\n",
      "   macro avg       0.58      0.56      0.56      4866\n",
      "weighted avg       0.72      0.74      0.73      4866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos\n",
    "evaluar_modelos(model_eng_tree, model_spa_tree, X_eng_test, X_spa_test, y_eng_test, y_spa_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados del modelo tras probarlo con los 4 algoritmos de clasificaci√≥n.\n",
    "\n",
    "Los mejores resultados vuelven a obtenerse de forma general, teniendo en cuenta las clases en conjunto y las distintas m√©tricas que se obtienen en cada evaluaci√≥n, con las m√°quinas vector soporte, en este caso el algoritmo LinearSVC.\n",
    "\n",
    "Los resultados suelen rondar aproximadamente sobre el 75% con los datos en Ingl√©s, y el 80% con los datos en Espa√±ol, llegando a sus m√°ximos valores con LinearSVC como hemos dicho, donde se alcanza un 77% y un 82% respectivamente.\n",
    "\n",
    "Como ya se dijo anteriormente en el trabajo, se podr√≠an probar muchos par√°metros m√°s, con distintos valores y combinaciones, para optimizar el modelo y poder subir los % para cada algoritmo, pero requiere un tiempo y un coste computacional que no se puede asumir.\n",
    "\n",
    "Nos quedamos con el clasificador LinearSVC y un modelo como el probado aqu√≠, para aplicarlo en streaming a los tweets que se vayan recogiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a cargar los csv que se generaron en ingl√©s y espa√±ol con el mismo n√∫mero de registros positivos y negativos en cada dataset, y los que tienen el mismo n√∫mero de registros positivos, negativos y neutros por dataset.\n",
    "\n",
    "Probaremos el modelo anterior con el clasificador que mejor resultado se ha obtenido con estos datasets, para ver como influye el reparto de clases por dataset, y el n√∫mero de registros, adem√°s de si hay 2 o 3 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows: 42000\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    14000\n",
      "1    14000\n",
      "0    14000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 9000\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    3000\n",
      "1    3000\n",
      "0    3000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 80000\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    40000\n",
      "0    40000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n",
      "num_rows: 38000\tColumnas: 2\n",
      "\n",
      "Columnas:\n",
      " ['text', 'sentiment']\n",
      "\n",
      "\n",
      "Recuento valores columna sentimiento:\n",
      " 2    19000\n",
      "0    19000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cargamos los 4 datasets\n",
    "df_csv_english_neutro = carga_datos_csv('english_neutro')\n",
    "df_csv_spanish_neutro = carga_datos_csv('spanish_neutro')\n",
    "df_csv_english_noNeutro = carga_datos_csv('english_noNeutro')\n",
    "df_csv_spanish_noNeutro = carga_datos_csv('spanish_noNeutro')\n",
    "\n",
    "visualizar_datos_df(df_csv_english_neutro,0)\n",
    "visualizar_datos_df(df_csv_spanish_neutro,0)\n",
    "visualizar_datos_df(df_csv_english_noNeutro,0)\n",
    "visualizar_datos_df(df_csv_spanish_noNeutro,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpiamos los datos\n",
    "df_clean_english_neutro = limpiar_df(df_csv_english_neutro)\n",
    "df_clean_spanish_neutro = limpiar_df(df_csv_spanish_neutro)\n",
    "df_clean_english_noNeutro = limpiar_df(df_csv_english_noNeutro)\n",
    "df_clean_spanish_noNeutro = limpiar_df(df_csv_spanish_noNeutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos el reparto de datos para entrenamiento y testing\n",
    "X_eng_train_neu, X_eng_test_neu, y_eng_train_neu, y_eng_test_neu = split_df(df_clean_english_neutro)\n",
    "X_spa_train_neu, X_spa_test_neu, y_spa_train_neu, y_spa_test_neu = split_df(df_clean_spanish_neutro)\n",
    "X_eng_train_noNeu, X_eng_test_noNeu, y_eng_train_noNeu, y_eng_test_noNeu = split_df(df_clean_english_noNeutro)\n",
    "X_spa_train_noNeu, X_spa_test_noNeu, y_spa_train_noNeu, y_spa_test_noNeu = split_df(df_clean_spanish_noNeutro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos el modelo con los 4 datasets anteriores\n",
    "clasificador = LinearSVC()\n",
    "\n",
    "model_eng_svms_neu = generar_modelo(clasificador, english_stopwords, X_eng_train_neu, y_eng_train_neu)\n",
    "model_spa_svms_neu = generar_modelo(clasificador, spanish_stopwords, X_spa_train_neu, y_spa_train_neu)\n",
    "model_eng_svms_noNeu = generar_modelo(clasificador, english_stopwords, X_eng_train_noNeu, y_eng_train_noNeu)\n",
    "model_spa_svms_noNeu = generar_modelo(clasificador, spanish_stopwords, X_spa_train_noNeu, y_spa_train_noNeu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores par√°metros del modelo para datos en ingl√©s: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a3a23f470>>,\n",
      "        use_idf=True, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores par√°metros del modelo para datos en espa√±ol: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a3a23f518>>,\n",
      "        use_idf=True, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en ingl√©s:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.66      1398\n",
      "           1       0.67      0.68      0.68      1375\n",
      "           2       0.65      0.62      0.63      1427\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      4200\n",
      "   macro avg       0.66      0.66      0.66      4200\n",
      "weighted avg       0.66      0.66      0.66      4200\n",
      "\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en espa√±ol:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75       282\n",
      "           1       0.72      0.79      0.76       302\n",
      "           2       0.83      0.79      0.81       316\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       900\n",
      "   macro avg       0.77      0.77      0.77       900\n",
      "weighted avg       0.77      0.77      0.77       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos con 3 clases: positivos, negativos y neutros\n",
    "evaluar_modelos(model_eng_svms_neu, model_spa_svms_neu, X_eng_test_neu, X_spa_test_neu,\\\n",
    "                y_eng_test_neu, y_spa_test_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores par√°metros del modelo para datos en ingl√©s: \n",
      " {'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a3a23f7b8>>,\n",
      "        use_idf=False, vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "Mejores par√°metros del modelo para datos en espa√±ol: \n",
      " {'vectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x1a3cdd0f98>>,\n",
      "        vocabulary=None), 'vectorizer__stop_words': None}\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en ingl√©s:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78      4025\n",
      "           2       0.78      0.79      0.78      3975\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      8000\n",
      "   macro avg       0.78      0.78      0.78      8000\n",
      "weighted avg       0.78      0.78      0.78      8000\n",
      "\n",
      "\n",
      "\n",
      "M√©tricas del modelo con datos en espa√±ol:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      1902\n",
      "           2       0.94      0.94      0.94      1898\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      3800\n",
      "   macro avg       0.94      0.94      0.94      3800\n",
      "weighted avg       0.94      0.94      0.94      3800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluamos los modelos con 2 clases: positivos y negativos\n",
    "evaluar_modelos(model_eng_svms_noNeu, model_spa_svms_noNeu, X_eng_test_noNeu, X_spa_test_noNeu,\\\n",
    "                y_eng_test_noNeu, y_spa_test_noNeu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen los siguientes resultados:\n",
    "- Modelo entrenado y testeado con los datasets con 3 clases y equilibrados en el n√∫mero de registros por clase: 66% con datos en ingl√©s, y 77% con datos en espa√±ol.\n",
    "- Modelo entrenado y testeado con los datasets con 2 clases (positivo y negativo) y equilibrados en el n√∫mero de registros por clase: 78% y 94%, para ingl√©s y espa√±ol respectivamente.\n",
    "\n",
    "Como era de suponer se obtienen mejores resultados s√≥lo con 2 clases, y se sigue manteniendo la tendencia de mejor clasificaci√≥n con datos en espa√±ol que en ingl√©s. De todos modos tambi√©n hay que pensar que los datasets son m√°s peque√±os y con menor n√∫mero de datos los resultados pueden ser menos fiables.\n",
    "Adem√°s la realidad es que al menos deben tener en cuenta 3 clases, ya que como m√≠nimo siempre se deben clasificar los sentimientos en negativo, neutro o positivo.\n",
    "\n",
    "Se puede apreciar como entrenando con datasets equilibrados entre sus distintas clases, se obtienen resultados buenos y similares para cada clase, no como ocurr√≠a al entrenar y testear con un dataset desequilibrado, que la clase con menos registros obtiene unos pobres resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba a predecir el sentimiento de los datos cargados desde MongoDB con el modelo con mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos cargados desde MongoDB\n",
    "# limpiamos los textos\n",
    "df_clean_mongo_english = limpiar_df(df_mongo_english)\n",
    "df_clean_mongo_spanish = limpiar_df(df_mongo_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y messi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these group photos deserve more attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overacting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pay day play day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>strange that tony blair has suddenly become...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hard work puts you where good luck can find you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when creative kids try playing comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m  i m working on one as we speak  thank you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>after pulwamaattack  terrorists scored a bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                            y messi\n",
       "1         these group photos deserve more attention \n",
       "2                                       overacting  \n",
       "3                                 pay day play day  \n",
       "4                                         thank you \n",
       "5     strange that tony blair has suddenly become...\n",
       "6   hard work puts you where good luck can find you \n",
       "7               when creative kids try playing comp \n",
       "8    m  i m working on one as we speak  thank you...\n",
       "9     after pulwamaattack  terrorists scored a bi..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_mongo_english.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y messi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hermoso y sensual viernes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tv  suman  muertos y  heridos por explosion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dice que fue tan grande la gloria de be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me retracto de mi respuesta   el gobierno se s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anda a saber si te esta eligiendo o sos lo uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hay que propiciar una consulta popular para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>terraplanistas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jeje jeje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas  no quiero ser part...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                            y messi\n",
       "1                        hermoso y sensual viernes  \n",
       "2    tv  suman  muertos y  heridos por explosion ...\n",
       "3         dice que fue tan grande la gloria de be...\n",
       "4  me retracto de mi respuesta   el gobierno se s...\n",
       "5  anda a saber si te esta eligiendo o sos lo uni...\n",
       "6     hay que propiciar una consulta popular para...\n",
       "7                                     terraplanistas\n",
       "8                                         jeje jeje \n",
       "9  quienes son las veganarcas  no quiero ser part..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_mongo_spanish.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecimos usando el modelo de SVMs que ha obtenido los mejores resultados\n",
    "y_pred_mongo_eng = model_eng_svms.predict(df_clean_mongo_english.text)\n",
    "y_pred_mongo_spa = model_spa_svms.predict(df_clean_mongo_spanish.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos una copia de los dataframes con los textos limpios, y les a√±adimos la columna sentimiento con los valores\n",
    "# que se han predecido con el modelo.\n",
    "df_predict_english = df_clean_mongo_english.copy()\n",
    "df_predict_spanish = df_clean_mongo_spanish.copy()\n",
    "\n",
    "df_predict_english['sentiment'] = y_pred_mongo_eng\n",
    "df_predict_spanish['sentiment'] = y_pred_mongo_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y messi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these group photos deserve more attention</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overacting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pay day play day</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>strange that tony blair has suddenly become...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hard work puts you where good luck can find you</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when creative kids try playing comp</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m  i m working on one as we speak  thank you...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>after pulwamaattack  terrorists scored a bi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i am people</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vintage  piece hammered copper pot large bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>beyonce  world stop   world</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>no  it just needs money and connections  your ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the upcoming album from   boasts  new songs...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i see ya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>it s time to run our own country    retweet...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>this works  i m not even gonna front  ret...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>how many followers you want     retweet   f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>do me a favor right  follow this absolute top ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>parkland shooting survivor sydney aiello ta...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>trying to prove a point  lets settle this o...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nwofra why can t the feminist follow yo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>be among the top followers retweeting thi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>love you</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>allergies are already acting up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>activist asks if russia is living in the  ston...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>am shocked to find my name on the poster of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cronos group is set to report earnings tuesday...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "0                                             y messi          2\n",
       "1          these group photos deserve more attention           2\n",
       "2                                        overacting            2\n",
       "3                                  pay day play day            2\n",
       "4                                          thank you           2\n",
       "5      strange that tony blair has suddenly become...          0\n",
       "6    hard work puts you where good luck can find you           2\n",
       "7                when creative kids try playing comp           2\n",
       "8     m  i m working on one as we speak  thank you...          2\n",
       "9      after pulwamaattack  terrorists scored a bi...          2\n",
       "10                                       i am people           2\n",
       "11     vintage  piece hammered copper pot large bo...          0\n",
       "12                     beyonce  world stop   world             2\n",
       "13  no  it just needs money and connections  your ...          0\n",
       "14     the upcoming album from   boasts  new songs...          2\n",
       "15                                         i see ya            2\n",
       "16     it s time to run our own country    retweet...          2\n",
       "17       this works  i m not even gonna front  ret...          2\n",
       "18     how many followers you want     retweet   f...          2\n",
       "19  do me a favor right  follow this absolute top ...          2\n",
       "20     parkland shooting survivor sydney aiello ta...          2\n",
       "21     trying to prove a point  lets settle this o...          2\n",
       "22         nwofra why can t the feminist follow yo...          2\n",
       "23       be among the top followers retweeting thi...          2\n",
       "24                                          love you           2\n",
       "25                                                ...          0\n",
       "26                    allergies are already acting up          0\n",
       "27  activist asks if russia is living in the  ston...          2\n",
       "28     am shocked to find my name on the poster of...          2\n",
       "29  cronos group is set to report earnings tuesday...          2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict_english.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y messi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hermoso y sensual viernes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tv  suman  muertos y  heridos por explosion ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dice que fue tan grande la gloria de be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me retracto de mi respuesta   el gobierno se s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anda a saber si te esta eligiendo o sos lo uni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hay que propiciar una consulta popular para...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>terraplanistas</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jeje jeje</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quienes son las veganarcas  no quiero ser part...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mas dias con este clima</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mood del fandom despues de haber recaudado ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>yo te quiero pa  mi no te quiero pa  mas na...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>me llego a poner eso y parezco un arrollado...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>un gran saludo a mis camaradas patriotas   ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>yo tambien vi a una chica army en la u pero ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cultura el festival de comic europeo de ubeda ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>solo entro en twitter por ti</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cas  que tan feo debes ser  para preferir un...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>portararmasjelo    estados unidos es el uni...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lastima  gracias</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>profe  hoy   de marzo de   las cortes espano...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>una falta de respeto q yo siga viva</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>te acostas con las patas asi en mi cama y t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vieron que taehyung anda super agresivo ult...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tara</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>preguntas fueron</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pero en cataluna quien cono trabaja  la em...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>amigos un amigo que era super socialista empez...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>un album con sonidos del oceano hechos con...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "0                                             y messi          2\n",
       "1                         hermoso y sensual viernes            2\n",
       "2     tv  suman  muertos y  heridos por explosion ...          0\n",
       "3          dice que fue tan grande la gloria de be...          0\n",
       "4   me retracto de mi respuesta   el gobierno se s...          0\n",
       "5   anda a saber si te esta eligiendo o sos lo uni...          0\n",
       "6      hay que propiciar una consulta popular para...          2\n",
       "7                                      terraplanistas          2\n",
       "8                                          jeje jeje           2\n",
       "9   quienes son las veganarcas  no quiero ser part...          0\n",
       "10                           mas dias con este clima           2\n",
       "11     mood del fandom despues de haber recaudado ...          2\n",
       "12     yo te quiero pa  mi no te quiero pa  mas na...          2\n",
       "13     me llego a poner eso y parezco un arrollado...          2\n",
       "14     un gran saludo a mis camaradas patriotas   ...          2\n",
       "15    yo tambien vi a una chica army en la u pero ...          2\n",
       "16  cultura el festival de comic europeo de ubeda ...          2\n",
       "17                       solo entro en twitter por ti          2\n",
       "18    cas  que tan feo debes ser  para preferir un...          0\n",
       "19     portararmasjelo    estados unidos es el uni...          2\n",
       "20                                  lastima  gracias           2\n",
       "21    profe  hoy   de marzo de   las cortes espano...          2\n",
       "22               una falta de respeto q yo siga viva           2\n",
       "23     te acostas con las patas asi en mi cama y t...          2\n",
       "24     vieron que taehyung anda super agresivo ult...          0\n",
       "25                                             tara            2\n",
       "26                                   preguntas fueron          0\n",
       "27      pero en cataluna quien cono trabaja  la em...          0\n",
       "28  amigos un amigo que era super socialista empez...          2\n",
       "29      un album con sonidos del oceano hechos con...          2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predict_spanish.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan los primeros 30 tweets con el sentimiento predecido por el modelo, y se pueden ver varios ejemplos de un funcionamiento aceptablemente correcto.\n",
    "\n",
    "Tambi√©n se puede ver como se predicen m√°s etiquetas con valor positivo o 2, que del resto. Esto es debido a que si se ve al inicio el reparto de valores en los datasets usados para entrenar los modelos, el valor positivo es el que m√°s n√∫mero de datos tiene. Adem√°s, no se ve ning√∫n valor neutral, ya que los datasets tienen muy pocos valores neutrales, y por tanto el modelo no va a predecir f√°cilmente esta etiqueta.\n",
    "\n",
    "Para obtener un modelo con el mejor funcionamiento posible, har√≠a falta que estuviera totalmente balanceado entre todas sus clases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
