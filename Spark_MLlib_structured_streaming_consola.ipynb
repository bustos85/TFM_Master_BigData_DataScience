{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Structured streaming con Spark ML.\n",
    "\n",
    "Se recogen los datos en streaming desde Kafka, y se les aplicar√° el an√°lisis de sentimiento para etiquetar el sentimiento de los tweets recogidos en streaming, utilizando Spark ML. Tambi√©n se almacenar√°n los tweets y su predicci√≥n en distintos formatos como ser√≠an una tabla de MongoDB, ficheros csv y en ElasticSearch para su posterior visualizaci√≥n con Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports y configuraciones necesarias\n",
    "# Spark Streaming\n",
    "from pyspark.streaming import StreamingContext  \n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "from pyspark.mllib.util import *\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# cargamos las stopwords para cada idioma\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# !pip install elasticsearch --si fuera necesario instalarlo\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"twitter\")\\\n",
    "        .master(\"spark://MacBook-Pro-de-Jose.local:7077\")\\\n",
    "        .config(\"spark.io.compression.codec\", \"snappy\")\\\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\\\n",
    "        .config(\"spark.mongodb.input.uri\",\"mongodb://localhost:27017/tfm_twitter\")\\\n",
    "        .config(\"spark.mongodb.output.uri\",\"mongodb://localhost:27017/tfm_twitter\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####¬†Creamos las funciones necesarias para los procesos posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de carga de datos desde los csv generados con anterioridad con registros ya con el sentimiento anotado.\n",
    "# Estos datos servir√°n para entrenar el modelo con el que predecir los tweets en streaming.\n",
    "def carga_datos_csv(csv):\n",
    "    schema_csv = StructType([ \n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"sentiment\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    # cargamos los csv\n",
    "    if(csv=='english_full'): fichero = './data/df_result_english.csv'\n",
    "    elif(csv=='spanish_full'): fichero = './data/df_result_spanish.csv'\n",
    "    elif(csv=='english_neutro'): fichero = './data/df_result_english_neutral.csv'\n",
    "    elif(csv=='spanish_neutro'): fichero = './data/df_result_spanish_neutral.csv'\n",
    "    elif(csv=='english_noNeutro'): fichero = './data/df_result_english_noNeutral.csv'\n",
    "    elif(csv=='spanish_noNeutro'): fichero = './data/df_result_spanish_noNeutral.csv'\n",
    "    \n",
    "    df_csv = sqlContext.\\\n",
    "        read.format(\"com.databricks.spark.csv\").\\\n",
    "        option(\"header\", \"true\").\\\n",
    "        option(\"inferschema\", \"true\").\\\n",
    "        option(\"mode\", \"DROPMALFORMED\").\\\n",
    "        schema(schema_csv).\\\n",
    "        load(fichero).\\\n",
    "        cache()\n",
    "    \n",
    "    return df_csv\n",
    "\n",
    "\n",
    "# Funciones de visualizaci√≥n de DFs para ver las columnas, el n√∫mero de registros o dimensiones, y el recuento\n",
    "# de valores del atributo sentimiento.\n",
    "def visualizar_datos_csv(df):\n",
    "    print(\"Columnas del dataframe: \", df.columns)\n",
    "    print(\"Numero de registros = %d\" % df.count())\n",
    "    print(\"\\n\")\n",
    "    print(df.limit(10).toPandas())\n",
    "    print(\"\\n\")\n",
    "    recuento_sentiment = df.select('sentiment').groupBy(\"sentiment\").count().show()\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# Funci√≥n para eliminar palabras que no queramos analizar. Ser√°n las palabras m√°s comunes de cada lenguaje, que no\n",
    "# tienen valor para hacer un an√°lisis de sentimiento, como podr√≠an ser art√≠culos o pronombres.\n",
    "def eliminar_stopwords(texto, palabras_eliminar):\n",
    "    tok = nltk.tokenize\n",
    "    palabras = tok.word_tokenize(texto)\n",
    "        \n",
    "    palabras_salida = []\n",
    "        \n",
    "    for palabra in palabras:\n",
    "        if palabra not in palabras_eliminar:\n",
    "            palabras_salida.append(palabra)\n",
    "        \n",
    "    salida = \"\"\n",
    "    for i in range(len(palabras_salida)):\n",
    "        if palabras_salida[i] in string.punctuation:\n",
    "            salida = salida.strip()+palabras_salida[i] + \" \"\n",
    "        else:\n",
    "            salida += palabras_salida[i] + \" \"\n",
    "\n",
    "    return salida\n",
    "\n",
    "\n",
    "# Funciones de limpieza de los tweets, para limpiar los textos de caracteres extra√±os, n√∫meros, urls o emoticonos.\n",
    "def limpieza_tweets_spanish(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    # eliminar full_text que es como comienzan los textos que son extendidos\n",
    "    tweet = [re.sub(r'full_text', '', s) for s in tweet] \n",
    "    \n",
    "    # convertir la repetici√≥n de una letra m√°s de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "    # eliminar acentos\n",
    "    tweet = [''.join((c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')) for s in tweet]\n",
    "        \n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\¬∫\\'\\‚Äô\\‚Äò\\|()%/\\\"{}@;:+\\[\\]\\‚Äì\\‚Äù\\‚Ä¶\\‚Äú\\„Äë\\„Äê=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, spanish_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "# A los datos en ingl√©s le aplicamos sus stopwords correspondientes y no les quitamos los acentos\n",
    "def limpieza_tweets_english(tokens : list) -> list:\n",
    "    tweet = [re.sub('  +', ' ', s).strip() for s in tokens]\n",
    "    tweet = [re.sub(r'http\\S+', '', s) for s in tweet]  \n",
    "    tweet = [re.sub(r'@[\\S]+', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'#(\\S+)', r' \\1 ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\brt\\b', '', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\.{2,}', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub(r'\\s+', ' ', s) for s in tweet]\n",
    "    tweet = [re.sub('','',s).lower() for s in tweet]\n",
    "    # eliminar full_text que es como comienzan los textos que son extendidos\n",
    "    tweet = [re.sub(r'full_text', '', s) for s in tweet] \n",
    "    \n",
    "    # convertir la repetici√≥n de una letra m√°s de 2 veces a 1\n",
    "    # biennnnn --> bien\n",
    "    tweet = [re.sub(r'(.)\\1+', r'\\1\\1', s) for s in tweet]\n",
    "    # remover - & '\n",
    "    tweet = [re.sub(r'(-|\\')', '', s) for s in tweet] \n",
    "\n",
    "    # reemplazar emojis\n",
    "    emoji_pattern = re.compile(u'['u'\\U0001F300-\\U0001F64F'u'\\U0001F680-\\U0001F6FF'u'\\\n",
    "                               \\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    " \n",
    "    tweet = [emoji_pattern.sub(r' ', s) for s in tweet]\n",
    "    tweet = [re.sub(\"[^A-Za-z]+$\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"^[^A-Za-z]+\",'',s) for s in tweet]\n",
    "    tweet = [re.sub(\"[\\$*&!?///\\¬∫\\'\\‚Äô\\‚Äò\\|()%/\\\"{}@;:+\\[\\]\\‚Äì\\‚Äù\\‚Ä¶\\‚Äú\\„Äë\\„Äê=]\",'',s) for s in tweet]  \n",
    "    \n",
    "    # remover stopwords\n",
    "    tweet = [eliminar_stopwords(s, english_stopwords) for s in tweet]\n",
    "\n",
    "    filtered = filter(None, tweet)\n",
    "    \n",
    "    return list(filtered)\n",
    "\n",
    "\n",
    "# Funci√≥n de preprocesado de los datos que tokeniza los textos y llama a las funciones de limpieza anteriores.\n",
    "# Devolver√° los dos dataframes, con datos en ingl√©s y espa√±ol, con los datos listos para aplicar el modelo.\n",
    "def preprocesado_dataframe(df1, df2, tipo):\n",
    "    # primero usamos tokenizer y vamos a partir los tweets por palabras\n",
    "    if(tipo==0): tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"token\")\n",
    "    elif(tipo==1): tokenizer = Tokenizer(inputCol = \"texto\", outputCol = \"token\")\n",
    "\n",
    "    df_tokens_english = tokenizer.transform(df1)\n",
    "    df_tokens_spanish = tokenizer.transform(df2)  \n",
    "    \n",
    "    # usamos las funciones de limpieza y preprocesado con ambos DFs ya con los textos divididos en tokens\n",
    "    limpiezaUDF_english = F.udf(limpieza_tweets_english, ArrayType(StringType()))\n",
    "    limpiezaUDF_spanish = F.udf(limpieza_tweets_spanish, ArrayType(StringType()))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.withColumn(\"tokens_clean\", limpiezaUDF_english(df_tokens_english[\"token\"]))\n",
    "    df_tokens_spanish = df_tokens_spanish.withColumn(\"tokens_clean\", limpiezaUDF_spanish(df_tokens_spanish[\"token\"]))\n",
    "\n",
    "    df_tokens_english = df_tokens_english.drop(\"token\")\n",
    "    df_tokens_spanish = df_tokens_spanish.drop(\"token\")\n",
    "\n",
    "    df_tokens_english_clean = df_tokens_english.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    df_tokens_spanish_clean = df_tokens_spanish.where(F.size(F.col(\"tokens_clean\")) > 0)\n",
    "    \n",
    "    return df_tokens_english_clean, df_tokens_spanish_clean\n",
    "\n",
    "\n",
    "# Funci√≥n que seg√∫n el valor del atributo Truncated, se queda con el valor de Text o de Extended_tweet, si el texto\n",
    "# del tweet es demasiado largo y est√° extendido.\n",
    "def udf_cambiar_texto(col1,col2,col3):\n",
    "    if(col1==True):\n",
    "        col2=col3\n",
    "    return col2\n",
    "\n",
    "\n",
    "# Funci√≥n udf para calcular fecha y hora y a√±adirla posteriormente al dataframe.\n",
    "def add_timestamp():\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return timestamp\n",
    "\n",
    "\n",
    "# Funci√≥n que pasa del tipo DenseVector a un array, que es mejor para tratar con estos datos.\n",
    "def sparse_to_array(v):\n",
    "    v = DenseVector(v)\n",
    "    new_array = list([float(x) for x in v])\n",
    "    return new_array\n",
    "\n",
    "\n",
    "# Funci√≥n udf para seg√∫n el valor del sentimiento calculado por el modelo, quedarnos con el valor\n",
    "# del array de probabilidad que corresponde a esa etiqueta de sentimiento, y as√≠ ver en cada caso el % con el\n",
    "# cual el modelo ha etiquetado el sentimiento.\n",
    "def udf_probability(col1,col2):\n",
    "    if(col2==0.0):\n",
    "        col1=col1[0]\n",
    "    elif(col2==1.0):\n",
    "        col1=col1[1]\n",
    "    elif(col2==2.0):\n",
    "        col1=col1[2]\n",
    "        \n",
    "    return col1\n",
    "\n",
    "\n",
    "# Funci√≥n que engloba las transformaciones y predicciones a realizar a los datos en streaming, para acabar teniendo\n",
    "# las columnas que queremos de los datos y el sentimiento calculado.\n",
    "def generacion_dataframe_streaming(df_1, df_2, model_1, model_2):\n",
    "    # sumamos las 4 columnas del recuento de replies, favorites, quotes y retweet para tener una sola columna con la\n",
    "    # suma de todas estas interacciones de un tweet.\n",
    "    df_english = df_1.withColumn('interacciones', df_1.favorite_count\\\n",
    "                + df_1.quote_count + df_1.reply_count + df_1.retweet_count)\n",
    "    df_spanish = df_2.withColumn('interacciones', df_2.favorite_count\\\n",
    "                + df_2.quote_count + df_2.reply_count + df_2.retweet_count)\n",
    "\n",
    "    # aplicamos la funci√≥n para coger el texto de la columna extended_tweet si es necesario, y sustituir al texto.\n",
    "    cambiar_texto_udf=F.udf(udf_cambiar_texto)\n",
    "    df_spanish = df_spanish.select('*').withColumn(\"texto\", cambiar_texto_udf(\"truncated\",\"text\",\"extended_tweet\"))\n",
    "    df_english = df_english.select('*').withColumn(\"texto\", cambiar_texto_udf(\"truncated\",\"text\",\"extended_tweet\"))\n",
    "    \n",
    "    # llamamos a la funci√≥n que engloba el preprocesado de los datos, con la tokenizaci√≥n y la limpieza de tweets.\n",
    "    df_tokens_english, df_tokens_spanish = preprocesado_dataframe(df_english,df_spanish,1)\n",
    "    df_english_clean = df_tokens_english.withColumnRenamed(\"sentiment\", \"label\")\n",
    "    df_spanish_clean = df_tokens_spanish.withColumnRenamed(\"sentiment\", \"label\")\n",
    "\n",
    "    # usamos el modelo para predecir el sentimiento de los tweets en los distintos DFs por idioma.\n",
    "    pred_eng = model_1.transform(df_english_clean)\n",
    "    pred_spa = model_2.transform(df_spanish_clean)\n",
    "\n",
    "    # usamos la funci√≥n para a√±adir la fecha y hora a cada registro\n",
    "    add_timestamp_udf = F.udf(add_timestamp, StringType())\n",
    "    # a√±adimos la columna con el timestamp a los datos\n",
    "    pred_eng = pred_eng.withColumn(\"timestamp\", add_timestamp_udf())\n",
    "    pred_spa = pred_spa.withColumn(\"timestamp\", add_timestamp_udf())\n",
    "\n",
    "    # pasamos a array el DenseVector con la probabilidad calculada por el modelo al predecir el sentimiento.\n",
    "    sparse_to_array_udf = F.udf(sparse_to_array, ArrayType(FloatType()))\n",
    "    pred_eng = pred_eng.withColumn('probability_array', sparse_to_array_udf('probability'))\n",
    "    pred_spa = pred_spa.withColumn('probability_array', sparse_to_array_udf('probability'))\n",
    "    # del array de probabilidad nos quedamos solo con la probabilidad del sentimiento predecido en cada registro.\n",
    "    probability_udf=F.udf(udf_probability)\n",
    "    pred_eng = pred_eng.withColumn('probabilidad', probability_udf('probability_array','prediction'))\n",
    "    pred_spa = pred_spa.withColumn('probabilidad', probability_udf('probability_array','prediction'))\n",
    "\n",
    "    # eliminamos las columnas que no queremos sacar con los datos, y posteriormente las ordenamos seg√∫n se desea.\n",
    "    columns_to_drop = ['favorite_count', 'quote_count', 'reply_count', 'retweet_count', 'label',\\\n",
    "                       'countfeatures', 'features', 'rawPrediction', 'tokens_clean', 'sentiment']\n",
    "    pred_spa = pred_spa.drop(*columns_to_drop)\n",
    "    pred_eng = pred_eng.drop(*columns_to_drop)\n",
    "\n",
    "    columns = ['id', 'texto', 'interacciones', 'probabilidad', 'prediction', 'timestamp']\n",
    "    pred_eng = pred_eng[columns] \n",
    "    pred_spa = pred_spa[columns]\n",
    "    \n",
    "    return pred_eng, pred_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de los datos guardados en formato .csv para entrenar el modelo a usar con los tweets que vienen en streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los distintos csv generados con anterioridad y con el sentimiento ya etiquetado.\n",
    "df_csv_english_full = carga_datos_csv('english_full')\n",
    "df_csv_spanish_full = carga_datos_csv('spanish_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 175818\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @VirginAmerica amazing to me that we can't get...          0\n",
      "1  @VirginAmerica View of downtown Los Angeles, t...          2\n",
      "2  @VirginAmerica plz help me win my bid upgrade ...          1\n",
      "3  @VirginAmerica I'm #elevategold for a good rea...          2\n",
      "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1\n",
      "5  @VirginAmerica I‚Äôm having trouble adding this ...          0\n",
      "6  @VirginAmerica you have the absolute best team...          2\n",
      "7  @VirginAmerica has flight number 276 from SFO ...          1\n",
      "8  @VirginAmerica Another delayed flight? #liking...          0\n",
      "9  @VirginAmerica Can you find us a flt out of LA...          1\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 1372|\n",
      "|        2|87372|\n",
      "|        0|87074|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cogemos un sample del DF con datos en ingl√©s ya que tiene muchos datos y da problemas de c√≥mputo y tiempos.\n",
    "df_csv_english_sample = df_csv_english_full.sample(False, 0.1, 45)\n",
    "\n",
    "visualizar_datos_csv(df_csv_english_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del dataframe:  ['text', 'sentiment']\n",
      "Numero de registros = 46787\n",
      "\n",
      "\n",
      "                                                text  sentiment\n",
      "0  @PauladeLasHeras No te libraras de ayudar me/n...          1\n",
      "1                          @marodriguezb Gracias MAR          2\n",
      "2  Off pensando en el regalito Sinde, la que se v...          0\n",
      "3  Conozco a alguien q es adicto al drama! Ja ja ...          2\n",
      "4  Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...          2\n",
      "5  Buen d√≠a todos! Lo primero mandar un abrazo gr...          2\n",
      "6  Desde el esca√±o. Todo listo para empezar #endi...          2\n",
      "7  Bd√≠as. EM no se ira de puente. Si vosotros os ...          2\n",
      "8  Un sistema econ√≥mico q recorta dinero para pre...          2\n",
      "9                  #programascambiados caca d ajuste          0\n",
      "\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|        1| 2927|\n",
      "|        2|25392|\n",
      "|        0|18468|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualizar_datos_csv(df_csv_spanish_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generamos el modelo que se prob√≥ anteriormente y dio los mejores resultados, y lo entrenamos con los datos recogidos de los csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la funci√≥n que engloba el preprocesado de los datos, con la tokenizaci√≥n y la limpieza de tweets.\n",
    "df_tokens_english_clean, df_tokens_spanish_clean = preprocesado_dataframe(df_csv_english_sample,df_csv_spanish_full,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica amazing to me that we can't get...</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazing , cant , get , cold , air , vents , v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica View of downtown Los Angeles, t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[view , downtown , los , angeles , hollywood ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[plz , help , win , bid , upgrade , flight , l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica I'm #elevategold for a good rea...</td>\n",
       "      <td>2</td>\n",
       "      <td>[im , elevategold , good , reason , rock ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood Juli...</td>\n",
       "      <td>1</td>\n",
       "      <td>[julie , andrews , first , lady , gaga , wowd ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  @VirginAmerica amazing to me that we can't get...          0   \n",
       "1  @VirginAmerica View of downtown Los Angeles, t...          2   \n",
       "2  @VirginAmerica plz help me win my bid upgrade ...          1   \n",
       "3  @VirginAmerica I'm #elevategold for a good rea...          2   \n",
       "4  @VirginAmerica @ladygaga @carrieunderwood Juli...          1   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0  [amazing , cant , get , cold , air , vents , v...  \n",
       "1  [view , downtown , los , angeles , hollywood ,...  \n",
       "2  [plz , help , win , bid , upgrade , flight , l...  \n",
       "3         [im , elevategold , good , reason , rock ]  \n",
       "4  [julie , andrews , first , lady , gaga , wowd ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_english_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>1</td>\n",
       "      <td>[libraras , ayudar , menos , besos , gracias ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>2</td>\n",
       "      <td>[gracias , mar ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>0</td>\n",
       "      <td>[off , pensando , regalito , sinde , va , sgae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[conozco , alguien , q , adicto , drama , ja ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...</td>\n",
       "      <td>2</td>\n",
       "      <td>[toca , grabacion , dl , especial , navideno m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  @PauladeLasHeras No te libraras de ayudar me/n...          1   \n",
       "1                          @marodriguezb Gracias MAR          2   \n",
       "2  Off pensando en el regalito Sinde, la que se v...          0   \n",
       "3  Conozco a alguien q es adicto al drama! Ja ja ...          2   \n",
       "4  Toca @crackoviadeTV3 . Grabaci√≥n dl especial N...          2   \n",
       "\n",
       "                                        tokens_clean  \n",
       "0     [libraras , ayudar , menos , besos , gracias ]  \n",
       "1                                   [gracias , mar ]  \n",
       "2  [off , pensando , regalito , sinde , va , sgae...  \n",
       "3  [conozco , alguien , q , adicto , drama , ja ,...  \n",
       "4  [toca , grabacion , dl , especial , navideno m...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_spanish_clean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a partir del dataframe anterior, vamos a eliminar la columna texto ya que nos interesa quedarnos con los textos en\n",
    "# limpio y tras pasar por el preprocesado para el modelo.\n",
    "df_tokens_english_clean = df_tokens_english_clean.drop(\"text\")\n",
    "df_tokens_spanish_clean = df_tokens_spanish_clean.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos el nombre de la columna sentiment por label a los dataframes para los modelos.\n",
    "df_english_clean = df_tokens_english_clean.withColumnRenamed(\"sentiment\", \"label\").cache()\n",
    "df_spanish_clean = df_tokens_spanish_clean.withColumnRenamed(\"sentiment\", \"label\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generaci√≥n de conjunto de training y test de los dataframes.\n",
    "train_english_clean, test_english_clean = df_english_clean.randomSplit([0.9, 0.1], seed=12345)\n",
    "train_spanish_clean, test_spanish_clean = df_spanish_clean.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos el pipeline del modelo con sus etapas y par√°metros, adem√°s de utilizar TrainValidationSplit,\n",
    "# ser√≠a la configuraci√≥n del modelo con el que se obtuvieron los mejores resultados en el notebook de pruebas de ML.\n",
    "countVec = CountVectorizer(inputCol=\"tokens_clean\", outputCol=\"countfeatures\")\n",
    "idf = IDF(inputCol=countVec.getOutputCol(), outputCol=\"features\", minDocFreq=2)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "pipeline = Pipeline(stages=[countVec, idf, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.005]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2])\\\n",
    "    .addGrid(lr.maxIter, [5, 20])\\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(countVec.minTF, [1.0, 3.0, 5.0])\\\n",
    "    .build()\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=MulticlassClassificationEvaluator(),\n",
    "                           trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos los modelos\n",
    "model_eng = tvs.fit(train_english_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spa = tvs.fit(train_spanish_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conectamos con el streaming para recoger los datos de Kafka y tenerlos en un dataframe, al que poder aplicar los modelos para calcular el sentimiento de cada tweet en streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el schema de los datos que queremos guardar de los datos de entrada al sistema.\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"truncated\", BooleanType(), True),\n",
    "    StructField(\"extended_tweet\", StringType(), True),\n",
    "    StructField(\"favorite_count\", IntegerType(), True),\n",
    "    StructField(\"quote_count\", IntegerType(), True),\n",
    "    StructField(\"reply_count\", IntegerType(), True),\n",
    "    StructField(\"retweet_count\", IntegerType(), True),\n",
    "    StructField(\"sentiment\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtienen desde Kafka los datos del topic en espa√±ol mediante el schema.\n",
    "df_spanish = spark\\\n",
    ".readStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    ".option(\"startingOffsets\", \"latest\")\\\n",
    ".option(\"subscribe\", \"TopicSpanish\")\\\n",
    ".option(\"failOnDataLoss\", \"false\") \\\n",
    ".load()\n",
    "\n",
    "df_spanish = df_spanish.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_spanish = df_spanish.select(F.from_json(F.col(\"value\"), schema).alias(\"data\")).select(\"data.*\")           \n",
    "                                                                                          \n",
    "df_spanish.createOrReplaceTempView(\"spanish_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, truncated: boolean, extended_tweet: string, favorite_count: int, quote_count: int, reply_count: int, retweet_count: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtienen desde Kafka los datos del topic en ingl√©s mediante el schema.\n",
    "df_english = spark\\\n",
    ".readStream\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9093\")\\\n",
    ".option(\"startingOffsets\", \"latest\")\\\n",
    ".option(\"subscribe\", \"TopicEnglish\")\\\n",
    ".option(\"failOnDataLoss\", \"false\") \\\n",
    ".load()\n",
    "\n",
    "df_english = df_english.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "df_english = df_english.select(F.from_json(F.col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "df_english.createOrReplaceTempView(\"english_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, truncated: boolean, extended_tweet: string, favorite_count: int, quote_count: int, reply_count: int, retweet_count: int, sentiment: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Is the stream ready?\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "# chequeamos los datos en streaming\n",
    "print(\" \")\n",
    "print(\"Is the stream ready?\")\n",
    "print(df_spanish.isStreaming, df_english.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamamos a la funci√≥n que engloba todo el preprocesado y an√°lisis a aplicar a los datos, y nos devuelve los datos\n",
    "# recogidos en streaming con el sentimiento ya calculado usando los modelos entrenados anteriormente.\n",
    "pred_eng, pred_spa = generacion_dataframe_streaming(df_english, df_spanish, model_eng, model_spa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos en los dataframes con las siguientes columnas:\n",
    "- ID: id √∫nico del tweet\n",
    "- Texto: el texto del tweet\n",
    "- Interacciones: ser√≠a la suma de respuestas, citas, retweets y favoritos del tweet.\n",
    "- Probability: probabilidad con la que se obtiene el valor del sentimiento de cada tweet.\n",
    "- Prediction: etiqueta de sentimiento elegida por el modelo para el tweet.\n",
    "- Timestamp: a√±adimos fecha y hora del procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, texto: string, interacciones: int, probabilidad: string, prediction: double, timestamp: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, texto: string, interacciones: int, probabilidad: string, prediction: double, timestamp: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pred_eng)\n",
    "display(pred_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por si acaso hubiera duplicados los eliminamos.\n",
    "pred_eng = pred_eng.dropDuplicates(subset=['id'])\n",
    "pred_spa = pred_spa.dropDuplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos una funci√≥n a la que pasamos los datos, y la configuraci√≥n para lanzar el streaming, y poder ver por\n",
    "# consola el procesamiento de datos en streaming. Procesamos cada segundo, y usando time.sleep paramos luego la \n",
    "# query de forma programada y c√≥moda.\n",
    "def mostrar_datos(df, outputMode, formato, tiempo, query_name):\n",
    "    query = df.writeStream.outputMode(outputMode).queryName(query_name).format(formato)\\\n",
    "            .option(\"truncate\", \"False\").trigger(processingTime='1 second').start()\n",
    "    time.sleep(tiempo)\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos con los datos en ingl√©s y modo append, donde en cada ejecuci√≥n s√≥lo saldr√°n los nuevos registros.\n",
    "mostrar_datos(pred_eng, 'append', 'console', 60, 'query_eng_append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos con los datos en espa√±ol y modo append, donde en cada ejecuci√≥n s√≥lo saldr√°n los nuevos registros.\n",
    "mostrar_datos(pred_spa, 'append', 'console', 60, 'query_spa_append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos con los datos en ingl√©s y modo update, donde en cada ejecuci√≥n s√≥lo saldr√°n los nuevos registros y \n",
    "# los que hayan sido actualizados.\n",
    "mostrar_datos(pred_eng, 'update', 'console', 60, 'query_eng_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probamos con los datos en espa√±ol y modo update, donde en cada ejecuci√≥n s√≥lo saldr√°n los nuevos registros y\n",
    "# los que hayan sido actualizados.\n",
    "mostrar_datos(pred_spa, 'update', 'console', 60, 'query_spa_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para poder insertar en streaming los datos en MongoDB debemos hacer uso de funciones foreachBatch.\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .option(\"database\", \"tfm_twitter\")\\\n",
    "        .option(\"collection\", \"tweets_streaming_english\").save()\n",
    "    \n",
    "def foreach_batch_function_spa(df, epoch_id):\n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .mode(\"append\")\\\n",
    "        .option(\"database\", \"tfm_twitter\")\\\n",
    "        .option(\"collection\", \"tweets_streaming_spanish\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insertamos los datos en ingl√©s\n",
    "mongo_eng = pred_eng.writeStream\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .start()\n",
    "\n",
    "time.sleep(60)\n",
    "mongo_eng.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insertamos los datos en espa√±ol\n",
    "mongo_spa = pred_spa.writeStream\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .foreachBatch(foreach_batch_function_spa)\\\n",
    "        .start()\n",
    "\n",
    "time.sleep(60)\n",
    "mongo_spa.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funci√≥n para generar el √≠ndice\n",
    "def generateIndexName(name):\n",
    "    return(name + time.strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se insertan los datos en ingl√©s en streaming en elasticsearch\n",
    "elastic_eng = pred_eng.writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"es\")\\\n",
    "    .option(\"es.nodes\",\"localhost\")\\\n",
    "    .option(\"es.port\",\"9200\")\\\n",
    "    .option(\"checkpointLocation\",\"/tmp/\")\\\n",
    "    .option(\"es.resource\", generateIndexName('tweets_sentiment_eng_') + \"/tweets_sentiment_eng_\") \\\n",
    "    .option(\"es.codec\", \"best_compression\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(60)\n",
    "elastic_eng.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se insertan los datos en espa√±ol en streaming en elasticsearch\n",
    "elastic_spa = pred_spa.writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"es\")\\\n",
    "    .option(\"es.nodes\",\"localhost\")\\\n",
    "    .option(\"es.port\",\"9200\")\\\n",
    "    .option(\"checkpointLocation\",\"/tmp/\")\\\n",
    "    .option(\"es.resource\", generateIndexName('tweets_sentiment_spa_') + \"/tweets_sentiment_spa_\") \\\n",
    "    .option(\"es.codec\", \"best_compression\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(60)\n",
    "elastic_spa.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se prueba a hacer las consultas en memoria, obteniendo un dataframe por idioma e insert√°ndolo posteriormente en Mongo y ElasticSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Getting offsets from KafkaV2[Subscribe[TopicSpanish]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "{'message': 'Getting offsets from KafkaV2[Subscribe[TopicEnglish]]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "# vamos a obtener los datos en memoria\n",
    "query_spanish_memory = pred_spa.writeStream.outputMode(\"append\").queryName(\"spanish_memory\").format(\"memory\")\\\n",
    ".option(\"truncate\", \"False\").start()\n",
    "\n",
    "query_english_memory = pred_eng.writeStream.outputMode(\"append\").queryName(\"english_memory\").format(\"memory\")\\\n",
    ".option(\"truncate\", \"False\").start()\n",
    "\n",
    "print(query_spanish_memory.status)\n",
    "print(query_english_memory.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_spanish = spark.table(\"spanish_memory\")\n",
    "result_english = spark.table(\"english_memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>probabilidad</th>\n",
       "      <th>prediction</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167084394420592642</td>\n",
       "      <td>RT @LegalAppMJ: El Estado obliga a las entidad...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8828351497650146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167084394395459584</td>\n",
       "      <td>RT @Zuanna_Sol: Buenos d√≠as üòàüòà https://t.co/tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8228078484535217</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167084394395394051</td>\n",
       "      <td>RT @petrogustavo: Por ahora los violentos gana...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9783394932746887</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167084394399580165</td>\n",
       "      <td>Fuck el desayuno merezco el almuerzo</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4916864037513733</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167084394407956483</td>\n",
       "      <td>@Poetadelciel0 Am√©n üôè</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7538945078849792</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1167084394387050497</td>\n",
       "      <td>RT @anacastelan_: Donde no te buscan, no haces...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5875048637390137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              texto  \\\n",
       "0  1167084394420592642  RT @LegalAppMJ: El Estado obliga a las entidad...   \n",
       "1  1167084394395459584  RT @Zuanna_Sol: Buenos d√≠as üòàüòà https://t.co/tr...   \n",
       "2  1167084394395394051  RT @petrogustavo: Por ahora los violentos gana...   \n",
       "3  1167084394399580165               Fuck el desayuno merezco el almuerzo   \n",
       "4  1167084394407956483                              @Poetadelciel0 Am√©n üôè   \n",
       "5  1167084394387050497  RT @anacastelan_: Donde no te buscan, no haces...   \n",
       "\n",
       "   interacciones        probabilidad  prediction            timestamp  \n",
       "0              0  0.8828351497650146         0.0  2019-08-29 16:39:58  \n",
       "1              0  0.8228078484535217         2.0  2019-08-29 16:39:58  \n",
       "2              0  0.9783394932746887         2.0  2019-08-29 16:39:58  \n",
       "3              0  0.4916864037513733         1.0  2019-08-29 16:39:58  \n",
       "4              0  0.7538945078849792         2.0  2019-08-29 16:39:58  \n",
       "5              0  0.5875048637390137         0.0  2019-08-29 16:39:58  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos los datos en espa√±ol\n",
    "result_spanish.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>texto</th>\n",
       "      <th>interacciones</th>\n",
       "      <th>probabilidad</th>\n",
       "      <th>prediction</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1167084394382790657</td>\n",
       "      <td>@BTS_twt OMG MY LOVEüò≠üò≠</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6848946213722229</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167084394382856193</td>\n",
       "      <td>@EmmaSpoonley Yes! I know I'm as appalled at m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5481786727905273</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1167084394399633410</td>\n",
       "      <td>RT @sawxcy: If you ain‚Äôt trynna b like this I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7860847115516663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1167084394391265286</td>\n",
       "      <td>RT @nielo_2wit: You cannot always be right... ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6511359214782715</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167084394387054592</td>\n",
       "      <td>RT @AccidentalNut: I got y‚Äôall , rt Bc these h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5655750632286072</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-08-29 16:39:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                              texto  \\\n",
       "0  1167084394382790657                             @BTS_twt OMG MY LOVEüò≠üò≠   \n",
       "1  1167084394382856193  @EmmaSpoonley Yes! I know I'm as appalled at m...   \n",
       "2  1167084394399633410  RT @sawxcy: If you ain‚Äôt trynna b like this I ...   \n",
       "3  1167084394391265286  RT @nielo_2wit: You cannot always be right... ...   \n",
       "4  1167084394387054592  RT @AccidentalNut: I got y‚Äôall , rt Bc these h...   \n",
       "\n",
       "   interacciones        probabilidad  prediction            timestamp  \n",
       "0              0  0.6848946213722229         2.0  2019-08-29 16:39:58  \n",
       "1              0  0.5481786727905273         2.0  2019-08-29 16:39:58  \n",
       "2              0  0.7860847115516663         0.0  2019-08-29 16:39:58  \n",
       "3              0  0.6511359214782715         2.0  2019-08-29 16:39:58  \n",
       "4              0  0.5655750632286072         2.0  2019-08-29 16:39:58  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizamos los datos en ingl√©s\n",
    "result_english.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end streaming\n",
    "query_spanish_memory.stop()\n",
    "query_english_memory.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almacenar datos en CSV, MongoDB y ElasticSearch.\n",
    "\n",
    "Probamos a almacenar los datos etiquetados en diferentes formatos: archivos csv, MongoDB, y en ElasticSearch para su posterior visualizaci√≥n con Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190829-164021\n"
     ]
    }
   ],
   "source": [
    "# sacamos una cadena con fecha y hora para a√±adirla al nombre de los csv que se generan a continuaci√≥n\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasamos los dataframes con el sentimiento calculado a pandas\n",
    "df_eng_pandas = result_english.toPandas()\n",
    "df_spa_pandas = result_spanish.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se guardan los dataframes con el sentimiento calculado en csv\n",
    "df_eng_pandas.to_csv('./data/predict_streaming_english_'+timestr+'.csv', index=False)\n",
    "df_spa_pandas.to_csv('./data/predict_streaming_spanish_'+timestr+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1a22f76388>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardar datos en MongoDB\n",
    "conexion = 'mongodb://localhost:27017'\n",
    "client = MongoClient(conexion)\n",
    "\n",
    "# accedemos a la base de datos\n",
    "db = client.tfm_twitter\n",
    "# insertamos los dataframes en la tabla correspondiente\n",
    "db.tweets_streaming_english.insert_many(df_eng_pandas.to_dict(\"records\"))\n",
    "db.tweets_streaming_spanish.insert_many(df_spa_pandas.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tweets_sentiment_spa_20190829-164021\"}{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tweets_sentiment_eng_20190829-164021\"}True True\n"
     ]
    }
   ],
   "source": [
    "# guardar datos en ElasticSearch\n",
    "es = Elasticsearch('http://localhost:9200/')\n",
    "\n",
    "# a√±adimos fecha y hora a los √≠ndices\n",
    "indice_spa = \"tweets_sentiment_spa_\"+timestr\n",
    "indice_eng = \"tweets_sentiment_eng_\"+timestr\n",
    "\n",
    "# primero comprobamos si ya existen los √≠ndices y se borran\n",
    "if es.indices.exists(indice_spa):\n",
    "    !curl -X DELETE localhost:9200/{indice_spa}\n",
    "if es.indices.exists(indice_eng):\n",
    "    !curl -X DELETE localhost:9200/{indice_eng}\n",
    "\n",
    "# generaci√≥n de √≠ndices\n",
    "!curl -X PUT localhost:9200/{indice_spa}\n",
    "!curl -X PUT localhost:9200/{indice_eng}\n",
    "\n",
    "TYPE = \"record\"\n",
    "\n",
    "def rec_to_actions(df, lang):\n",
    "    for record in df.to_dict(orient=\"records\"):\n",
    "        if(lang==0): yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (indice_spa, TYPE))\n",
    "        elif(lang==1): yield ('{ \"index\" : { \"_index\" : \"%s\", \"_type\" : \"%s\" }}'% (indice_eng, TYPE))\n",
    "        yield (json.dumps(record, default=int))\n",
    "\n",
    "if not es.indices.exists(indice_spa):\n",
    "    raise RuntimeError('index does not exists, use `curl -X PUT \"localhost:9200/%s\"` and try again'%indice_spa)\n",
    "if not es.indices.exists(indice_eng):\n",
    "    raise RuntimeError('index does not exists, use `curl -X PUT \"localhost:9200/%s\"` and try again'%indice_eng)\n",
    "\n",
    "r_spa = es.bulk(rec_to_actions(df_spa_pandas, 0))\n",
    "r_eng = es.bulk(rec_to_actions(df_eng_pandas, 1)) \n",
    "\n",
    "print(not r_spa[\"errors\"], not r_eng[\"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se guardan los modelos utilizados para la predicci√≥n\n",
    "model_eng_best = model_eng.bestModel\n",
    "model_spa_best = model_spa.bestModel\n",
    "model_eng_best.write().overwrite().save('./data/models/model_eng_'+timestr)\n",
    "model_spa_best.write().overwrite().save('./data/models/model_spa_'+timestr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
